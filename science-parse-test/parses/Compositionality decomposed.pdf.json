{
  "name" : "Compositionality decomposed.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "Compositionality decomposed: how do neural networks generalise?",
    "authors" : [ "Dieuwke Hupkes", "Verna Dankers", "Elia Bruni" ],
    "emails" : [ "dieuwkehupkes@gmail.com", "vernadankers@gmail.com", "mathijsmul@gmail.com", "elia.bruni@gmail.com" ],
    "sections" : [ {
      "heading" : null,
      "text" : "are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models’ composition operations are local or global (iv) if models’ predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement."
    }, {
      "heading" : "1. Introduction",
      "text" : "The advancements of distributional semantics of the word level allowed the field of natural language processing to move from discrete mathematical methods to models that use continuous numerical vectors (see, e.g. Clark, 2015; Erk, 2012; Turney and Pantel, 2010). Such continuous vector representations operationalise the distributional semantics hypothesis – stating that semantically similar words have similar contextual distributions (e.g. Miller and Charles, 1991) – by keeping track of contextual information from large textual corpora. They can then act as surrogates for word meaning and be used, for example, to quantify the degree of semantic similarity between words, by means of simple geometric operations (Clark, 2015). Words represented in this way can be an integral part of the computational pipeline and have proven to be useful for almost all natural language processing tasks (see, e.g. Hirschberg and Manning, 2015).\nAfter the introduction of continuous word representations, a logical next step involved understanding how to compose these representations to obtain representations for phrases, sentences and even larger pieces of discourse. Some early approaches to do so stayed close to formal symbolic theories of language and sought to explicitly model semantic composition by finding a composition function that could be used to combine word representations. The adjective-noun compound blue\nc©2020 AI Access Foundation. All rights reserved.\nar X\niv :1\n90 8.\n08 35\n1v 2\n[ cs\n.C L\n] 2\n3 Fe\nsky, for instance, would be represented as a new vector resulting from the composition of the representations for blue and sky. Examples of such composition functions are as simple as vector addition and (point-wise) multiplication (e.g. Mitchell and Lapata, 2008) up to more powerful tensor-based operations (Smolensky, 1990; Plate, 1991) where, for instance, the adjective blue would be represented as a matrix, which would be multiplied with the noun vector sky to return the representation for blue sky (e.g. Baroni and Zamparelli, 2010; Coecke et al., 2010).1\nA more recent trend in word composition exploits deep learning, a class of machine learning techniques that model language in a completely data-driven fashion, by defining a loss on a downstream task (such as sentiment analysis, language modelling or machine translation) and learning the representations for larger chunks from a signal back-propagated from this loss. In terms of how they compose representations, models using deep learning can be divided into roughly two categories.\nIn the first category, deep learning is exploited to learn only the actual composition functions, while the order of composition is defined by the modeller. An example is the recursive neural network of Socher et al. (2010), in which representations for larger chunks are computed recursively following a predefined syntactic parse tree of the sentence. While the composition function in this approach is fully learned from data using back-propagation through structure (Goller and Kuchler, 1996), the tree structure that defines the order of application has to be provided to the model, allowing models to be ‘compositional by design’. More recent variants lift this dependency on external parse trees by jointly learning the composition function and the parse tree (Le and Zuidema, 2015; Kim et al., 2019, i.a.), often at the cost of computational feasibility.\nIn the second type of deep learning models, no explicit notion of (linguistic) trees or arbitrary depth hierarchy is entertained. Earlier models of this type deal with language processing sequentially and use recurrent processing units such as LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Chung et al., 2014) at their core (Sutskever et al., 2014) or are based on convolutional networks (Kalchbrenner et al., 2014). An important contribution to their effectiveness comes from attention mechanisms, which allow recurrent models to keep track of long-distance dependencies more effectively (Bahdanau et al., 2015). More recently, these models went all-in on attention, abandoning sequential processing in favour of massively distributed sequence processing all based on attention (Vaswani et al., 2017). While the architectural design of this class of models is not motivated by knowledge about linguistics or human processing, they are – through their ability to easily process very large amounts of data – more successful than the previously mentioned (sub)symbolic models on a variety of natural language processing tasks.\nDifferent types of models that compose smaller representations into larger ones can be compared along many dimensions. Commonly, they are evaluated by the usefulness of their representations for different types of tasks, but also scalability, how much data they need to develop their representations (sample efficiency), and their computational feasibility play a role in their evaluation. It remains, however, difficult to explicitly assess if the composition functions they implement are appropriate for natural language and, importantly, to what extent they are in line with the vast amount of knowledge and theories about semantic composition from formal semantics and (psycho)linguistics. While the composition functions of symbolic models are easy to understand (because they are defined on a mathematical level), it is not empirically established that their rigidity is appropriate for dealing with the noisiness and complexity of natural language (e.g. Potts, 2019). Neural models, on the other hand, seem very well up to handling noisy scenarios but are often argued to be fundamentally incapable of conducting the types of compositions required to process natural language (for more information on this debate, see Pinker, 1984; Fodor and Pylyshyn, 1988; Smolensky, 1990; Marcus, 2003) or at least to not use those types of compositions to solve their tasks (e.g. Lake and Baroni, 2018).\nIn this work, we consider the latter type of models and focus in particular on whether these models are capable of learning compositional solutions, a question that recently, with the rise of the\n1. For a more complete overview and an analysis of such approaches in the light of formal semantics, we refer to Kartsaklis (2014) and Boleda and Herbelot (2016), respectively.\nsuccess of such models, has attracted the attention of several researchers. While many empirical studies can be found that explore the compositional abilities of neural models, they have not managed to convince the community of either side of the debate: whether neural networks are able to learn and behave compositionally is still an open question. One issue standing in the way of more clarity on this matter is that different researchers have different interpretations of what exactly it means to say that a model is or is not compositional, a point exemplified by the vast number of different tests that exist for compositionality. Some studies focused on testing if models are able to productively use symbolic rules (e.g. Lake and Baroni, 2018); Some instead consider models’ ability to process hierarchical structures (Hupkes et al., 2018; Linzen et al., 2016); Yet others consider if models can segment the input into reusable parts (Johnson et al., 2017).\nThis variety of tests for compositionality of neural networks existing in the literature is better understandable considering the open nature of the principle of compositionality, by Partee (1995) phrased as “The meaning of a whole is a function of the meanings of the parts and of the way they are syntactically combined”. While there is ample support for this principle, there is less consensus about its exact interpretation and practical implications. One important reason for this is that the principle is not theory-neutral: it requires a theory of both syntax and meaning, as well as functions to determine the meaning of composed parts. Without these components, the principle of compositionality is formally vacuous (Janssen, 1983; Zadrozny, 1994), because also trivial and intuitively non-compositional solutions that cast every expression as one part and assign it a meaning as a whole do not formally violate the principle of compositionality. Furthermore, the principle of compositionality concerns the compositionality of language but does not specify what it means for a language user or model to be compositional. Can a model be called compositional when it can represent a compositional language? Are there any restrictions on how it has to do so? To empirically test models for compositionality it is necessary to first establish what is to be considered compositional behaviour. With this work, we aim to contribute to clarity on this point, by presenting a study in which we collect different aspects of and intuitions about compositionality of language from linguistics and philosophy and translate them into concrete tests that can be used to better understand the composition functions learned by neural models trained end-to-end on a downstream task.\nThe contribution of our work, we believe, is three-fold. First, we provide a bridge between, on the one hand, the vast amount of theory about compositionality that underpins symbolic models of language and semantic composition and, on the other hand, the neural models of language that have proven to be very effective in many natural language tasks that seem to require compositional capacities. Importantly, we do not aim to provide a new definition of compositionality, but rather we identify different components of compositionality within the literature and design behavioural tests that allow testing for these components independently. We believe that the field will profit from such a principled analysis of compositionality and that this analysis will provide clarity concerning the different interpretations that may be entertained by different researchers. A division into clearly understood components can help to identify and categorise the strengths and weaknesses of different models. We provide concrete and usable tests, bundled in a versatile test suite that can be applied to any kind of model.\nSecondly, to demonstrate the usefulness of this test suite, we apply our tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, uncovering interesting strengths and weaknesses of these three architectures.\nLastly, we touch upon the complex question that concerns the extent to which a model needs to be explicitly compositional to adequately model data of which the underlying structure is, or seems, compositional. We believe that, in a time where the most successful natural language processing methods require large amounts of data and are not directly motivated by linguistic knowledge or structure, this question bears more relevance than ever.\nOutline In what follows, we first briefly revise other literature with similar aims and sketch how our work stands apart from previous attempts to assess the extent to which networks implement compositionality (Section 2). We describe previously proposed data sets to evaluate compositionality as well as studies that evaluate the representations of pre-trained models. In Section 3, we give a theoretical explanation of the five notions for which we devise tests, and we propose how to behaviourally test for them. In Section 4, we describe the data set that we use for our study, followed by a brief description of the three types of architectures that we compare in our experiments in Section 5. We then detail our experiments and report and analyse their results in Section 6 and further reflect upon their implications in Section 7."
    }, {
      "heading" : "2. Related work",
      "text" : "Whether artificial neural networks are fundamentally capable of representing compositionality, trees and hierarchical structure has been a prevalent topic ever since the first connectionism models for natural language were introduced. Recently, this topic has regained attention, and a substantial number of empirical studies can be found that explore the compositional abilities of neural models, with a specific focus on their ability to represent hierarchy. These studies can be roughly divided into two categories: studies that devise specific data sets that models can be trained and tested on to assess if they behave compositionally, and studies that focus on assessing the representations that are learned by models trained on some independent (often natural) data set."
    }, {
      "heading" : "2.1 Evaluating compositionality with artificial data",
      "text" : "Specifically crafted, artificial data sets to evaluate compositionality are typically generated from an underlying grammar. It is then assumed that models can only find the right solution to the test set if they learned to interpret the training data in a compositional fashion. Below, we discuss a selection of such data sets and briefly review their results.2"
    }, {
      "heading" : "2.1.1 Arithmetic language and mathematical reasoning",
      "text" : "One of the first (recent) data sets proposed as a testbed to reveal how neural networks process hierarchical structure is the arithmetic language, introduced by Veldhoen et al. (2016). Veldhoen et al. (2016) test networks for algebraic compositionality by looking at their ability to process spelled out, nested arithmetic expressions. In a follow-up paper, to gain insight into the types of solution that networks encode, the same authors introduce diagnostic classifiers, trained to fire for specific strategies used to solve the problem. They show that simple recurrent networks do not perform well on the task, but gated recurrent networks can generalise well to lengths and depths of arithmetic expressions that were not in the training set, although their performance quickly deteriorates when the length of expressions grows (Hupkes et al., 2018).3 From this, they conclude that these models are – to some extent – able to capture the underlying compositional structure of the data.\nMore recently, Saxton et al. (2019) released another data set in which maths was used to probe the compositional generalisation skills of neural networks. Saxton et al. (2019) compare transformers and LSTM-based architectures trained on a data set with mathematical questions and find that the\n2. Discussing in detail all different data sets that have been proposed to evaluate compositionality in neural networks falls outside the scope of this paper. We aimed to make a representative selection of studies, using as a criterion that they should involve sequential inputs and explicitly mention compositionality. We excluded grounded data sets such as CLEVR (Johnson et al., 2017) and SQOOP (Bahdanau et al., 2018), which contain more than one modality. Furthermore, we did not include studies whose primary focus is on how neural networks implement compositional structures (Lakretz et al., 2019; Giulianelli et al., 2018; McCoy et al., 2019; Soulos et al., 2019; Weiss et al., 2018a) or studies that evaluate compositionality only based on models’ representations (Andreas, 2019). 3. Zaremba and Sutskever (2014) also used a task based on arithmetics, which requires learning to execute computer programs, which they use to compare different learning curricula.\ntransformer models generalise better than the LSTM models. Specifically, transformers outperform LSTMs on a set of extrapolation tests that require compositional skills such as generalising to questions involving larger numbers, more numbers or more compositions. However, performance deteriorates for questions that require the computation of intermediate values, which Saxton et al. (2019) reason indicates that the model has not truly learned to treat the task in a compositional manner but instead applies shallow tricks."
    }, {
      "heading" : "2.1.2 SCAN",
      "text" : "In 2018, Lake and Baroni proposed the SCAN data set, describing a simple navigation task that requires an agent to execute commands expressed in a compositional language. The authors test various sequence-to-sequence models on three different splits of the data: a random split, a split testing for longer action sequences and a split that requires compositional application of words learned in isolation. The models obtain almost perfect accuracy on the first split while performing very poorly on the last two, which the authors argue require a compositional understanding of the task. They conclude that – after all these years – sequence-to-sequence recurrent networks are still not systematic. In a follow-up paper by Loula et al. (2018), the same authors criticise these findings and propose a new set of splits which focuses on rearranging familiar words (i.e. “jump”, “right” and “around”) to form novel meanings (“jump around right”) . Although they collect considerably more evidence for systematic generalisation within their amended setup, the authors confirm their previous findings that the models do not learn compositionally. Very recently, SCAN was also used to diagnose convolutional networks. Comparing to recurrent networks, Dess̀ı and Baroni (2019) find that convolutional networks exhibit improved compositional generalisation skills but their errors are unsystematic, indicating that the model did not fully master any of the systematic rules."
    }, {
      "heading" : "2.1.3 Lookup tables",
      "text" : "Lǐska et al. (2018) introduce a minimal compositional test where neural networks need to apply function compositions to correctly compute the meaning of sequences of lookup tables. The meanings of these lookup tables are exhaustively defined and presented to the model, so that applying them does not require more than rote memorisation. The authors show that out of many models trained with different initialisations only a very small fraction exhibits compositional behaviour, while the vast majority does not.4"
    }, {
      "heading" : "2.1.4 Logical inference",
      "text" : "Bowman et al. (2015) propose a data set which uses a slightly different setup: they assess models’ compositional skills by testing their ability to infer logical entailment relations between pairs of sentences in an artificial language. The grammar they use licenses short, simple sentences; the relations between these sentences are inferred using a natural logic calculus that acts directly on the generated expressions. Bowman et al. (2015) show that recursive neural networks, which recursively apply the same composition function and are thus compositional by design, obtain high accuracies on this task. Mul and Zuidema (2019) show that also gated recurrent models can perform well on an adapted version of the same task, which uses a more complex grammar. With a series of additional tests, Mul and Zuidema (2019) provide further proof for basic compositional generalisation skills of the best-performing recurrent models. Tran et al. (2018) report similar findings, and furthermore show that while a transformer performs similar to an LSTM model when the entire data set is used, an LSTM model generalises better when smaller training data is used.\n4. Hupkes et al. (2019) show how adding an extra supervision signal to the network’s attention consistently results in a complete solution of the task, but it is not clear how their results extend to other, more complicated scenarios. Korrel et al. (2019) propose a novel architecture with analogous, complete solutions without the need for extra supervision."
    }, {
      "heading" : "2.2 Evaluating compositionality with natural data",
      "text" : "While very few studies present methods to explicitly evaluate how compositional the representations of models that are trained on independent data sets are, there are several studies that focus on evaluating aspects of such models that are related to compositionality. In particular, starting from the seminal work of Linzen et al. (2016), the evaluation of the syntactic capabilities of neural language models has attracted a considerable amount of attention. While the explicit focus of such studies is on the syntactic capabilities of different models and not on providing tests for compositionality, many of the results in fact concern the way that neural networks process the types of hierarchical structures often assumed to underpin compositionality.5"
    }, {
      "heading" : "2.2.1 Number agreement",
      "text" : "Linzen et al. (2016) propose to test the syntactic abilities of LSTMs by testing to what extent they are capable of correctly processing long-distance subject-verb agreement, a phenomenon they argue to be commonly regarded as evidence for hierarchical structure in natural language. They devise a number-agreement task and find that a pre-trained state-of-the-art LSTM model (Jozefowicz et al., 2016) does not capture the structure-sensitive dependencies.\nLater, these results were contested by a different research group, who repeated and extended the study with a different language model and tested a number of different long-distance dependencies for English, Italian, Hebrew and Russian (Gulordava et al., 2018). Their results do not match the findings of the earlier study: Gulordava et al. (2018) find that an LSTM language model can solve the subject-verb agreement problem well, even when the words in the sentence are replaced by syntactically nonsensical words, which they take as evidence that the model is indeed relying on syntactic and not semantic clues.6 Whether the very recent all-attention language models do also capture syntax-sensitive dependencies is still an open question. Some (still unpublished) studies find evidence that such models score high on the previously described number-agreement task (Goldberg, 2019; Lin et al., 2019). More mixed results are reported by others (Tran et al., 2018; Wolf, 2019)."
    }, {
      "heading" : "2.2.2 Syntax in machine translation",
      "text" : "The subfield of natural language processing that is most related to ours in terms of setup is the field of machine translation (MT). There are little detailed studies concerning the compositional behaviour of neural MT models but many that consider the representations of trained models. Analyses in this line of work typically consider which properties are encoded by MT models, with a specific focus on the difference between the representations within layers that are situated at different levels of the hierarchy of a model. A robust finding from such analyses is that features such as syntactic constituency, part-of-speech tags and dependency edges can be reliably predicted from the hidden representations of both recurrent neural networks (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018) and transformer models (Raganato and Tiedemann, 2018; Tenney et al., 2019b). Generally, lower-level features are encoded in lower layers, while higher-level syntactic and semantic features are better represented in deeper layers (e.g. Blevins et al., 2018; Tenney et al., 2019a). For transformer models, a recent wave of papers demonstrates that such features can also\n5. In fact, there are quite a few earlier studies relating to the ability of neural networks to implement grammatical structure that consider a similar paradigm, albeit using artificial languages. Such studies consider how well neural networks can represent formal languages generated by grammars from different classes of the Chomsky Hierarchy (e.g. Elman, 1991; Christiansen and Chater, 1999; Rodriguez, 2001; Wiles and Elman, 1995; Rodriguez et al., 1999; Batali, 1994; Weiss et al., 2018b). Like the studies with natural language described in this chapter, these studies focus on rules and hierarchical structure but do not specifically target compositionality, which requires not only syntax but also meaning. 6. The task proposed by Linzen et al. (2016) served as inspiration for many studies investigating the linguistic or syntactic capabilities for neural language models, and also the task itself was used in many follow-up studies. Such studies, which we will not further discuss, are generally positive about the extent to which recurrent language models represent syntax.\nbe extracted from attention patterns (Vig and Belinkov, 2019; Mareček and Rosa, 2018; Lin et al., 2019). While these results do not straightforwardly extend to the questions about compositionality that we are considering in this work, they do demonstrate that both recurrent and attention-based models trained in a setup similar to the one considered for this work are able to capture the types of higher-level syntactic features that are often considered to be key for compositional behaviour."
    }, {
      "heading" : "2.3 Intermediate conclusions",
      "text" : "We reviewed various attempts to assess the extent to which neural models are able to implement compositionality and hierarchy. This overview illustrated the difficulty and importance of evaluating the behaviour of neural models but also showed that whether neural networks can or do learn compositionally is still an open question. Both strands of approaches we considered – approaches that use special compositional data sets to train and test models, and approaches that instead focus on the evaluation of pre-trained models – report positive as well as negative results.\nIn the first approach, researchers try to encode a certain notion of compositionality in the task itself. While it is important, when testing for compositionality, to make sure the specific task that networks are trained on has a clear demand for compositional solutions, we believe these studies fall short in explicitly linking the task they propose to clearly-defined notions of compositionality. Further, we believe that the multifaceted notion of compositionality cannot be exhausted in one single task. In the following section, we disconnect testing compositionality from the task at hand and disentangle five different theoretically motivated ways in which a network can exhibit compositional behaviours that are not a priori linked to a specific downstream task.\nThe second type of studies roots its tests into clear linguistic hypotheses. However, by testing neural networks that are trained on uncontrolled data, they lose the direct connection between compositionality and the downstream task. Although compositionality is widely considered to play an important role for natural language, it is unknown what type of compositional skills – if any – a model needs to have to successfully model tasks involving natural language, such as for instance language modelling. If it cannot be excluded that successful heuristics or syntax-insensitive approximations exist, a negative result can not be taken as evidence that a particular type of model cannot capture compositionality, it merely indicates that this exact model instance did not capture it in this exact case. While, in the long run, we also wish to reconnect the notion of compositionality to natural data, we believe that before being able to do so, it is of primary importance to reach an agreement about what defines compositional behaviour and how it should be tested for in neural networks."
    }, {
      "heading" : "3. Testing compositionality",
      "text" : "In the previous section, we discussed various attempts to evaluate the compositional skills of neural network models. We argued that progressing further on this question requires more clarity on what defines compositionality for neural networks, which we address in this work by providing tests that are more strongly grounded in the literature about compositionality. We now arrive at the theoretical part of the core of our research, in which we set the theoretical ground for the five tests we propose and conduct in this paper. We describe five aspects of compositionality that are explicitly motivated by theoretical literature on this topic and propose, on a high level, how to translate them into behavioural tests for (neural) models.\nWe propose to test (i) if models systematically recombine known parts and rules (systematicity) (ii) if models can extend their predictions beyond the length they have seen in the training data (productivity) (iii) if models’ predictions are robust to synonym substitutions (substitutivity) (iv) if models’ composition operations are local or global (localism) and (v) if models favour rules or exceptions during training (overgeneralisation). Below, we describe the theory that motivated us to\nselect these aspects, and we describe on an abstract level how we translate them into concrete tests. A systematic depiction is shown in Figure 1.\n(a) Systematicity\n+\n(b) Productivity (c) Substitutivity"
    }, {
      "heading" : "3.1 Systematicity",
      "text" : "The first property we propose to test for – following many of the works presented in the related work section of this article – is systematicity, a notion frequently used in the context of compositionality. The term was introduced by Fodor and Pylyshyn (1988) – notably, in a paper that argued against connectionist architectures – who used it to denote that\n“[t]he ability to produce/understand some sentences is intrinsically connected to the ability to produce/understand certain others” (Fodor and Pylyshyn, 1988, p. 25)\nThis ability concerns the recombination of known parts and rules: anyone who understands a number of complex expressions also understands other complex expressions that can be built up from the constituents and syntactical rules employed in the familiar expressions. To use a classic example from Szabó (2012): someone who understands ‘brown dog’ and ‘black cat’ also understands ‘brown cat’.\nFodor and Pylyshyn (1988) contrast systematicity with storing all sentences in an atomic way, in a dictionary-like mapping from sentences to meanings. Someone who entertains such a dictionary would not be able to understand new sentences, even if these sentences were similar to the ones occurring in their dictionary. Since humans are able to infer meanings for sentences they have never heard before, they must use some systematic process to construct these meanings from the ones they internalised before. By the same argument, however, any model that is able to generalise to\na sequence outside its training space (its test set), should have learned to construct outputs from parts it perceived during training and some rule of recombination. Thus, rather than asking if a model is systematic, a more interesting question is whether the rules and constituents the model uses are in line with what we believe to be the actual rules and constituents underlying a particular data set or language."
    }, {
      "heading" : "3.1.1 Testing systematicity",
      "text" : "With our systematicity test, we aim to pull out that specific aspect, by testing if a model can recombine constituents that have not been seen together during training. In particular, we focus on combinations of words a and b that meet the requirements that (i) the model has only been familiarised with a in contexts excluding b and vice versa but (ii) the combination a b is plausible given the rest of the corpus."
    }, {
      "heading" : "3.2 Productivity",
      "text" : "A notion closely related to systematicity is productivity, which concerns the open-ended nature of natural language: language appears to be infinite, but has to be stored with finite capacity. Hence, there must be some productive way to generate new sentences from this finite storage.7 While this ‘generative’ view of language became popular with Chomsky in the early sixties (Chomsky, 1956), Chomsky himself traces it back to Von Humboldt, who stated that ‘language makes infinite use of finite means’.\nBoth systematicity and productivity rely on the recombination of known constituents into larger compounds. However, whereas systematicity can be – to some extent – empirically established, productivity cannot, as it is not possible to prove that natural languages in fact contain an infinite number of complex expressions (Pullum and Scholz, 2010). Even if humans’ memory allowed them to produce infinitely long sentences, their finite life prevents them from doing so. Productivity of language is therefore more controversial than systematicity."
    }, {
      "heading" : "3.2.1 Testing productivity",
      "text" : "To separate systematicity from productivity, in our productivity test we specifically focus on the aspect of unboundedness. We test whether a model can understand sentences that are longer than the ones encountered during training. To test this, we separate sequences in the data based on length and evaluate models on their ability to cope with longer sequences after having been familiarised with the shorter ones."
    }, {
      "heading" : "3.3 Substitutivity",
      "text" : "A principle closely related to the principle of compositionality is the principle of substitutivity. This principle, which finds its origin in philosophical logic, states that if an expression is altered by replacing one of its constituents with another constituent with the same meaning (a synonym), this does not affect the meaning of the expression (Pagin, 2003). In other words, if a substitution preserves the meaning of the parts of a complex expression, it also preserves the meaning of the whole. In the latter formulation, the correspondence with the principle of compositionality itself can be easily seen: as substituting part of an expression with a synonym changes neither the structure of the expression nor the meaning of its parts, it should not change the meaning of the expression itself either.\nLike the principle of compositionality, the substitutivity principle in the context of natural language is subject to interpretation and discussion. Husserl (1913) pointed out that the substitution of expressions with the same meaning can result in nonsensical sentences if the expressions belong\n7. The term productivity also has a technical meaning in morphology, which we do not imply here.\nto different semantic categories (the philosopher Geach (1965) illustrated this considering the two expressions Plato was bald and baldness was an attribute of Plato. While these expressions are synonymous, it is not possible to substitute the first with the second in the sentence The philosopher whose most eminent pupil was Plato was bald).\nA second context which poses a challenge for the substitutivity principle concerns embedded statements about beliefs. As already sketched out in the previous section, if X and Y are synonymous, this does not necessarily imply that the expressions Peter thinks that X and Peter thinks that Y are both true. In this work, we do not consider these cases, but instead focus on the more general question: is substitutivity a salient notion for neural networks and under what conditions can and do they infer synonymity?"
    }, {
      "heading" : "3.3.1 Testing substitutivity",
      "text" : "We test substitutivity by probing under which conditions a model considers two atomic units to be synonymous. To this end, we artificially introduce synonyms and consider how the prediction of a model changes when an atomic unit in an expression is replaced by its synonym. We consider two different cases. Firstly, we analyse the case in which synonymous words occur equally often and in comparable contexts. In this case, synonymity can be inferred from the corresponding meanings on the output side but is aided by distributional similarities on the input side. Secondly, we consider pairs of words in which one of the words occurs only in very short sentences, which we call primitive contexts. In this case, synonymity can only be inferred from the (implicit) semantic mapping, which is identical for both words, but not from the context that those words appear in."
    }, {
      "heading" : "3.4 Localism",
      "text" : "In its basic form, the principle of compositionality states that the meaning of a complex expression derives from the meanings of its constituents and how they are combined. It does not impose any restrictions on what counts as an admissible way of combining different elements, which is why the principle taken in isolation is formally vacuous.8 As a consequence, the interpretation of the principle of compositionality depends on the type of constraints that are put on the semantic and syntactic theories involved. One important consideration concerns – on an abstract level – how local the composition operations should be. When operations are very local (a case also referred to as strong or first-level compositionality), the meaning of a complex expression depends only on its local structure and the meanings of its immediate parts (Pagin and Westerst̊ahl, 2010; Jacobson, 2002). In other words, the meaning of a compound is only dependent on the meaning of its immediate ‘children’, regardless of the way that their meaning was built up. In global or weak compositionality, the meaning of an expression follows from its total (global) structure and the meanings of its atomic parts. In this interpretation, compounds can have different meanings, depending on the larger expression that they are a part of.\nCarnap (1947) presents an example that nicely illustrates the difference between these two interpretations, in which he considers sentences with tautologies. Under the view that the meaning of declarative sentences is determined by the set of all worlds in which this sentence is true, any two tautologies X and Y are synonymous. Under the local interpretation of compositionality, this entails that also the phrases Peter thinks that X and Peter thinks that Y should be synonymous, which is not necessarily the case, as Peter may be aware of some tautologies but unaware of others. The global interpretation of compositionality does not give rise to such a conflict, as X and Y, despite being identical from a truth-conditional perspective, are not structurally identical. Under this representation, the meanings of X and Y are locally identical, but not globally, if also the phrase they are a part of is considered. For natural language, contextual effects, such as the disambiguation\n8. We previously cited Janssen (1983), who proved this claim by showing that arbitrary sets of expressions can be mapped to arbitrary sets of meanings without violating the principle of compositionality, as long as one is not bound to a fixed syntax.\nof a phrase or word by a full utterance or even larger piece of discourse, make the localist account highly controversial. As a contrast, consider an arithmetic task, where the outcome of 14 - (2 + 3) does not change when the subsequence (2 + 3) is replaced by 5, a sequence with the same (local) meaning, but a different structure."
    }, {
      "heading" : "3.4.1 Testing localism",
      "text" : "We test if a model’s composition operations are local or global by comparing the meanings the model assigns to stand-alone sequences to those it assigns to the same sequences when they are part of a larger compound. More specifically, we compare a model’s output when it is given a composed sequence X, built up from two parts A and B with the output the same model gives when it is forced to first separately process A and B in a local fashion. If the model employs a local composition operation that is true to the underlying compositional system that generated the language, there should be no difference between these two outputs. A difference between these two outputs, instead, indicates that the model does not compute meanings by first computing the meanings of all subparts, but pursues a different, more global, strategy."
    }, {
      "heading" : "3.5 Overgeneralisation",
      "text" : "The previously discussed compositionality arguments are of mixed nature. Some – such as productivity and systematicity – are linked to the way that humans acquire and process language. Others – such as substitutivity and localism – are properties of the mapping from signals to meanings in a particular language. While it can be tested if a language user abides by these principles, these principles themselves do not relate directly to language users. To complete our set of tests to assess whether a model learns compositionally, we include also a notion that exclusively concerns the acquisition of the language by a model: we consider if models exhibit overgeneralisation when faced with non-compositional phenomena.\nOvergeneralisation (or overregularisation) is a language acquisition term, which refers to the scenario in which a language learner applies a general rule in a case that forms an exception to this rule. One of the most well-known examples, which served also as the subject of the famous past-tense debate between symbolism and connectionism (Rumelhart and McClelland, 1986; Marcus et al., 1992), concerns the rule that English past-tense verbs can be formed by appending -ed to the stem of the verb. During the acquisition of past-tense forms, learners infrequently use this rule also for irregular verbs, resulting in forms like goed (instead of went) or breaked (instead of broke).\nThe relation of overgeneralisation with compositionality comes from the supposed evidence that overgeneralisation errors provide for the presence of symbolic rules in the human language system (see, e.g. Penke, 2012). In this work, we follow this line of reasoning and take the application of a rule in a case where this is contradicted by the data as evidence that the model in fact internalised this rule. As such, we regard a model’s inclination to apply rules as the expression of a compositional bias. This inclination is most easily observed in the case of exceptions, where the correct strategy is to ignore the rules and learn on a case-by-case basis. If a model overgeneralises by applying the rules also in such cases, we hypothesise that this in particular demonstrates compositional awareness."
    }, {
      "heading" : "3.5.1 Testing overgeneralisation",
      "text" : "We propose an experimental setup where a model’s tendency to overgeneralise is evaluated by monitoring its behaviour on exceptions. We identify samples that do not adhere to the rules underlying the data distribution – exceptions – in the training data sets and assess a model’s tendency to overgeneralise by observing how they respond to these exceptions during training: (when) do they consistently follow a global rule set, and (when) do they (over)fit the training samples individually?"
    }, {
      "heading" : "4. Data",
      "text" : "As observed by many others before us, insight in the compositional skills of neural networks is not easily acquired by studying models trained on natural language directly. While it is generally agreed upon that compositional skills are required to appropriately model natural language, successfully modelling natural data requires far more than understanding compositional structures. As a consequence, a negative result may stem not from a model’s incapability to model compositionality, but rather from the lack of signal in the data that should induce compositional behaviour. A positive result, on the other hand, cannot necessarily be explained as successful compositional learning, since it is difficult to establish that a good performance cannot be reached through heuristics and memorisation. In this article, we therefore consider an artificial translation task, in which sequences that are generated by a probabilistic context free grammar (PCFG) should be translated into output sequences that represent their meanings. These output sequences are constructed by recursively applying the string edit operations that are specified in the input sequence. The task, which we dub PCFG SET, does not contain any non-compositional phenomena, and we can thus be certain that compositionality is in fact a salient feature. At the same time, we construct the input data such that in other dimensions – such as the lengths of the sentences and depths of the parse trees – it matches the statistical properties of a corpus with sentences from natural language (English)."
    }, {
      "heading" : "4.1 Input sequences: syntax",
      "text" : "The input alphabet of PCFG SET contains three types of words: words for unary and binary functions that represent string edit operations (e.g. append, copy, reverse), elements to form the string sequences that these functions can be applied to (e.g. A, B, A1, B1), and a separator to separate the arguments of a binary function (,). The input sequences that are formed with this alphabet are sequences describing how a series of such operations are to be applied to a string argument. For instance:\nrepeat A B C echo remove first D K , E F append swap F G H , repeat I J\nWe generate input sequences with a PCFG, shown in Figure 2 (production probabilities are omitted). As the grammar that we use is recursive, we can generate an infinite number of admissible input sequences. Because the operations can be nested, the parse trees of valid sequences can be arbitrarily deep and long. Additionally, the distributional properties of a particular PCFG SET data set can be controlled by adjusting the probabilities of the grammar and varying the number of input characters. We will use this to naturalise the data set such that its distribution of lengths and depths correspond to the distribution observed in a data set containing English sentences."
    }, {
      "heading" : "4.2 Output sequences: semantics",
      "text" : "The meaning of a PCFG SET input sequence is constructed by recursively applying the string edit operations specified in the sequence. This mapping is governed by the interpretation functions listed in Figure 3. Following these interpretation functions, the three sequences listed above would be mapped to output sequences as follows:\nrepeat A B C → A B C A B C echo remove first D K , E F → E F F append swap F G H , repeat I J → H G F I J I J\nThe definitions of the interpretation functions specify the systematic procedure by which an output sequence should be formed from an input sequence, without having to enumerate particular\ninput-output pairs. In this sense, PCFG SET is similar to SCAN (Lake and Baroni, 2018) but differs from a task such as the lookup table task introduced by Lǐska et al. (2018), where functions must be exhaustively defined because there is no systematic connection between arguments and the values to which functions map them."
    }, {
      "heading" : "4.3 Data construction",
      "text" : "PCFG SET describes a general framework for producing many different data sets. We used several criteria to select the PCFG SET input-output pairs for our experiments."
    }, {
      "heading" : "4.3.1 Naturalisation of structural properties",
      "text" : "The probabilistic nature of the PCFG SET input grammar offers a high level of control over the generated input sequences. We use this control to enforce an input distribution that resembles the statistics of a more natural data set in two relevant respects: the length of the expressions, and the depth of their parse trees. To obtain these statistics, we use the English side of a large machine translation corpus: WMT 2017 (Bojar et al., 2017). We parse this corpus with a statistical parser (Manning et al., 2014) and extract the distribution of length and depths from the annotated corpus. We then use expectation maximisation to tune the PCFG parameters in such a way that the resulting bivariate distribution of the generated data mimics the one extracted from the WMT data. For a more detailed description of the naturalisation procedure we refer to Appendix A. In Figure 4a and Figure 4b, we plot the distributions of the WMT data and a sample of around ten thousand sentences of the resulting PCFG SET data."
    }, {
      "heading" : "4.3.2 Sentence selection",
      "text" : "We set the size of the string alphabet to 520 and create a base corpus of around 100 thousand distinct input-output pairs. We limit the length of the string arguments given to the functions to 5. We use 85% of this corpus for training, 5% for validation and 10% for testing. During data generation, further care is taken to make memorisation as unattractive as possible by controlling the string sequences that feature as primitive arguments in the input expressions: we make sure that the same string arguments are never repeated. While we do not control re-occurrence of specific subsequence in general, the relatively large string alphabet makes it improbable that particular subsequences occur often enough to make memorisation a profitable learning strategy."
    }, {
      "heading" : "5. Architectures",
      "text" : "To showcase our compositionality test suite, we compare three currently popular neural architectures for sequence-to-sequence language processing tasks such as machine translation, speech processing and language understanding: recurrent neural networks (Sutskever et al., 2014), convolutional neural networks (Gehring et al., 2017b) and transformer networks (Vaswani et al., 2017). In this section we explain their most important features, we give a brief overview of their potential strengths and weaknesses in relation to compositionality, and we describe how we implemented them in our experiments."
    }, {
      "heading" : "5.1 LSTMS2S",
      "text" : "The first architecture we consider is a recurrent encoder-decoder model with attention. This setup is considered to be the most basic of the three setups we consider, but is still the basis of many MT applications (e.g. OpenNMT, Klein et al., 2017) and has also been successful in the fields of speech recognition (e.g. Chorowski et al., 2015) and question answering (e.g. He and Golub, 2016).\nWe consider the version of this model in which both the decoder and encoder are LSTMs and refer to this setup with the abbreviation LSTMS2S."
    }, {
      "heading" : "5.1.1 Model basics",
      "text" : "LSTMS2S is a fully recurrent, bidirectional model. The encoder processes an input by iterating over all of its elements in both directions and incrementally constructing a representation for the entire sequence. Upon receiving the encoder output, the decoder performs a similar, sequential computation to unroll the predicted sequence. Here, LSTMS2S uses an attention mechanism, which allows it to focus on the parts of the encoded input that are estimated to be most important at each moment in the decoding process.\nThe sequential fashion with which the LSTMS2S architecture processes each input potentially limits the model’s abilities to recombine components hierarchically. While depth – and, as shown by Blevins et al. (2018), thus hierarchy – can be created by stacking neural layers, the number of layers in popular recurrent sequence-to-sequence setups tends to be limited. The attention mechanism of the encoder-decoder setup positively influences the skills of LSTMS2S to hierarchically process inputs, as it allows the decoder to focus on input tokens out of the sequential order."
    }, {
      "heading" : "5.1.2 Implementation",
      "text" : "We use the LSTMS2S implementation of the OpenNMT-py framework (Klein et al., 2017). We set the hidden layer size to 512, number of layers to 2 and the word embedding dimensionality to 512, matching their best setup for translation from English to German with the WMT 2017 corpus, which we used to shape the distribution of the PCFG SET data. We use mini-batches of 64 sequences and stochastic gradient descent with an initial learning rate of 0.1."
    }, {
      "heading" : "5.2 ConvS2S",
      "text" : "The second architecture we consider is a convolutional-based architecture. Convolutional sequenceto-sequence models have obtained competitive results in machine translation (Gehring et al., 2017a)\nand abstractive summarisation (Denil et al., 2014). In this paper, we follow the setup described by Gehring et al. (2017b) and use also their nomenclature: we refer to this model with the abbreviation ConvS2S."
    }, {
      "heading" : "5.2.1 Model basics",
      "text" : "ConvS2S uses a convolutional model to encode input sequences, instead of a recurrent one. The decoder uses a multi-step attention mechanism – every layer has a separate attention mechanism – to generate outputs from the encoded input representations. Although the convolutions already contextualise information in a sequential order, the source and target embeddings are also combined with position embeddings that explicitly encode order. As at the core of the ConvS2S model lies the local mechanism of one-dimensional convolutions, which are repeatedly and hierarchically applied, ConvS2S has a built-in bias for creating compositional representations. Its topology is also biased towards the integration of local information, which may hinder modelling long-distance relations. However, convolutional networks have been found to maintain a much longer effective history than their recurrent counterparts (Bai et al., 2018). Within ConvS2S, distant portions in the input sequence may be combined primarily through the multi-step attention, which has been shown to improve the generalisation abilities of the model compared to single-step attention (Dess̀ı and Baroni, 2019)."
    }, {
      "heading" : "5.2.2 Model implementation",
      "text" : "In the ConvS2S setup that was presented by Gehring et al. (2017b) that we use in this work, word vectors are 512-dimensional. Both the encoder and decoder have 15 layers, with 512 hidden units in the first 10 layers, followed by 768 units in two layers, all using kernel width 3. The final three layers are 2048-dimensional. We train the network with the Fairseq Python toolkit9, using the predefined fconv wmt en de architecture. Unless mentioned otherwise, we use the default hyperparameters of this library. We replicate the training procedure of Gehring et al. (2017b), using Nesterov’s accelerated gradient method and an initial learning rate of 0.25. We use mini-batches of 64 sentences, with a maximum number of tokens of 3000. The gradients are normalised by the number of non-padded tokens in a batch."
    }, {
      "heading" : "5.3 Transformer",
      "text" : "The last architecture we consider is the recently introduced transformer model (Vaswani et al., 2017). Transformers constitute the current state of the art in machine translation and are becoming increasingly popular also in other domains, such as language modelling (e.g. Radford et al., 2019). We refer to this setup with simply the name Transformer."
    }, {
      "heading" : "5.3.1 Model basics",
      "text" : "Transformers use neither recurrent cells nor convolutions to convert an input sequence to an output sequence. Instead, they are fully based on a multitude of attention mechanisms. Both the encoder and decoder of a transformer are composed of a number of feed-forward layers, each containing two sub-layers: a multi-head attention module and a traditional feed-forward layer. In the multi-head attention layers, several attention tensors (the ‘heads’) are computed in parallel, concatenated and projected. In addition to a self-attention layer, the decoder has a layer that computes multi-head attention over the outputs of the encoder.\nSince transformers do not have any inherent notion of sequentiality, the input embeddings are combined with position embeddings, from which the model can infer order. For transformers, the cost of relating symbols that are far apart is thus not higher than relating words that are close together,\n9. Fairseq toolkit: https://github.com/pytorch/fairseq\nwhich – in principle – should make it easier to model long-distance dependencies. Furthermore, the relatively many stacked layers in a transformer model should facilitate modelling hierarchical structure. On the other hand, the non-sequential nature of the transformer could be a handicap as well, particularly for relating consecutive portions in the input sequence. A transformer’s receptive field is inherently global, which can be challenging in such cases."
    }, {
      "heading" : "5.3.2 Implementation",
      "text" : "We use a transformer model with an encoder and decoder that both contain six stacked layers. The multi-head self-attention module of the model has eight heads, and the feed-forward network has a hidden size of 2048. All embedding layers and sub-layers in the network produce outputs of dimensionality 512. In addition to word embeddings, positional embeddings are used to indicate word order. We use OpenNMT-py10 (Klein et al., 2017) to train the model according to the guidelines provided by the framework11: with the Adam optimiser (β1 = 0.9 and β2 = 0.98) and a learning rate increasing for the first 8000 ‘warm-up steps’ and decreasing afterwards."
    }, {
      "heading" : "6. Experiments and results",
      "text" : "We now proceed to test our tests on the three previously described architectures. Below, we describe the precise experiments we conducted and report their results, going test by test. We train all models of all architectures for 25 epochs, or until convergence, and select the best-performing model based on the performance on the validation set. For every experiment, we conduct three runs per architecture and report both the average and standard deviation of their scores.12 A summary of the results is shown in Table 1. The data and scripts to run these experiments as well as the trained models are all available online.13\nAs described before, we did not run a grid-search to optimise the hyper-parameters of the three architectures we investigate, but instead selected reasonable hyper-parameters from papers that previously used these architectures for comparable data. It is possible that changing the hyperparameters would also change the results of the experiments. It is thus important to keep in mind that the described experiments and result serve as an illustration of the usefulness of our tests. With fixed data and a varying training seed, our tests show consistent and interesting differences and similarities between the three setups we used, but these results should not be taken as general claims about LSTMs, convolutional networks or transformers."
    }, {
      "heading" : "6.1 Task accuracy",
      "text" : "We first consider the correctness of the output sequences of the three different architectures on the data as described in Section 4.3. In particular, we consider their sequence accuracy, where only instances for which the entire output sequence equals the target are considered correct. We use this accuracy measure to evaluate the overall task performance, and we use it later also for the systematicity, productivity, and overgeneralisation tests. In the rest of this paper, we denote accuracy scores with ∗.\nThe average task performance on the PCFG SET data for the three different architectures is shown in the first row of Table 1. The Transformer outperforms both LSTMS2S and ConvS2S (p ≈ 10−4 and p ≈ 10−3, respectively), with a surprisingly high accuracy of 0.92. ConvS2S, in turn, is with its 0.85 accuracy significantly better than LSTMS2S (p ≈ 10−3), which has an accuracy 0.79. The scores of the three architectures are robust with respect to initialisation and order of presentation\n10. Pytorch port of OpenNMT: https://github.com/OpenNMT/OpenNMT-py. 11. Visit http://opennmt.net/OpenNMT-py/FAQ.html for the guidelines. 12. Some experiments, such as the localism experiment, can be conducted directly on models trained for other tests\nand thus do not require training new models. 13. https://github.com/i-machine-think/am-i-compositional\nof the data, as evidenced by the low variation across runs. We now present a breakdown of this task accuracy on different types of subsets of the data."
    }, {
      "heading" : "6.1.1 Impact of length, depth and number of functions",
      "text" : "We explore how the accuracy of the three different architectures develops with increasing difficulty of the input sequences, as measured in the input sequence’s depth (the maximum level of nestedness observed in a sequence), the input sequence’s length (number of tokens) and the number of functions in the input sequence. In Figure 6, we plot the average sequence accuracy for all three architectures as a function of those difficulty measures. Unsurprisingly, the accuracy of all architecture types decreases with the length, depth and number of functions in the input. All architectures have learned to successfully model sequences with low depths and lengths and a small number of functions (reflected by accuracies close to 1). Their performance drops for longer sequences with more functions. Overall the Transformer > ConvS2S > LSTMS2S trend is preserved across the different data subsets."
    }, {
      "heading" : "6.1.2 Function difficulty",
      "text" : "Since the input sequences typically contain multiple functions, it is not possible to directly evaluate whether some functions are more difficult for models than others. On sequences that contain only one function, all models achieve a maximum accuracy. To compare the difficulty of the functions, we create one corpus with composed input sequences and derive for each function a separate corpus in which this function is applied to those composed input sequences. We then express the comparative difficulty of a function for a model as this model’s accuracy on the corpus corresponding to this function. For example, to compare the functions echo and reverse, we create two minimally different corpora that only differ with respect to the first input function in the sequence (e.g. echo append swap F G H , repeat I J and reverse append swap F G H , repeat I J), and compute the model’s accuracy on both corpora.14 We plot the results in Figure 7.\nThe ranking of functions in terms of difficulty is similar for all models, suggesting that the difficulties are to a large extent stemming from the objective complexity of the functions themselves, rather than from specific biases in the models. In some cases, it is very clear why. The function echo requires copying the input sequence and repeating its last element – regardless of the bias of the model, this should be at least as difficult as copy which requires just to copy the input. Similarly, prepend and append require repeating two string arguments, whereas for remove first and remove second only one argument needs to be repeated. The latter functions should thus be easier, irrespective of the architecture. The relative difficulty of repeat reflects that generating longer output sequences proves challenging for all architectures. As this function requires to output the input sequence twice, its output is on average twice as long as the output of another unary function applied to an input string of the same length.\nAn interesting difference between architectures occurs for the function reverse. For both LSTMS2S and ConvS2S this is a difficult function (although repeat is even harder than reverse for LSTMS2S). For Transformer, the accuracy for reverse is on par with the accuracies of echo, swap and shift, functions that are substantially easier than reverse for the other two architectures. This difference follows directly from architectural differences: while LSTMS2S and ConvS2S are forced to encode ordered local context – as they are recurrent or apply local convolutions – Transformer is not bound to such an ordering and can thus more easily deal with inverted sequences."
    }, {
      "heading" : "6.2 Systematicity",
      "text" : "The task success results for PCFG SET already reflect whether models can recombine functions and input strings that were not seen together during training. In the systematicity test, we focus\n14. Note that since inputs to unary and binary functions are different, we have to use two different corpora to compare binary and unary function difficulty. The unary and binary function scores in Figure 7 are thus not directly comparable.\nexplicitly on models’ ability to interpret pairs of functions that were never seen together while training."
    }, {
      "heading" : "6.2.1 Test details",
      "text" : "We evaluate four pairs of functions: swap repeat, append remove second, repeat remove second and append swap.15 We redistribute the training and test data such that the training data does not contain any input sequences including these specific four pairs and all sequences in the test data contain at least one. After this redistribution, the training set contains 82 thousand input-output pairs, while the test set contains 10 thousand examples. Note that while the training data does not contain any of the function pairs listed above, it still may contain sequences that contain both functions. E.g. reverse repeat remove second A B , C D cannot appear in the training set, but repeat reverse remove second A B , C D might."
    }, {
      "heading" : "6.2.2 Results",
      "text" : "The results of the systematicity test are reported in row 2 of Table 1. In Table 2, we show the average accuracies of the three architectures on all four held out function pairs. Following the task accuracy, also for the systematicity test, Transformer obtains higher scores than both LSTMS2S and ConvS2S (p ≈ 10−2 and p ≈ 10−3, respectively). The difference between the latter two, however, is for this test statistically insignificant (p ≈ 10−1). The relative differences between Transformer and the other two architectures gets larger. Intriguingly, the systematicity scores of all models are substantially lower than their overall task accuracies. This large difference is surprising, since PCFG SET is constructed such that a high task accuracy requires systematic recombination. As such, these results serve as a reminder that models may find unexpected solutions, even when the data is very carefully constructed.\nOne potential explanation for this score discrepancy is that, due to the slightly different distribution of examples in the systematicity data set, the models learn a different solution than before. Since the functions occurring in the held out pairs are slightly under-sampled, it could be that the models’ representations of these functions are not as good as the ones they develop when trained on the regular data set. A second explanation, to which our localism test will lend more support, is that models do treat the inputs and functions systematically, but analyse the sequences in terms of different units. Obtaining a high accuracy for PCFG SET undoubtedly requires being able to systematically recombine functions and input strings, but it does not necessarily require developing separate representations that capture the semantics of the different functions individually.\nFor instance, if there is enough evidence for repeat copy, a model may learn to directly apply the combination of these two functions to an input string, rather than consecutively appealing to separate representations for the two functions. Thus, to compute the output of a sequence like repeat copy swap echo X, the model may apply two times a pair of functions, instead of four separate functions. Such a strategy would not necessarily harm performance in the overall data set, since plenty of evidence for all function pairs is present, but it would affect performance on the systematicity test, where this is not the case. While larger chunking to ease processing is not necessarily a bad strategy, we argue that it is desirable if models can also maintain a separate representation of the units that make up such chunks, which may be needed in other contexts."
    }, {
      "heading" : "6.3 Productivity",
      "text" : "In Figure 6, we saw that longer sequences are more difficult for all models, even if their length and depth fall within the range of lengths and depths observed in the training examples. There\n15. To decrease the number of dimensions of variation, we keep the specific pairs of functions fixed during evaluation: rather than varying the function pairs evaluated across runs, we vary the initialisation and order of presentation of the training examples.\n.\nare several potential causes for this drop in accuracy. It could be that longer sequences are simply more difficult than shorter ones: they contain more functions, and there is thus more opportunity to make an error. Additionally, simply because they contain more functions, longer sequences are more likely to contain at least one of the more difficult functions (see Figure 7). Lastly, due to the naturalisation of the distribution of lengths, longer sequences are underrepresented in the training data. There is thus fewer evidence for long sequences than there is for shorter ones. As such, models may have to perform a different kind of generalisation to infer the meaning of longer sequences than they do for shorter ones. Their decrease in performance when sequences grow longer could thus also be explained by a general poor ability to generalise to lengths outside their training space, a type of generalisation sometimes referred to with the term extrapolation. With our productivity test, we focus purely on this extrapolation aspect, by studying models’ ability to successfully generalise to longer sequences, which we will call the model’s productive power."
    }, {
      "heading" : "6.3.1 Test details",
      "text" : "To test for productivity, we redistribute the training and testing data such that there is no evidence at all for longer sequences in the training set. Sequences containing up to eight functions are collected in the training set, consisting of 81 thousand sequences, while input sequences containing at least nine functions are used for evaluation and collected in a test set containing 11 thousand sequences. The average, minimum and maximum length, depth and number of functions for the train and test set of the productivity test are shown in Table 3."
    }, {
      "heading" : "6.3.2 Results",
      "text" : "The overall accuracy scores on the productivity test in Table 1 demonstrate that all models have great difficulty with extrapolating to sequences with a higher length than those seen during training.\nTransformer drops to a mean accuracy of 0.50; LSTMS2S and ConvS2S have a test accuracy of 0.30 and 0.31, respectively. Relatively speaking, removing evidence for longer sequences thus resulted in a 62% drop for LSTMS2S, a 64% drop in ConvS2S, and a 46% drop for Transformer. Both in terms of absolute and relative performance, Transformer thus has a much greater productive potential than the other models, although its absolute performance is still poor.\nComparing just the task accuracy and productivity accuracy of models shows that models have difficulty with longer sequences but does still not give a definitive answer about the source of the performance decrease. Since the productivity test set contains on average longer sequences, we cannot see if the drop in performance is caused by poor productive power or by the inherent difficulty of longer sequences. In Figure 8, we show the performance of the three models in relation to depth, length and number of functions of the input sequences (blue lines) compared with the task accuracy of the standard PCFG SET test data for the same lengths as plotted in Figure 6. For all models, the productivity scores are lower for almost every depth, length and number of functions. This decrease in performance is solely caused by the decrease in evidence for such sequences: the total number of examples that models were trained on is roughly the same across the two conditions, and the absolute difficulty of the longer sequences is as well. With these two components factored out, we conclude that models in fact struggle to productively generalise to longer sequences.16\nThe depth plot in Figure 6 also provides some evidence for the inherent difficulty of deeper functions: it shows that all models suffer from decreasing test accuracies for higher depths, even if these depths are well-represented in the training data. When looking at the number of functions, the productivity score of Transformer is worse than its overall task success for any considered number of functions. The scores for LSTMS2S and ConvS2S are instead very similar to the ones they reached after training on the regular data. This shows that functions with high depths are difficult for LSTMS2S and ConvS2S, even when some of them are included in the training data.\nInterestingly, considering only the development of the productivity scores (in blue), it appears that both the LSTMS2S and ConvS2S are relatively insensitive to the increasing length as measured by the number of tokens. Their performance is just as bad for input sequences with 20 or 50\n16. To stop their generation of the answer, models have to explicitly generate an end of sequence symbol (<eos>). A reasonable hypothesis concerning the low scores on longer sequences is that they are due to models’ inability to postpone the emission of this <eos> symbol. Following Dubois et al. (2019), we call this problem the <eos>problem. To test whether the low scores are due to early <eos> emissions, we compute how many of the wrongly emitted answers were contained in the right answer. For LSTMS2S, ConvS2S and Transformer this was the case in 22%, 6% and 8% of the wrong predictions. These numbers illustrate that the <eos>-problem indeed exists, but is not the main source of the poor productive capacity of the different models.\ncharacters, which is on a par with the scores they obtain on the longest sequences after training on the regular data. Apparently, shorter sequences of unseen lengths are as challenging for these models as sequences of extremely long lengths. Later, in the localism experiment, we will find more evidence that this sharp difference between seen and unseen lengths is not accidental for LSTMS2S but characteristic for the representations learned by this architecture."
    }, {
      "heading" : "6.4 Substitutivity",
      "text" : "While the previous two experiments were centred around models’ ability to recombine known phrases and rules to create new phrases, we now focus on the extent to which models are able to draw analogies between words. In particular, we study under what conditions models treat words as synonyms: we consider what happens when synonyms are equally distributed in the input sequences and when one of the synonyms only occurs in primitive contexts."
    }, {
      "heading" : "6.4.1 Test details",
      "text" : "We select two binary and two unary functions (swap, repeat, append and remove second), for which we artificially introduce synonyms during training: swap syn, repeat syn, append syn and remove second syn. Like in the systematicity test, we keep those four functions fixed across all experiments, varying only the model initialisation and order of presentation of the training data. The introduced synonyms have the same interpretation functions as the terms they substitute, so they are semantically equivalent to their counterparts. We consider two different conditions that differ in the syntactic distribution of the synonyms in the training data.\nEqually distributed synonyms For the first substitutivity test we randomly replace half of the occurrences of the chosen functions F with Fsyn, keeping the target constant. On average, the individual functions appeared in 39% of the training samples. After synonym substitution, they appear in approximately 19% of the training samples, on average. In this test, F and Fsyn are distributionally similar, which should facilitate inferring that they are synonyms.\nPrimitive synonyms In the second and more difficult substitutivity test, we introduce Fsyn only in primitive contexts, where F is the only function call in the input sequence. Fsyn is introduced in 0.1% of the training set samples. In this primitive condition, the function F and its synonymous counterpart Fsyn are distributionally not equivalent\nEvaluation For the substitutivity test, we do not evaluate models’ accuracy but assess their robustness to meaning-invariant synonym substitutions in the input sequence. The most important point is not whether a model correctly predicts the target for an adapted input sequence, but whether its prediction matches the prediction it made before the transformation. We evaluate models based on this interchangeability of F with Fsyn. We quantify this with a consistency score, which expresses a pairwise equality, where a model’s outputs on two different inputs are compared to each other, instead of to the target output. As with accuracy, also here only instances for which there is a complete match between the compared outputs are considered correct.\nThe consistency metric allows us to evaluate compositionality aspects isolated from task performance. Even for models that may not have a near-perfect task performance and therefore have not mastered the rules underlying the data, we want to evaluate whether they consistently apply and generalise the knowledge they did acquire. We use the consistency score for the current substitutivity test and later for the localism tests. In the next sections, consistency scores are marked with †."
    }, {
      "heading" : "6.4.2 Equally distributed substitutions",
      "text" : "For the substitutivity experiment where words and synonyms are equally distributed, Transformer and ConvS2S perform nearly on par. They both obtain a very high consistency score (0.98 and\n0.95, respectively). In Table 4, we see that both architectures put words and their synonyms closely together in the embedding space, truly respecting the distributional hypothesis. Surprisingly, LSTMS2S does not identify that two words are synonyms, even in this relatively simple condition where the words are distributionally identical. Words and synonyms are at very distinct positions in the embedding space, although the distance between them is smaller than the average between all words in the embedding space. We hypothesise that this low score of the LSTMS2S reflects the architecture’s inability to draw the type of analogies required to model PCFG SET data, which is also mirrored in its relatively low overall task accuracy."
    }, {
      "heading" : "6.4.3 Primitive substitutions",
      "text" : "The primitive substitutivity test is substantially more challenging than the equally distributed one, since models are only shown examples of synonymous expressions in a small number of primitive contexts. This implies that words and their synonyms are no longer distributionally similar and that models are provided much fewer evidence for the meaning of synonyms, as there are simply fewer primitive than composed contexts.\nWhile the consistency scores for all models decrease substantially compared to the equally distributed setup, all models do pick up that there is a similarity between a word and its synonym. This is reflected not only in the consistency scores (0.60, 0.58 and 0.90 on average for LSTMS2S, ConvS2S and Transformer, respectively), but is also evident from the distances between words and their synonyms, which are substantially lower than the average distances to other function embeddings (Table 4). For LSTMS2S, the average distance is very comparable to the average distance observed in the equally distributed setup. Its consistency score, however, goes down substantially, indicating that word distances (computed between embeddings) give an incomplete picture of how well models can account for synonymity when there is a distributional imbalance.\nSynonymity vs few-shot learning The consistency score of the primitive substitutivity test reflects two skills that are partly intertwined: the ability to few-shot learn the meanings of words from very few samples and the ability to bootstrap information about a word from its synonym. As already observed in the equally distributed experiment for LSTMS2S, it is difficult to draw hard conclusions about a model’s ability to infer synonymity when it is not able to infer consistent meanings of words in general. When a model has a high score, on the other hand, it is difficult to disentangle if it achieved this high score because it has learned the correct meaning of both words separately, or because it has in fact understood that the meaning of those words is similar. That\nis: the consistency score does not tell us whether output sequences are identical because the model knows they should be the same, or simply because they are both correct. In the equally distributed setup, the low word embedding distances for the ConvS2S and the Transformer strongly pointed to the first explanation. For the primitive setup, the two aspects are more difficult to take apart.\nError consistency To separate a model’s ability to few-shot learn the meaning of a word from very few primitive examples and its ability to bootstrap information about synonyms , we compute the consistency score for model outputs that do not match the target output (incorrect outputs). When a model makes identical but incorrect predictions for two input sequences with a synonym substitution, this cannot be because the model merely correctly learned the meanings of the two words. It can thus be taken as evidence that it treats the word and its synonyms indeed as synonyms.\nIn Table 5, we show the consistency scores for all output pairs (identical to the scores in Table 1), the breakdown of this score into correct (consistent correct) and incorrect (consistent incorrect) output pairs, and the ratio of incorrect output pairs that is consistent. The scores in row two and three show that the larger part of the consistency scores for all models is due to correct outputs. In row 4, we see that models are seldom consistent on incorrect outputs. The Transformer maintains its first place , but none of the architectures can be said to treat a word and its synonymous counterpart as true synonyms. An interesting difference occurs between LSTMS2S and ConvS2S, whose consistency scores on all outputs are similar, but differ in consistency of erroneous outputs. These scores suggest that ConvS2S is better at few-shot learning than LSTMS2S, but LSTMS2S is better at inferring synonymity. These results are in line with the embedding distances shown for the primitive substitutivity experiment in Table 4, which are on average also lower for LSTMS2S than for ConvS2S."
    }, {
      "heading" : "6.5 Localism",
      "text" : "In the localism test, we investigate whether models compute the meanings of input sequences using local composition operations, following the hierarchical trees that specify their compositional structure."
    }, {
      "heading" : "6.5.1 Test details",
      "text" : "We test for localism by considering models’ behaviour when a subsequence in an input sequence is replaced with its meaning (see Figure 9 for an example). Thanks to the recursive nature of the PCFG SET expressions and interpretation functions, this is a relatively straightforward substitution in our data. If a model uses local composition operations to build up the meanings of input sequences,\nfollowing the hierarchy that it is dictated by the underlying system, its output meaning should not change as a consequence of such a substitution.\nUnrolling computations We compare the output sequence that is generated by a model for a particular input sequence with the output sequence that the same model generates when we explicitly unroll the processing of the input sequence. That is, instead of presenting the entire input sequence to the model at once, we force the model to evaluate the outcome of smaller constituents before computing the outcome of bigger ones, in the following way: we iterate through the syntactic tree of the input sequence and use the model to compute the meanings of the smallest constituents. We then replace these constituents by the model’s output and use the model to again compute the meanings of the smallest constituents in this new tree. This process is continued until the meaning for the entire sequence is found. A concrete example is visualised in Figure 9.\nWe conduct the localism test on sentences from the PCFG SET test set. On average, unrolling the computation of these sequences involves five steps.\nEvaluation We evaluate a model by comparing the final output of the enforced recursive method to the output emitted when the sequence is presented in its original form. Again, during evaluation we focus on checking whether the two outputs are identical, rather than if they are correct. If a model wrongfully emits B A for input sequence prepend B , A, this is not penalised in this experiment, provided that the regular input sequence yields the same prediction as its hierarchical variant. This method of evaluation matches the previously mentioned consistency score that was also used in the previous section for the substitutivity test."
    }, {
      "heading" : "6.5.2 Results",
      "text" : "None of the evaluated architectures obtains a high consistency score for this experiment (0.46, 0.59 and 0.54 for LSTMS2S, ConvS2S and Transformer, respectively). Also in this test, Transformer ranks high, but the best-performing architecture is ConvS2S (significant in comparison with both LSTMS2S and Transformer with p ≈ 10−4 and p ≈ 10−2, respectively). Since the ConvS2S models are explicitly using local operations, this is in line with our expectations.\nInput string length To understand the main cause of the relatively low scores on this experiment, we manually analyse 300 samples (100 per model type), in which at least one mistake was made during the unrolled processing of the sample. We observe that the most common mistakes involve unrolled samples that contain function applications to string inputs with more than five characters. An example of such a mistake would be a model that is able to compute the meaning of reverse echo A B C D E but not the meaning of reverse A B C D E E. As the outputs for these two phrases are identical, it is clear that this inadequacy does not stem from models’ inability to generate the correct output string. Instead, it indicates that the model does not compute the meaning of reverse echo A B C D E by consecutively applying the functions echo and reverse. We hypothesise that, rather, models generate representations for combinations of functions that are then applied to the input string at once.\nFunction representations While developing ‘shortcuts’ to apply combinations of functions all at once instead of explicitly unfolding the computation does not necessarily contradict compositional understanding – imagine, for instance, computing the outcome of the sum 5 + 3 - 3 – the results of the localism experiment do point to an interesting aspect of the learned representations. Since unrolling computations mostly leads to mistakes when the character length of unrolled inputs is longer than the maximum character string length of five seen during training, it casts some doubt on whether the models have developed consistent function representations.\nIf a model truly understands the meaning of a particular function in PCFG SET, it should in principle be able to apply this function to an input string of arbitrary length. Note that, in our case, this ability does not require productivity in generating output strings, since the correct output sequences are not distributionally different from those in the training data (in some cases, they may even be exactly the same). Contrary to in other setups, a failure to apply functions to longer sequence lengths can thus not be explained by distributional or memory arguments. Therefore, the consistent failure of all models to apply functions to character strings that are longer than the ones seen in training suggests that, while models may have learned to adequately copy strings of length two to five, they do not necessarily consider those operations the same.\nTo check this hypothesis, we test all functions in a primitive setup where we vary the length of the string arguments they are applied to.17 For a model that develops several length-specific representations for the same function, we expect the performance to go down abruptly when the input string length exceeds the maximum length seen during training. If a model instead develops a more general representation, it should be able to apply learned functions also to longer input strings. Its performance on longer strings may drop for other, practical, reasons, but this drop should be more smooth than for a model that has not learned a general-purpose representation at all.\n17. For binary functions, only one of the two string arguments exceeds the regular argument lengths.\nThe results of this experiment, plotted in Figure 10, demonstrate that all models have learned to apply all functions to input strings up until length five, as evidenced by their near-perfect accuracy on the samples of these lengths. On longer lengths, however, none of the models performs well. For all runs, the performance of LSTMS2S immediately drops to zero when string arguments exceed length five, the maximum string length seen during training. The model does not seem to be able to leverage a general concept of any of the functions. ConvS2S and Transformer do exhibit some generalisation beyond the maximum string input length seen during training, indicating that their representations are more general. The accuracy of Transformer reaches zero only for input arguments of more than nine characters, ConvS2S outputs some correct responses even for input arguments of 12 or 13 characters. This suggests that the descending scores may be due to factors of ‘performance’ rather than ‘competence’. The accuracies for Transformer and ConvS2S are comparable for almost all functions, except reverse, for which the ConvS2S accuracy drops to almost zero for length six in all three runs. Interestingly, none of the three architectures suffers from increasing the character length of the first and second argument to remove first and remove second, respectively (not plotted)."
    }, {
      "heading" : "6.6 Overgeneralisation",
      "text" : "In our last test, we focus on the learning process, rather than on the final solution that is implemented by converged models. In particular, we study if – during training – a model overgeneralises when it is presented with an exception to a rule and – in case it does – how much evidence it needs to see to memorise the exception. Whether a model overgeneralises indicates its willingness to prefer rules over memorisation, but while strong overgeneralisation characterises compositionality, more overgeneralisation is not necessarily better. An optimal model, after all, should be able to deal with exceptions as well as with the compositional part of the data."
    }, {
      "heading" : "6.6.1 Test details",
      "text" : "As the language defined through the PCFG is designed to be strictly compositional, it does not contain exceptions. We therefore manually add them to the data set, which allows us to have a large control over their occurrence and frequency.\nExceptions We select four pairs of functions that are assigned a new meaning when they appear together in an input sequence: reverse echo, prepend remove first, echo remove first and prepend reverse. Whenever these functions occur together in the training data, we remap the meaning of those functions, as if an alternative set of interpretation functions is used in these few cases. As a consequence, the model has no evidence for the compositional interpretation of these function pairs, unless it overgeneralises by applying the rule observed in the rest of the training data. For example, the meaning of echo remove first A , B C would normally be B C C, but has now become A B C. The remapped definitions, which we call exceptions, can be found in Table 6.\nException frequency In our main experiment, the number of exceptions in the data set is 0.1% of the number of occurrences of the least occurring function of the function pair F1F2. We present also the results of a grid-search in which we consider exception percentages of 0.01%, 0.05%, 0.1% and 0.5%."
    }, {
      "heading" : "6.6.2 Results",
      "text" : "We monitor the accuracy of both the original and the exception targets during training and compare how often a model correctly memorises the exception target and how often it overgeneralises to the compositional meaning, despite the evidence in the data. To summarise a model’s tendency to overgeneralise, we take the highest overgeneralisation accuracy that is encountered during training. For more qualitative analysis, we visualise the development of both memorisation and overgeneralisation\nduring training, resulting in overgeneralisation profiles. During training, we monitor the number of exception samples for which a model does not generate the correct meaning, but instead outputs the meaning that is in line with the rule instantiated in the rest of the data. At every point in training, we define the strength of the overgeneralisation as the percentage of exceptions for which a model exhibits this behaviour.\nOvergeneralisation peak We call the point in training where the overgeneralisation is highest the overgeneralisation peak. In Table 1, we show the average height of this overgeneralisation peak for all three architectures, using an exception percentage of 0.1%. This quantity equals the accuracy of the model predictions on the input sequences whose outputs have been replaced by exceptions, but measured on the original targets that follow from the interpretation functions of PCFG SET. The numbers in Table 1 illustrate that all models show a rather high degree of overgeneralisation. At some point during the learning process, Transformer applies the rule to 88% of the exceptions and LSTMS2S and ConvS2S to 68% and 79% respectively.\nOvergeneralisation profile More interesting than the height of the peak is the profile that different architectures show during learning. In Figure 11, we plot this profile for four different exception percentages. The lower areas (in red), indicate the overgeneralisation strength, whereas the memorisation strength – the accuracy of a model on the adapted outputs, which can only be learned by memorisation – is indicated in the upper part of the plots, in blue. The grey area in between indicates the percentage of exception examples for which a model outputs neither the correct answer nor the rule-based answer.\nException percentage The profiles show that, for all architectures, the degree of overgeneralisation strongly depends on the number of exceptions present in the data. All architectures show overgeneralisation behaviour for exception percentages lower than 0.5% (first three rows), but hardly any overgeneralisation is observed when 0.5% of a function’s occurrence is an exception (bottom row). When the percentage of exceptions becomes too low, on the other hand, all models have difficulties memorising them at all: when the exception percentage is 0.01% of the overall function occurrence, only ConvS2S can memorise the correct answers to some extent (middle column, top row). LSTMS2S and Transformer keep predicting the rule-based output for the sequences containing exceptions, even after the training converged.\nLearning an exception LSTMS2S, in general, appears to find it difficult to accommodate both rules and exceptions at the same time. Transformer and ConvS2S overgeneralise at the beginning of training, but then, once enough evidence for the exception is accumulated, gradually change to predicting the correct output for the exception sequences. This behaviour is most strongly present for ConvS2S, as evidenced by the thinness of the grey stripe separating the red and the blue area during training. For LSTMS2S, on the other hand, the decreasing overgeneralisation strength is not\nmatched by an increasing memorisation strength. After identifying that a certain sequence is not following the same rule as the rest of the corpus, LSTMS2S does not predict the correct meaning but instead starts generating outputs that match neither the correct exception output nor the original target for the sequence. After convergence, its accuracy on the exception sequences is substantially lower than the overall corpus accuracy. As the bottom plot (with an exception percentage of 0.5%) indicates that LSTMS2S models do not have problems with learning exceptions per se, they appear to struggle with hosting exceptions for words if little evidence for such anomalous behaviour is present in the training data."
    }, {
      "heading" : "7. Discussion",
      "text" : "With the rising successes of models based on deep learning, evaluating the compositional skills of neural network models has attracted the attention of many researchers. Many empirical studies have been presented that evaluate the compositionality of neural models in different ways, but they have\nnot led to a consensus about whether neural models can in fact adequately model compositional data. We argue that this lack of consensus stems from a deeper issue than the results of the proposed tests: while many researchers have a strong intuition about what it means for a model to be compositional, there is no explicit agreement on what defines compositionality of a model or how it should be tested for in a neural model."
    }, {
      "heading" : "7.1 An evaluation framework to evaluate compositionality",
      "text" : "In this paper, we proposed an evaluation framework that addresses this problem, with a series of tests that translate theoretical concepts related to compositionality of language into behavioural tests for models of language. Our evaluation framework contains five independent tests that consider complementary aspects of compositionality that are frequently mentioned in the literature about compositionality. These five tests allow us to investigate (i) if models systematically recombine known parts and rules (systematicity) (ii) if models can extend their predictions beyond the length they have seen in the training data (productivity) (iii) if models’ predictions are robust to synonym substitutions (substitutivity) (iv) if models’ composition operations are local or global (localism) and (v) if models favour rules or exceptions during training (overgeneralisation). We formulated these tests on a task-independent level, disentangled from a specific downstream task. With this, we offer a versatile evaluation paradigm which can be used to evaluate the compositional abilities of a model on five different levels, that can be instantiated for any chosen sequence-to-sequence task. Importantly, our collection of tests should not be taken as a normative specification of what models should and should not do. Rather, they are meant to discover which aspects of compositionality a model does or does not implement and learn more about a model’s strengths and weaknesses.\nTo showcase our evaluation paradigm, we instantiated the five tests on a highly compositional artificial data set we dub PCFG SET: a sequence-to-sequence translation task which requires to compute meanings of sequences that are generated by a probabilistic context-free grammar by recursively applying string edit operations. This data set is designed such that modelling it adequately should require a compositional solution, and it is generated such that its length and depth distributions match those of a natural corpus of English. We then used these instantiated tests to compare three popular sequence-to-sequence architectures: an LSTM-based (LSTMS2S ), a convolution-based (ConvS2S ) and an all-attention model (Transformer). For each test, we conducted a number of auxiliary tests that can be used to further increase the understanding of how this aspect is treated by a particular architecture. Below, we provide a summary of the results of these experiments.18"
    }, {
      "heading" : "7.2 Summary of results",
      "text" : "While the overall accuracy on PCFG SET was relatively high for all models, a more detailed picture is given by the five compositionality tests. These tests indicated that, despite our careful data design, high scores do still not necessarily imply that the trained models fully represent the true underlying generative system and illustrated how different models handle different aspects that could be considered important for compositional learning.\nFirstly, our systematicity test showed that none of the architectures successfully generalises to pairs of words that were not observed together during training, a result that confirms earlier studies such as the ones from Loula et al. (2018) and Lake and Baroni (2018). The difference between the systematicity scores and the overall task accuracy is quite stark for all models: a drop of 33%, 34% and 22% for LSTMS2S, ConvS2S and Transformer, respectively. This suggests that the low\n18. At the risk of being redundant, we repeat that these results should not be taken as general claims about LSTMs, convolutional networks or transformers. Neural models can be sensitive to small changes in hyper-parameters and learning regimes and we did not investigate the effect of changing the hyper-parameters. Perhaps using a deeper LSTM, a transformer with more attention heads, or convolutions with a wider kernel width would show different patterns. We leave these questions open for future work. With the results below, we merely want to show that – keeping the tests fixed – interesting differences but also similarities between different models can be found.\naccuracy on the systematicity test does not stem from poor systematic capacity in general, but that rather from the fact that the models use different segmentations of the input, applying – for instance – multiple functions at once, instead of all of the functions in a sequential manner. While larger chunking to ease processing is not necessarily a bad strategy, it is desirable if models can also maintain a separate representation of the units that make up such chunks, as these units could be useful or needed in other sequences.\nWith our productivity test, we assessed if models can productively generalise to sequences that are longer than the ones they observed in training. To evaluate this, we redistributed the training examples such that there is a strict separation of the input sequence lengths in the train and test data. To tease apart the overall difficulty of modelling longer sequences from the ability to generalise to unseen lengths, we compared the results with the accuracies of models that are trained on data sets that contain at least some evidence for longer sequences. None of the architectures exhibited strong productive power to sequences of unseen lengths. By computing how often models’ predictions were strictly contained within the true output sequence, we assess if the poor productive power of all models is caused by early emission of the end-of-sequence symbol. We find that such cases indeed exist, but that early stopping of the generation is not the main cause of the low productivity scores.\nWith our substitutivity test, we compared how models react to artificially introduced synonyms occurring in different types of scenarios. Rather than considering their behaviour in terms of sequence accuracy, in this test, we computed how consistent models’ predictions are – correct or incorrect – when a word is substituted with a synonym. When synonyms are equally distributed in the input data, both Transformer and ConvS2S obtain high consistency scores, while LSTMS2S is substantially less consistent. This difference is also reflected in the distance between the embeddings of words and synonyms, which is much lower for Transformer and ConvS2S. When one of the synonyms is only presented in a few very short sequences, the consistency score of ConvS2S drops to the same level as LSTMS2S, while Transformer still maintains a relatively high synonym consistency. Also the embeddings of synonyms remain relatively close in Transformer models’ embedding space, despite the fact that they are distributionally dissimilar.\nTo tease apart the ability to learn from very few examples and to infer synonymity, we also considered how consistent models are on incorrect outputs. Here, we observed that none of the models can be said to truly treat words and their counterparts as synonyms. Transformer is the most consistent, but with a low score of only 0.34. This test shows an interesting difference between LSTMS2S and ConvS2S: where the former appears to be better at inferring that words are synonyms, the latter is better at few-shot learning a word’s meaning from very few examples.\nWith our localism test, we considered if models apply local composition operations that are true to the syntactic tree of an input sequence, or rather compute the meaning of a sequence in a more global fashion. In line with the results of the systematicity test, models do not appear to truly follow the syntactic tree of the input to compute its meaning. In 54%, 41% and 46% of the test samples for LSTMS2S, ConvS2S and Transformer, respectively, enforcing a local computation results in a different answer than the original answer provided by the model. An error analysis suggests that these results are largely due to function applications to longer string sequences. With an additional test in which we monitored the accuracy of models on functions applied to increasingly long string inputs, we find evidence that models may not learn general-purpose representations of functions, but instead use different protocols for copy once or copy twice. We saw that the accuracy of LSTMS2S immediately drops to 0 when string inputs are longer than the ones observed in training. The performance of ConvS2S and Transformer, instead, drops rapidly, but remains above 0 for slightly longer string inputs. These results indicate that LSTMS2S may indeed not have learned a generalpurpose representation for functions, while the decreasing accuracy of ConvS2S and Transformer could be related more to performance rather than competence issues.\nIn our last test, we studied overgeneralisation during training, by monitoring the behaviour of models on artificially introduced exceptions to rules for four function pairs. We found that for small amounts of exceptions (up to 0.1% of the number of occurrences of the least occurring\nfunction of a function pair) all architectures overgeneralise at the beginning of their training. As overgeneralisation implies that models overextend rules in cases where this is explicitly contradicted by the data, we take this as a clear indication that models in fact capture the underlying rule at that point. For very small amounts of exceptions (0.01%), both Transformer and LSTMS2S failed to learn the exception at all: even after their training has converged they overgeneralise on the sequences containing exceptions. To a lesser extent, also ConvS2S struggles with capturing exceptions that have a low frequency. LSTMS2S generally appears to have difficulty with accommodating both rules and exceptions. Often, after learning that a certain rule should not be applied, LSTMS2S models do not memorise the true target but proceed to predict something which matches neither this target nor the general rule. ConvS2S and Transformer do not show such patterns: when their overgeneralisation score goes down, their memorisation score goes up. Aside from at the beginning of their training, they rarely predict something outside of these options. For larger percentages of exceptions (from 0.5%), none of the architectures really exhibits overgeneralisation behaviour."
    }, {
      "heading" : "7.3 Conclusion and future work",
      "text" : "With a proposed collection of tests, we aimed to cover several facets of compositionality. We believe that as such, this collection of tests can serve as an evaluation paradigm to probe the ability of different neural network architectures in the light of compositionality. We hope that the tests and their results can help facilitate a general discussion of what it means for neural models to be compositional and what we would like them to represent. There are, of course, also aspects of compositionality that we did not cover. We therefore do not consider our evaluation an endpoint, but rather a stepping stone on the way, which we hope can provide the grounds for a clearer discussion concerning the role and importance of compositionality in neural networks, including both aspects that we did and did not include.\nWe instantiated our tests on an artificial data set that is entirely explainable in terms of compositional phenomena. This permitted us to focus on the compositional ability of different models in the face of compositional data and allowed us to isolate compositional processing from other signals that are found in more realistic data sets. However, it leaves open the question of how much the compositional traits we identified are expressed and can be exploited by networks when facing natural data. Despite the fact that they are not informed by knowledge of language or semantic composition, neural networks have achieved tremendous successes in almost all natural language processing tasks. While their performance is still far from perfect, it is not evident that their remaining failures stem from their inability to deal with compositionality. In the future, we plan to instantiate our tests also in natural language domains such as translation and summarisation. The results of such a study would provide valuable information about how well models pick up compositional patterns in more noisy environments, but – perhaps even more importantly – could also provide insights about the importance of these different aspects of compositionality to model natural data.\nIn summary, we provided an evaluation paradigm that allows a researcher to test the extent to which five distinct, theoretically motivated aspects of compositionality are represented by artificial neural networks. By instantiating these tests for an artificial data set and applying the resulting tests on three different successful sequence-to-sequence architectures, we shed some light on which aspects of compositionality may prove problematic for different architectures. These results illustrate well that to test for compositionality in neural networks it does not suffice to consider an accuracy score on a single downstream task, even if this task is designed to be highly compositional. Models may capture some compositional aspects of this data set very well, but fail to model other aspects that could be considered part of a compositional behaviour. As such, the results themselves demonstrate the need for the more extensive set of evaluation criteria that we aim to provide with this work. We hope that future researchers will use our collection of tests to evaluate new models, to investigate the impact of hyper-parameters or to study how compositional behaviour is acquired during training. To facilitate the usage of our test suite we have made the PCFG SET data generator, all test sets\nand the models trained by us available online.19 We further hope that our theoretical motivation, the tests themselves and the analysis that we presented of its application on three different sequenceto-sequence architectures will prove to be a step in the direction of having a clearer discussion about compositionality in the context of deep learning, both from a practical and a theoretical perspective."
    }, {
      "heading" : "Acknowledgments",
      "text" : "We thank Marco Baroni, Yoav Goldberg, Aureli Herbelot, Louise McNally, Ryan Nefdt, Sandro Pezzelle, Shane Steinert-Threlkeld and Willem Zuidema for taking the time to proofread earlier versions of this paper and giving us feedback. Furthermore, we thank our anonymous reviewers and editor Stephen Clark for their interesting comments and helpful suggestions.\nDieuwke Hupkes is funded by the Netherlands Organization for Scientific Research (NWO), through a Gravitation Grant 024.001.006 to the Language in Interaction Consortium. Elia Bruni is funded by the European Union’s Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement No 790369 (MAGIC)."
    }, {
      "heading" : "Appendix A. Naturalisation of artificial data",
      "text" : "The artificially generated PCFG SET data are transformed so as to mimic the distribution of a natural language data set according to the following procedure:\n1. Use a natural language data set DN , define a set of features F , and for each f ∈ F , compute the value f(s) for each sentence s ∈ DN .\n2. Generate a large sample DR of PCFG SET data using random probabilities on production rules for each instance.\n3. Transform DR as follows:\n(i) For each feature f ∈ F , specify a feature increment if . (ii) For each s ∈ DN , compute the partitioning vector v(s), which is the concatenation of the\nvalues bf(s)/ifc for each feature f ∈ F . (iii) Partition DN into subsets by clustering instances with the same partitioning vector. For\nany such subset DiN , let v(DiN ) denote the partitioning vector of its members. And for any partitioning vector v, let v−1N (v) denote the subset DiN ⊆ DN whose members have partitioning vector v (so that v(DiN ) = v).\n(iv) Of the identified subsets, determine the largest set DiN ⊆ DN . Call this set DmaxN . (v) Partition DR in the same way as DN , yielding subsets DiR. Let the subset DiR such that v(DiR) = v(DmaxN ) be DmaxR . (vi) Initialise an empty set D′R.\n(vii) Of each DiR, randomly pick |v−1N (v(D i R))|×|D max R |\n|DmaxN | members, and assign them to D′R.\n(viii) If necessary, repeat (i) - (vii) for different feature increments fi. For n features, fit an n-variate Gaussian to each of the transformed sets D′R. Choose the set with the lowest Kullback-Leibler divergence from the n-variate Gaussian approximation of DN .\n4. Use maximum likelihood estimation to estimate the PCFG parameters of D′R and generate more PCFG SET data using these parameters.\n5. If necessary, apply step 3 to the data thus generated."
    } ],
    "references" : [ {
      "title" : "Measuring compositionality in representation learning",
      "author" : [ "J. Andreas" ],
      "venue" : "Proceedings of the 7th International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Andreas,? 2019",
      "shortCiteRegEx" : "Andreas",
      "year" : 2019
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "D. Bahdanau", "K. Cho", "Y. Bengio" ],
      "venue" : "Proceedings of the 3rd International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Bahdanau et al\\.,? 2015",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2015
    }, {
      "title" : "Systematic generalization: What is required and can it be learned",
      "author" : [ "D. Bahdanau", "S. Murty", "M. Noukhovitch", "T.H. Nguyen", "H. de Vries", "A. Courville" ],
      "venue" : "In Proceedings of the 6th International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2018
    }, {
      "title" : "An empirical evaluation of generic convolutional and recurrent networks for sequence modeling",
      "author" : [ "S. Bai", "J.Z. Kolter", "V. Koltun" ],
      "venue" : "CoRR, abs/1803.0127.",
      "citeRegEx" : "Bai et al\\.,? 2018",
      "shortCiteRegEx" : "Bai et al\\.",
      "year" : 2018
    }, {
      "title" : "Nouns are vectors, adjectives are matrices: representing adjective-noun constructions in semantic space",
      "author" : [ "M. Baroni", "R. Zamparelli" ],
      "venue" : "Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1183–1193.",
      "citeRegEx" : "Baroni and Zamparelli,? 2010",
      "shortCiteRegEx" : "Baroni and Zamparelli",
      "year" : 2010
    }, {
      "title" : "Artificial evolution of syntactic aptitude",
      "author" : [ "J. Batali" ],
      "venue" : "Proceedings of the 16th Annual Conference of the Cognitive Science Society (CogSci), pages 27–32.",
      "citeRegEx" : "Batali,? 1994",
      "shortCiteRegEx" : "Batali",
      "year" : 1994
    }, {
      "title" : "Evaluating layers of representation in neural machine translation on part-of-speech and semantic tagging tasks",
      "author" : [ "Y. Belinkov", "L. Màrquez", "H. Sajjad", "N. Durrani", "F. Dalvi", "J. Glass" ],
      "venue" : "Proceedings of the 8th International Joint Conference on Natural Language Processing (IJCNLP), volume 1, pages 1–10.",
      "citeRegEx" : "Belinkov et al\\.,? 2017",
      "shortCiteRegEx" : "Belinkov et al\\.",
      "year" : 2017
    }, {
      "title" : "Deep RNNs encode soft hierarchical syntax",
      "author" : [ "T. Blevins", "O. Levy", "L. Zettlemoyer" ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), volume 2, pages 14–19.",
      "citeRegEx" : "Blevins et al\\.,? 2018",
      "shortCiteRegEx" : "Blevins et al\\.",
      "year" : 2018
    }, {
      "title" : "Formal distributional semantics: introduction to the special issue",
      "author" : [ "G. Boleda", "A. Herbelot" ],
      "venue" : "Computational Linguistics, 42(4):619–635.",
      "citeRegEx" : "Boleda and Herbelot,? 2016",
      "shortCiteRegEx" : "Boleda and Herbelot",
      "year" : 2016
    }, {
      "title" : "Tree-structured composition in neural networks without tree-structured architectures",
      "author" : [ "S.R. Bowman", "C.D. Manning", "C. Potts" ],
      "venue" : "Proceedings of the 2015th International Conference on Cognitive Computation: Integrating Neural and Symbolic Approaches, pages 37–42.",
      "citeRegEx" : "Bowman et al\\.,? 2015",
      "shortCiteRegEx" : "Bowman et al\\.",
      "year" : 2015
    }, {
      "title" : "Meaning and necessity: A study in semantics and modal logic",
      "author" : [ "R. Carnap" ],
      "venue" : "University of Chicago Press.",
      "citeRegEx" : "Carnap,? 1947",
      "shortCiteRegEx" : "Carnap",
      "year" : 1947
    }, {
      "title" : "Three models for the description of language",
      "author" : [ "N. Chomsky" ],
      "venue" : "IRE Transactions on information theory, 2(3):113–124.",
      "citeRegEx" : "Chomsky,? 1956",
      "shortCiteRegEx" : "Chomsky",
      "year" : 1956
    }, {
      "title" : "Attention-based models for speech recognition",
      "author" : [ "J.K. Chorowski", "D. Bahdanau", "D. Serdyuk", "K. Cho", "Y. Bengio" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pages 577–585.",
      "citeRegEx" : "Chorowski et al\\.,? 2015",
      "shortCiteRegEx" : "Chorowski et al\\.",
      "year" : 2015
    }, {
      "title" : "Toward a connectionist model of recursion in human linguistic performance",
      "author" : [ "M.H. Christiansen", "N. Chater" ],
      "venue" : "Cognitive Science, 23(2):157–205.",
      "citeRegEx" : "Christiansen and Chater,? 1999",
      "shortCiteRegEx" : "Christiansen and Chater",
      "year" : 1999
    }, {
      "title" : "Empirical evaluation of gated recurrent neural networks on sequence modeling",
      "author" : [ "J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio" ],
      "venue" : "CoRR, abs/1412.3555.",
      "citeRegEx" : "Chung et al\\.,? 2014",
      "shortCiteRegEx" : "Chung et al\\.",
      "year" : 2014
    }, {
      "title" : "Vector space models of lexical meaning",
      "author" : [ "S. Clark" ],
      "venue" : "The Handbook of Contemporary semantic theory, pages 493–522.",
      "citeRegEx" : "Clark,? 2015",
      "shortCiteRegEx" : "Clark",
      "year" : 2015
    }, {
      "title" : "Mathematical foundations for a compositional distributional model of meaning",
      "author" : [ "B. Coecke", "M. Sadrzadeh", "S. Clark" ],
      "venue" : "Linguistic analysis, 36(1-4):345–384.",
      "citeRegEx" : "Coecke et al\\.,? 2010",
      "shortCiteRegEx" : "Coecke et al\\.",
      "year" : 2010
    }, {
      "title" : "Modelling, visualising and summarising documents with a single convolutional neural network. CoRR, abs/1406.3830",
      "author" : [ "M. Denil", "A. Demiraj", "N. Kalchbrenner", "P. Blunsom", "N. de Freitas" ],
      "venue" : null,
      "citeRegEx" : "Denil et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Denil et al\\.",
      "year" : 2014
    }, {
      "title" : "CNNs found to jump around more skillfully than RNNs: Compositional generalization in seq2seq convolutional networks",
      "author" : [ "R. Dess̀ı", "M. Baroni" ],
      "venue" : "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL),",
      "citeRegEx" : "Dess̀ı and Baroni,? \\Q2019\\E",
      "shortCiteRegEx" : "Dess̀ı and Baroni",
      "year" : 2019
    }, {
      "title" : "Location attention for extrapolation to longer sequences",
      "author" : [ "Y. Dubois", "G. Dagan", "D. Hupkes", "E. Bruni" ],
      "venue" : "CoRR, abs/1911.03872.",
      "citeRegEx" : "Dubois et al\\.,? 2019",
      "shortCiteRegEx" : "Dubois et al\\.",
      "year" : 2019
    }, {
      "title" : "Distributed representations, simple recurrent networks, and grammatical structure",
      "author" : [ "J.L. Elman" ],
      "venue" : "Machine learning, 7(2-3):195–225.",
      "citeRegEx" : "Elman,? 1991",
      "shortCiteRegEx" : "Elman",
      "year" : 1991
    }, {
      "title" : "Vector space models of word meaning and phrase meaning: A survey",
      "author" : [ "K. Erk" ],
      "venue" : "Language and Linguistics Compass, 6(10):635–653.",
      "citeRegEx" : "Erk,? 2012",
      "shortCiteRegEx" : "Erk",
      "year" : 2012
    }, {
      "title" : "Connectionism and cognitive architecture: a critical analysis",
      "author" : [ "J.A. Fodor", "Z.W. Pylyshyn" ],
      "venue" : "Cognition, 28(1-2):3–71.",
      "citeRegEx" : "Fodor and Pylyshyn,? 1988",
      "shortCiteRegEx" : "Fodor and Pylyshyn",
      "year" : 1988
    }, {
      "title" : "A convolutional encoder model for neural machine translation",
      "author" : [ "J. Gehring", "M. Auli", "D. Grangier", "Y.N. Dauphin" ],
      "venue" : "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), volume 1, pages 123–135.",
      "citeRegEx" : "Gehring et al\\.,? 2017a",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Convolutional sequence to sequence learning",
      "author" : [ "J. Gehring", "M. Auli", "D. Grangier", "D. Yarats", "Y.N. Dauphin" ],
      "venue" : "Proceedings of the 34th International Conference on Machine Learning (ICML), pages 1243–1252.",
      "citeRegEx" : "Gehring et al\\.,? 2017b",
      "shortCiteRegEx" : "Gehring et al\\.",
      "year" : 2017
    }, {
      "title" : "Under the hood: Using diagnostic classifiers to investigate and improve how language models track agreement information",
      "author" : [ "M. Giulianelli", "J. Harding", "F. Mohnert", "D. Hupkes", "W. Zuidema" ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 240–248.",
      "citeRegEx" : "Giulianelli et al\\.,? 2018",
      "shortCiteRegEx" : "Giulianelli et al\\.",
      "year" : 2018
    }, {
      "title" : "Assessing BERT’s syntactic abilities",
      "author" : [ "Y. Goldberg" ],
      "venue" : "CoRR, abs/1901.05287.",
      "citeRegEx" : "Goldberg,? 2019",
      "shortCiteRegEx" : "Goldberg",
      "year" : 2019
    }, {
      "title" : "Learning task-dependent distributed representations by backpropagation through structure",
      "author" : [ "C. Goller", "A. Kuchler" ],
      "venue" : "Proceedings of International Conference on Neural Networks (ICNN), volume 1, pages 347–352.",
      "citeRegEx" : "Goller and Kuchler,? 1996",
      "shortCiteRegEx" : "Goller and Kuchler",
      "year" : 1996
    }, {
      "title" : "Colorless green recurrent networks dream hierarchically",
      "author" : [ "K. Gulordava", "P. Bojanowski", "E. Grave", "T. Linzen", "M. Baroni" ],
      "venue" : "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), volume 1, pages 1195–1205.",
      "citeRegEx" : "Gulordava et al\\.,? 2018",
      "shortCiteRegEx" : "Gulordava et al\\.",
      "year" : 2018
    }, {
      "title" : "Character-level question answering with attention",
      "author" : [ "X. He", "D. Golub" ],
      "venue" : "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1598–1607.",
      "citeRegEx" : "He and Golub,? 2016",
      "shortCiteRegEx" : "He and Golub",
      "year" : 2016
    }, {
      "title" : "Advances in natural language processing",
      "author" : [ "J. Hirschberg", "C.D. Manning" ],
      "venue" : "Science, 349(6245):261–266.",
      "citeRegEx" : "Hirschberg and Manning,? 2015",
      "shortCiteRegEx" : "Hirschberg and Manning",
      "year" : 2015
    }, {
      "title" : "Long short-term memory",
      "author" : [ "S. Hochreiter", "J. Schmidhuber" ],
      "venue" : "Neural computation, 9(8):1735– 1780.",
      "citeRegEx" : "Hochreiter and Schmidhuber,? 1997",
      "shortCiteRegEx" : "Hochreiter and Schmidhuber",
      "year" : 1997
    }, {
      "title" : "Learning compositionally through attentive guidance",
      "author" : [ "D. Hupkes", "A. Singh", "K. Korrel", "G. Kruszewski", "E. Bruni" ],
      "venue" : "International Conference on Computational Linguistics and Intelligent Text Processing (CICLing).",
      "citeRegEx" : "Hupkes et al\\.,? 2019",
      "shortCiteRegEx" : "Hupkes et al\\.",
      "year" : 2019
    }, {
      "title" : "Visualisation and ‘diagnostic classifiers’ reveal how recurrent and recursive neural networks process hierarchical structure",
      "author" : [ "D. Hupkes", "S. Veldhoen", "W. Zuidema" ],
      "venue" : "Journal of Artificial Intelligence Research, 61:907–926.",
      "citeRegEx" : "Hupkes et al\\.,? 2018",
      "shortCiteRegEx" : "Hupkes et al\\.",
      "year" : 2018
    }, {
      "title" : "Logische Untersuchungen",
      "author" : [ "E. Husserl" ],
      "venue" : "Max Niemeyer.",
      "citeRegEx" : "Husserl,? 1913",
      "shortCiteRegEx" : "Husserl",
      "year" : 1913
    }, {
      "title" : "The (dis)organization of the grammar: 25 years",
      "author" : [ "P. Jacobson" ],
      "venue" : "Linguistics and Philosophy, 25(5):601–626.",
      "citeRegEx" : "Jacobson,? 2002",
      "shortCiteRegEx" : "Jacobson",
      "year" : 2002
    }, {
      "title" : "Foundations and applications of Montague grammar",
      "author" : [ "T. Janssen" ],
      "venue" : "Mathematisch Centrum.",
      "citeRegEx" : "Janssen,? 1983",
      "shortCiteRegEx" : "Janssen",
      "year" : 1983
    }, {
      "title" : "CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning",
      "author" : [ "J. Johnson", "B. Hariharan", "L. van der Maaten", "L. Fei-Fei", "C.L. Zitnick", "R. Girshick" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "citeRegEx" : "Johnson et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Johnson et al\\.",
      "year" : 2017
    }, {
      "title" : "Exploring the limits of language modeling",
      "author" : [ "R. Jozefowicz", "O. Vinyals", "M. Schuster", "N. Shazeer", "Y. Wu" ],
      "venue" : "CoRR, abs/1602.02410.",
      "citeRegEx" : "Jozefowicz et al\\.,? 2016",
      "shortCiteRegEx" : "Jozefowicz et al\\.",
      "year" : 2016
    }, {
      "title" : "A convolutional neural network for modelling sentences",
      "author" : [ "N. Kalchbrenner", "E. Grefenstette", "P. Blunsom" ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), volume 1, pages 655–665.",
      "citeRegEx" : "Kalchbrenner et al\\.,? 2014",
      "shortCiteRegEx" : "Kalchbrenner et al\\.",
      "year" : 2014
    }, {
      "title" : "Compositional operators in distributional semantics",
      "author" : [ "D. Kartsaklis" ],
      "venue" : "Springer Science Reviews, 2(1-2):161–177.",
      "citeRegEx" : "Kartsaklis,? 2014",
      "shortCiteRegEx" : "Kartsaklis",
      "year" : 2014
    }, {
      "title" : "Unsupervised recurrent neural network grammars",
      "author" : [ "Y. Kim", "A.M. Rush", "L. Yu", "A. Kuncoro", "C. Dyer", "G. Melis" ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), volume 1, pages 1105–1117.",
      "citeRegEx" : "Kim et al\\.,? 2019",
      "shortCiteRegEx" : "Kim et al\\.",
      "year" : 2019
    }, {
      "title" : "OpenNMT: Open-source toolkit for neural machine translation",
      "author" : [ "G. Klein", "Y. Kim", "Y. Deng", "J. Senellart", "A.M. Rush" ],
      "venue" : "Bansal, M. and Ji, H., editors, Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (ACL), System Demonstrations, pages 67–72.",
      "citeRegEx" : "Klein et al\\.,? 2017",
      "shortCiteRegEx" : "Klein et al\\.",
      "year" : 2017
    }, {
      "title" : "Transcoding compositionally: using attention to find more generalizable solutions",
      "author" : [ "K. Korrel", "D. Hupkes", "V. Dankers", "E. Bruni" ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, page 111.",
      "citeRegEx" : "Korrel et al\\.,? 2019",
      "shortCiteRegEx" : "Korrel et al\\.",
      "year" : 2019
    }, {
      "title" : "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
      "author" : [ "B. Lake", "M. Baroni" ],
      "venue" : "proceedings of the 35th International Conference on Machine Learning (ICML), pages 4487–4499.",
      "citeRegEx" : "Lake and Baroni,? 2018",
      "shortCiteRegEx" : "Lake and Baroni",
      "year" : 2018
    }, {
      "title" : "The emergence of number and syntax units in LSTM language models",
      "author" : [ "Y. Lakretz", "G. Kruszewski", "T. Desbordes", "D. Hupkes", "S. Dehaene", "M. Baroni" ],
      "venue" : "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), volume 1.",
      "citeRegEx" : "Lakretz et al\\.,? 2019",
      "shortCiteRegEx" : "Lakretz et al\\.",
      "year" : 2019
    }, {
      "title" : "The forest convolutional network: compositional distributional semantics with a neural chart and without binarization",
      "author" : [ "P. Le", "W. Zuidema" ],
      "venue" : "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1155–1164.",
      "citeRegEx" : "Le and Zuidema,? 2015",
      "shortCiteRegEx" : "Le and Zuidema",
      "year" : 2015
    }, {
      "title" : "Open sesame: Getting inside BERT’s linguistic knowledge",
      "author" : [ "Y. Lin", "Y.C. Tan", "R. Frank" ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 241–253.",
      "citeRegEx" : "Lin et al\\.,? 2019",
      "shortCiteRegEx" : "Lin et al\\.",
      "year" : 2019
    }, {
      "title" : "Assessing the ability of LSTMs to learn syntaxsensitive dependencies",
      "author" : [ "T. Linzen", "E. Dupoux", "Y. Goldberg" ],
      "venue" : "Transactions of the Association for Computational Linguistics, 4:521–535.",
      "citeRegEx" : "Linzen et al\\.,? 2016",
      "shortCiteRegEx" : "Linzen et al\\.",
      "year" : 2016
    }, {
      "title" : "Memorize or generalize? Searching for a compositional RNN in a haystack",
      "author" : [ "A. Lǐska", "G. Kruszewski", "M. Baroni" ],
      "venue" : "Proceedings of AEGAP (FAIM Joint Workshop on Architectures and Evaluation for Generality, Autonomy and Progress in AI).",
      "citeRegEx" : "Lǐska et al\\.,? 2018",
      "shortCiteRegEx" : "Lǐska et al\\.",
      "year" : 2018
    }, {
      "title" : "Rearranging the familiar: testing compositional generalization in recurrent networks",
      "author" : [ "J. Loula", "M. Baroni", "B.M. Lake" ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 108–114.",
      "citeRegEx" : "Loula et al\\.,? 2018",
      "shortCiteRegEx" : "Loula et al\\.",
      "year" : 2018
    }, {
      "title" : "The Stanford CoreNLP natural language processing toolkit",
      "author" : [ "C.D. Manning", "M. Surdeanu", "J. Bauer", "J. Finkel", "S.J. Bethard", "D. McClosky" ],
      "venue" : "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (ACL), System Demonstrations, pages 55–60.",
      "citeRegEx" : "Manning et al\\.,? 2014",
      "shortCiteRegEx" : "Manning et al\\.",
      "year" : 2014
    }, {
      "title" : "The algebraic mind: Integrating connectionism and cognitive science",
      "author" : [ "G.F. Marcus" ],
      "venue" : "MIT press.",
      "citeRegEx" : "Marcus,? 2003",
      "shortCiteRegEx" : "Marcus",
      "year" : 2003
    }, {
      "title" : "Overregularization in language acquisition",
      "author" : [ "G.F. Marcus", "S. Pinker", "M. Ullman", "M. Hollander", "T.J. Rosen", "F. Xu", "H. Clahsen" ],
      "venue" : "Monographs of the society for research in child development, pages i–178.",
      "citeRegEx" : "Marcus et al\\.,? 1992",
      "shortCiteRegEx" : "Marcus et al\\.",
      "year" : 1992
    }, {
      "title" : "Extracting syntactic trees from transformer encoder self-attentions",
      "author" : [ "D. Mareček", "R. Rosa" ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 347–349.",
      "citeRegEx" : "Mareček and Rosa,? 2018",
      "shortCiteRegEx" : "Mareček and Rosa",
      "year" : 2018
    }, {
      "title" : "RNNs implicitly implement tensor-product representations",
      "author" : [ "R.T. McCoy", "T. Linzen", "E. Dunbar", "P. Smolensky" ],
      "venue" : "Proceedings of the 7th International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "McCoy et al\\.,? 2019",
      "shortCiteRegEx" : "McCoy et al\\.",
      "year" : 2019
    }, {
      "title" : "Contextual correlates of semantic similarity",
      "author" : [ "G.A. Miller", "W.G. Charles" ],
      "venue" : "Language and cognitive processes, 6(1):1–28.",
      "citeRegEx" : "Miller and Charles,? 1991",
      "shortCiteRegEx" : "Miller and Charles",
      "year" : 1991
    }, {
      "title" : "Vector-based models of semantic composition",
      "author" : [ "J. Mitchell", "M. Lapata" ],
      "venue" : "Proceedings of the 46th Annual Meeting of the Association for Computational Linguistics: Human Language Technology (ACL-HLT), pages 236–244.",
      "citeRegEx" : "Mitchell and Lapata,? 2008",
      "shortCiteRegEx" : "Mitchell and Lapata",
      "year" : 2008
    }, {
      "title" : "Siamese recurrent networks learn first-order logic reasoning and exhibit zero-shot compositional generalization",
      "author" : [ "M. Mul", "W. Zuidema" ],
      "venue" : "CoRR, abs/1906.00180.",
      "citeRegEx" : "Mul and Zuidema,? 2019",
      "shortCiteRegEx" : "Mul and Zuidema",
      "year" : 2019
    }, {
      "title" : "Communication and strong compositionality",
      "author" : [ "P. Pagin" ],
      "venue" : "Journal of Philosophical Logic, 32(3):287–322.",
      "citeRegEx" : "Pagin,? 2003",
      "shortCiteRegEx" : "Pagin",
      "year" : 2003
    }, {
      "title" : "Compositionality i: Definitions and variants",
      "author" : [ "P. Pagin", "D. Westerst̊ahl" ],
      "venue" : "Philosophy Compass,",
      "citeRegEx" : "Pagin and Westerst̊ahl,? \\Q2010\\E",
      "shortCiteRegEx" : "Pagin and Westerst̊ahl",
      "year" : 2010
    }, {
      "title" : "Lexical semantics and compositionality",
      "author" : [ "B. Partee" ],
      "venue" : "An invitation to cognitive science: Language, 1:311–360.",
      "citeRegEx" : "Partee,? 1995",
      "shortCiteRegEx" : "Partee",
      "year" : 1995
    }, {
      "title" : "The dual-mechanism debate",
      "author" : [ "M. Penke" ],
      "venue" : "The Oxford handbook of compositionality. Oxford University Press.",
      "citeRegEx" : "Penke,? 2012",
      "shortCiteRegEx" : "Penke",
      "year" : 2012
    }, {
      "title" : "Language learnability and language development",
      "author" : [ "S. Pinker" ],
      "venue" : "Cambridge, MA: Harvard University Press.",
      "citeRegEx" : "Pinker,? 1984",
      "shortCiteRegEx" : "Pinker",
      "year" : 1984
    }, {
      "title" : "Holographic reduced representations: convolution algebra for compositional distributed representations",
      "author" : [ "T. Plate" ],
      "venue" : "Proceedings of the 12th International Joint Conference on Artificial Intelligence (IJCAI), volume 1, pages 30–35.",
      "citeRegEx" : "Plate,? 1991",
      "shortCiteRegEx" : "Plate",
      "year" : 1991
    }, {
      "title" : "A case for deep learning in semantics: Response to pater",
      "author" : [ "C. Potts" ],
      "venue" : "Language.",
      "citeRegEx" : "Potts,? 2019",
      "shortCiteRegEx" : "Potts",
      "year" : 2019
    }, {
      "title" : "Recursion and the infinitude claim",
      "author" : [ "G.K. Pullum", "B.C. Scholz" ],
      "venue" : "Recursion in human language, 104:113–38.",
      "citeRegEx" : "Pullum and Scholz,? 2010",
      "shortCiteRegEx" : "Pullum and Scholz",
      "year" : 2010
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "A. Radford", "J. Wu", "R. Child", "D. Luan", "D. Amodei", "I. Sutskever" ],
      "venue" : "OpenAI Blog, 1(8).",
      "citeRegEx" : "Radford et al\\.,? 2019",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "An analysis of encoder representations in transformer-based machine translation",
      "author" : [ "A. Raganato", "J. Tiedemann" ],
      "venue" : "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 287–297.",
      "citeRegEx" : "Raganato and Tiedemann,? 2018",
      "shortCiteRegEx" : "Raganato and Tiedemann",
      "year" : 2018
    }, {
      "title" : "Simple recurrent networks learn context-free and context-sensitive languages by counting",
      "author" : [ "P. Rodriguez" ],
      "venue" : "Neural computation, 13(9):2093–118.",
      "citeRegEx" : "Rodriguez,? 2001",
      "shortCiteRegEx" : "Rodriguez",
      "year" : 2001
    }, {
      "title" : "A recurrent neural network that learns to count",
      "author" : [ "P. Rodriguez", "J. Wiles", "J.L. Elman" ],
      "venue" : "Connection Science, 11(1):5–40.",
      "citeRegEx" : "Rodriguez et al\\.,? 1999",
      "shortCiteRegEx" : "Rodriguez et al\\.",
      "year" : 1999
    }, {
      "title" : "Parallel distributed processing: explorations in the microstructure of cognition, volume 2, chapter On learning the past tenses of English verbs, pages 216–271",
      "author" : [ "D.E. Rumelhart", "J.L. McClelland" ],
      "venue" : "MIT Press, Cambridge.",
      "citeRegEx" : "Rumelhart and McClelland,? 1986",
      "shortCiteRegEx" : "Rumelhart and McClelland",
      "year" : 1986
    }, {
      "title" : "Analysing mathematical reasoning abilities of neural models",
      "author" : [ "D. Saxton", "E. Grefenstette", "F. Hill", "P. Kohli" ],
      "venue" : "Proceedings of the 7th International Conference on Learning Representations (ICLR).",
      "citeRegEx" : "Saxton et al\\.,? 2019",
      "shortCiteRegEx" : "Saxton et al\\.",
      "year" : 2019
    }, {
      "title" : "Does string-based neural MT learn source syntax",
      "author" : [ "X. Shi", "I. Padhi", "K. Knight" ],
      "venue" : "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Shi et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Shi et al\\.",
      "year" : 2016
    }, {
      "title" : "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
      "author" : [ "P. Smolensky" ],
      "venue" : "Artificial intelligence, 46(1-2):159–216.",
      "citeRegEx" : "Smolensky,? 1990",
      "shortCiteRegEx" : "Smolensky",
      "year" : 1990
    }, {
      "title" : "Learning continuous phrase representations and syntactic parsing with recursive neural networks",
      "author" : [ "R. Socher", "C.D. Manning", "A.Y. Ng" ],
      "venue" : "Proceedings of the NIPS2010 Deep Learning and Unsupervised Feature Learning Workshop, pages 1–9.",
      "citeRegEx" : "Socher et al\\.,? 2010",
      "shortCiteRegEx" : "Socher et al\\.",
      "year" : 2010
    }, {
      "title" : "Discovering the compositional structure of vector representations with role learning networks",
      "author" : [ "P. Soulos", "T. McCoy", "T. Linzen", "P. Smolensky" ],
      "venue" : "Proceedings of the NeurIPS 2019 Workshop on Context and Compositionality in biological and artificial neural systems.",
      "citeRegEx" : "Soulos et al\\.,? 2019",
      "shortCiteRegEx" : "Soulos et al\\.",
      "year" : 2019
    }, {
      "title" : "Sequence to sequence learning with neural networks",
      "author" : [ "I. Sutskever", "O. Vinyals", "Q.V. Le" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pages 3104–3112.",
      "citeRegEx" : "Sutskever et al\\.,? 2014",
      "shortCiteRegEx" : "Sutskever et al\\.",
      "year" : 2014
    }, {
      "title" : "The case for compositionality",
      "author" : [ "Z. Szabó" ],
      "venue" : "The Oxford handbook of compositionality, 64:80.",
      "citeRegEx" : "Szabó,? 2012",
      "shortCiteRegEx" : "Szabó",
      "year" : 2012
    }, {
      "title" : "BERT rediscovers the classical NLP pipeline",
      "author" : [ "I. Tenney", "D. Das", "E. Pavlick" ],
      "venue" : "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 4593–4601.",
      "citeRegEx" : "Tenney et al\\.,? 2019a",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "What do you learn from context? Probing for sentence structure in contextualized word representations",
      "author" : [ "I. Tenney", "P. Xia", "B. Chen", "A. Wang", "A. Poliak", "R.T. McCoy", "N. Kim", "B. Van Durme", "S.R. Bowman", "D Das" ],
      "venue" : "In Proceedings of the 7th International Conference on Learning Representations (ICLR)",
      "citeRegEx" : "Tenney et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Tenney et al\\.",
      "year" : 2019
    }, {
      "title" : "The importance of being recurrent for modeling hierarchical structure",
      "author" : [ "K. Tran", "A. Bisazza", "C. Monz" ],
      "venue" : "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4731–4736.",
      "citeRegEx" : "Tran et al\\.,? 2018",
      "shortCiteRegEx" : "Tran et al\\.",
      "year" : 2018
    }, {
      "title" : "From frequency to meaning: Vector space models of semantics",
      "author" : [ "P.D. Turney", "P. Pantel" ],
      "venue" : "Journal of Artificial Intelligence Research, 37:141–188.",
      "citeRegEx" : "Turney and Pantel,? 2010",
      "shortCiteRegEx" : "Turney and Pantel",
      "year" : 2010
    }, {
      "title" : "Attention is all you need",
      "author" : [ "A. Vaswani", "N. Shazeer", "N. Parmar", "J. Uszkoreit", "L. Jones", "A.N. Gomez", "L. Kaiser", "I. Polosukhin" ],
      "venue" : "Advances in Neural Information Processing Systems (NIPS), pages 5998–6008.",
      "citeRegEx" : "Vaswani et al\\.,? 2017",
      "shortCiteRegEx" : "Vaswani et al\\.",
      "year" : 2017
    }, {
      "title" : "Diagnostic classifiers: Revealing how neural networks process hierarchical structure",
      "author" : [ "S. Veldhoen", "D. Hupkes", "W. Zuidema" ],
      "venue" : "Proceedings of the NIPS2016 Workshop on Cognitive Computation: Integrating Neural and Symbolic Approaches.",
      "citeRegEx" : "Veldhoen et al\\.,? 2016",
      "shortCiteRegEx" : "Veldhoen et al\\.",
      "year" : 2016
    }, {
      "title" : "Analyzing the structure of attention in a transformer language model",
      "author" : [ "J. Vig", "Y. Belinkov" ],
      "venue" : "Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 63–76.",
      "citeRegEx" : "Vig and Belinkov,? 2019",
      "shortCiteRegEx" : "Vig and Belinkov",
      "year" : 2019
    }, {
      "title" : "Extracting automata from recurrent neural networks using queries and counterexamples",
      "author" : [ "G. Weiss", "Y. Goldberg", "E. Yahav" ],
      "venue" : "Proceedings of the 35th International Conference on Machine Learning (ICML), volume 80, pages 5244–5253.",
      "citeRegEx" : "Weiss et al\\.,? 2018a",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2018
    }, {
      "title" : "On the practical computational power of finite precision RNNs for language recognition",
      "author" : [ "G. Weiss", "Y. Goldberg", "E. Yahav" ],
      "venue" : "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), volume 2, pages 740–745.",
      "citeRegEx" : "Weiss et al\\.,? 2018b",
      "shortCiteRegEx" : "Weiss et al\\.",
      "year" : 2018
    }, {
      "title" : "Learning to count without a counter: A case study of dynamics and activation landscapes in recurrent networks",
      "author" : [ "J. Wiles", "J. Elman" ],
      "venue" : "Proceedings of the 17th Annual Conference of the Cognitive Science Society (CogSci), pages 482–487.",
      "citeRegEx" : "Wiles and Elman,? 1995",
      "shortCiteRegEx" : "Wiles and Elman",
      "year" : 1995
    }, {
      "title" : "Some additional experiments extending the tech report “Assessing BERTs syntactic abilities” by Yoav Goldberg",
      "author" : [ "T. Wolf" ],
      "venue" : "Technical report.",
      "citeRegEx" : "Wolf,? 2019",
      "shortCiteRegEx" : "Wolf",
      "year" : 2019
    }, {
      "title" : "From compositional to systematic semantics",
      "author" : [ "W. Zadrozny" ],
      "venue" : "Linguistics and philosophy, 17(4):329–342.",
      "citeRegEx" : "Zadrozny,? 1994",
      "shortCiteRegEx" : "Zadrozny",
      "year" : 1994
    }, {
      "title" : "Learning to execute",
      "author" : [ "W. Zaremba", "I. Sutskever" ],
      "venue" : "CoRR, abs/1410.4615.",
      "citeRegEx" : "Zaremba and Sutskever,? 2014",
      "shortCiteRegEx" : "Zaremba and Sutskever",
      "year" : 2014
    } ],
    "referenceMentions" : [ {
      "referenceID" : 21,
      "context" : "The advancements of distributional semantics of the word level allowed the field of natural language processing to move from discrete mathematical methods to models that use continuous numerical vectors (see, e.g. Clark, 2015; Erk, 2012; Turney and Pantel, 2010).",
      "startOffset" : 203,
      "endOffset" : 262
    }, {
      "referenceID" : 82,
      "context" : "The advancements of distributional semantics of the word level allowed the field of natural language processing to move from discrete mathematical methods to models that use continuous numerical vectors (see, e.g. Clark, 2015; Erk, 2012; Turney and Pantel, 2010).",
      "startOffset" : 203,
      "endOffset" : 262
    }, {
      "referenceID" : 15,
      "context" : "They can then act as surrogates for word meaning and be used, for example, to quantify the degree of semantic similarity between words, by means of simple geometric operations (Clark, 2015).",
      "startOffset" : 176,
      "endOffset" : 189
    }, {
      "referenceID" : 74,
      "context" : "Mitchell and Lapata, 2008) up to more powerful tensor-based operations (Smolensky, 1990; Plate, 1991) where, for instance, the adjective blue would be represented as a matrix, which would be multiplied with the noun vector sky to return the representation for blue sky (e.",
      "startOffset" : 71,
      "endOffset" : 101
    }, {
      "referenceID" : 64,
      "context" : "Mitchell and Lapata, 2008) up to more powerful tensor-based operations (Smolensky, 1990; Plate, 1991) where, for instance, the adjective blue would be represented as a matrix, which would be multiplied with the noun vector sky to return the representation for blue sky (e.",
      "startOffset" : 71,
      "endOffset" : 101
    }, {
      "referenceID" : 16,
      "context" : "Mitchell and Lapata, 2008) up to more powerful tensor-based operations (Smolensky, 1990; Plate, 1991) where, for instance, the adjective blue would be represented as a matrix, which would be multiplied with the noun vector sky to return the representation for blue sky (e.g. Baroni and Zamparelli, 2010; Coecke et al., 2010).",
      "startOffset" : 269,
      "endOffset" : 324
    }, {
      "referenceID" : 27,
      "context" : "While the composition function in this approach is fully learned from data using back-propagation through structure (Goller and Kuchler, 1996), the tree structure that defines the order of application has to be provided to the model, allowing models to be ‘compositional by design’.",
      "startOffset" : 116,
      "endOffset" : 142
    }, {
      "referenceID" : 31,
      "context" : "Earlier models of this type deal with language processing sequentially and use recurrent processing units such as LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Chung et al.",
      "startOffset" : 120,
      "endOffset" : 154
    }, {
      "referenceID" : 14,
      "context" : "Earlier models of this type deal with language processing sequentially and use recurrent processing units such as LSTMs (Hochreiter and Schmidhuber, 1997) and GRUs (Chung et al., 2014) at their core (Sutskever et al.",
      "startOffset" : 164,
      "endOffset" : 184
    }, {
      "referenceID" : 77,
      "context" : ", 2014) at their core (Sutskever et al., 2014) or are based on convolutional networks (Kalchbrenner et al.",
      "startOffset" : 22,
      "endOffset" : 46
    }, {
      "referenceID" : 39,
      "context" : ", 2014) or are based on convolutional networks (Kalchbrenner et al., 2014).",
      "startOffset" : 47,
      "endOffset" : 74
    }, {
      "referenceID" : 1,
      "context" : "An important contribution to their effectiveness comes from attention mechanisms, which allow recurrent models to keep track of long-distance dependencies more effectively (Bahdanau et al., 2015).",
      "startOffset" : 172,
      "endOffset" : 195
    }, {
      "referenceID" : 83,
      "context" : "More recently, these models went all-in on attention, abandoning sequential processing in favour of massively distributed sequence processing all based on attention (Vaswani et al., 2017).",
      "startOffset" : 165,
      "endOffset" : 187
    }, {
      "referenceID" : 22,
      "context" : "Neural models, on the other hand, seem very well up to handling noisy scenarios but are often argued to be fundamentally incapable of conducting the types of compositions required to process natural language (for more information on this debate, see Pinker, 1984; Fodor and Pylyshyn, 1988; Smolensky, 1990; Marcus, 2003) or at least to not use those types of compositions to solve their tasks (e.",
      "startOffset" : 208,
      "endOffset" : 320
    }, {
      "referenceID" : 74,
      "context" : "Neural models, on the other hand, seem very well up to handling noisy scenarios but are often argued to be fundamentally incapable of conducting the types of compositions required to process natural language (for more information on this debate, see Pinker, 1984; Fodor and Pylyshyn, 1988; Smolensky, 1990; Marcus, 2003) or at least to not use those types of compositions to solve their tasks (e.",
      "startOffset" : 208,
      "endOffset" : 320
    }, {
      "referenceID" : 52,
      "context" : "Neural models, on the other hand, seem very well up to handling noisy scenarios but are often argued to be fundamentally incapable of conducting the types of compositions required to process natural language (for more information on this debate, see Pinker, 1984; Fodor and Pylyshyn, 1988; Smolensky, 1990; Marcus, 2003) or at least to not use those types of compositions to solve their tasks (e.",
      "startOffset" : 208,
      "endOffset" : 320
    }, {
      "referenceID" : 33,
      "context" : "Lake and Baroni, 2018); Some instead consider models’ ability to process hierarchical structures (Hupkes et al., 2018; Linzen et al., 2016); Yet others consider if models can segment the input into reusable parts (Johnson et al.",
      "startOffset" : 97,
      "endOffset" : 139
    }, {
      "referenceID" : 48,
      "context" : "Lake and Baroni, 2018); Some instead consider models’ ability to process hierarchical structures (Hupkes et al., 2018; Linzen et al., 2016); Yet others consider if models can segment the input into reusable parts (Johnson et al.",
      "startOffset" : 97,
      "endOffset" : 139
    }, {
      "referenceID" : 37,
      "context" : ", 2016); Yet others consider if models can segment the input into reusable parts (Johnson et al., 2017).",
      "startOffset" : 81,
      "endOffset" : 103
    }, {
      "referenceID" : 36,
      "context" : "Without these components, the principle of compositionality is formally vacuous (Janssen, 1983; Zadrozny, 1994), because also trivial and intuitively non-compositional solutions that cast every expression as one part and assign it a meaning as a whole do not formally violate the principle of compositionality.",
      "startOffset" : 80,
      "endOffset" : 111
    }, {
      "referenceID" : 90,
      "context" : "Without these components, the principle of compositionality is formally vacuous (Janssen, 1983; Zadrozny, 1994), because also trivial and intuitively non-compositional solutions that cast every expression as one part and assign it a meaning as a whole do not formally violate the principle of compositionality.",
      "startOffset" : 80,
      "endOffset" : 111
    }, {
      "referenceID" : 33,
      "context" : "They show that simple recurrent networks do not perform well on the task, but gated recurrent networks can generalise well to lengths and depths of arithmetic expressions that were not in the training set, although their performance quickly deteriorates when the length of expressions grows (Hupkes et al., 2018).",
      "startOffset" : 291,
      "endOffset" : 312
    }, {
      "referenceID" : 37,
      "context" : "We excluded grounded data sets such as CLEVR (Johnson et al., 2017) and SQOOP (Bahdanau et al.",
      "startOffset" : 45,
      "endOffset" : 67
    }, {
      "referenceID" : 2,
      "context" : ", 2017) and SQOOP (Bahdanau et al., 2018), which contain more than one modality.",
      "startOffset" : 18,
      "endOffset" : 41
    }, {
      "referenceID" : 45,
      "context" : "Furthermore, we did not include studies whose primary focus is on how neural networks implement compositional structures (Lakretz et al., 2019; Giulianelli et al., 2018; McCoy et al., 2019; Soulos et al., 2019; Weiss et al., 2018a) or studies that evaluate compositionality only based on models’ representations (Andreas, 2019).",
      "startOffset" : 121,
      "endOffset" : 231
    }, {
      "referenceID" : 25,
      "context" : "Furthermore, we did not include studies whose primary focus is on how neural networks implement compositional structures (Lakretz et al., 2019; Giulianelli et al., 2018; McCoy et al., 2019; Soulos et al., 2019; Weiss et al., 2018a) or studies that evaluate compositionality only based on models’ representations (Andreas, 2019).",
      "startOffset" : 121,
      "endOffset" : 231
    }, {
      "referenceID" : 55,
      "context" : "Furthermore, we did not include studies whose primary focus is on how neural networks implement compositional structures (Lakretz et al., 2019; Giulianelli et al., 2018; McCoy et al., 2019; Soulos et al., 2019; Weiss et al., 2018a) or studies that evaluate compositionality only based on models’ representations (Andreas, 2019).",
      "startOffset" : 121,
      "endOffset" : 231
    }, {
      "referenceID" : 76,
      "context" : "Furthermore, we did not include studies whose primary focus is on how neural networks implement compositional structures (Lakretz et al., 2019; Giulianelli et al., 2018; McCoy et al., 2019; Soulos et al., 2019; Weiss et al., 2018a) or studies that evaluate compositionality only based on models’ representations (Andreas, 2019).",
      "startOffset" : 121,
      "endOffset" : 231
    }, {
      "referenceID" : 86,
      "context" : "Furthermore, we did not include studies whose primary focus is on how neural networks implement compositional structures (Lakretz et al., 2019; Giulianelli et al., 2018; McCoy et al., 2019; Soulos et al., 2019; Weiss et al., 2018a) or studies that evaluate compositionality only based on models’ representations (Andreas, 2019).",
      "startOffset" : 121,
      "endOffset" : 231
    }, {
      "referenceID" : 0,
      "context" : ", 2018a) or studies that evaluate compositionality only based on models’ representations (Andreas, 2019).",
      "startOffset" : 89,
      "endOffset" : 104
    }, {
      "referenceID" : 38,
      "context" : "They devise a number-agreement task and find that a pre-trained state-of-the-art LSTM model (Jozefowicz et al., 2016) does not capture the structure-sensitive dependencies.",
      "startOffset" : 92,
      "endOffset" : 117
    }, {
      "referenceID" : 28,
      "context" : "Later, these results were contested by a different research group, who repeated and extended the study with a different language model and tested a number of different long-distance dependencies for English, Italian, Hebrew and Russian (Gulordava et al., 2018).",
      "startOffset" : 236,
      "endOffset" : 260
    }, {
      "referenceID" : 26,
      "context" : "Some (still unpublished) studies find evidence that such models score high on the previously described number-agreement task (Goldberg, 2019; Lin et al., 2019).",
      "startOffset" : 125,
      "endOffset" : 159
    }, {
      "referenceID" : 47,
      "context" : "Some (still unpublished) studies find evidence that such models score high on the previously described number-agreement task (Goldberg, 2019; Lin et al., 2019).",
      "startOffset" : 125,
      "endOffset" : 159
    }, {
      "referenceID" : 81,
      "context" : "More mixed results are reported by others (Tran et al., 2018; Wolf, 2019).",
      "startOffset" : 42,
      "endOffset" : 73
    }, {
      "referenceID" : 89,
      "context" : "More mixed results are reported by others (Tran et al., 2018; Wolf, 2019).",
      "startOffset" : 42,
      "endOffset" : 73
    }, {
      "referenceID" : 73,
      "context" : "A robust finding from such analyses is that features such as syntactic constituency, part-of-speech tags and dependency edges can be reliably predicted from the hidden representations of both recurrent neural networks (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018) and transformer models (Raganato and Tiedemann, 2018; Tenney et al.",
      "startOffset" : 218,
      "endOffset" : 281
    }, {
      "referenceID" : 6,
      "context" : "A robust finding from such analyses is that features such as syntactic constituency, part-of-speech tags and dependency edges can be reliably predicted from the hidden representations of both recurrent neural networks (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018) and transformer models (Raganato and Tiedemann, 2018; Tenney et al.",
      "startOffset" : 218,
      "endOffset" : 281
    }, {
      "referenceID" : 7,
      "context" : "A robust finding from such analyses is that features such as syntactic constituency, part-of-speech tags and dependency edges can be reliably predicted from the hidden representations of both recurrent neural networks (Shi et al., 2016; Belinkov et al., 2017; Blevins et al., 2018) and transformer models (Raganato and Tiedemann, 2018; Tenney et al.",
      "startOffset" : 218,
      "endOffset" : 281
    }, {
      "referenceID" : 79,
      "context" : "Generally, lower-level features are encoded in lower layers, while higher-level syntactic and semantic features are better represented in deeper layers (e.g. Blevins et al., 2018; Tenney et al., 2019a).",
      "startOffset" : 152,
      "endOffset" : 201
    }, {
      "referenceID" : 13,
      "context" : "Such studies consider how well neural networks can represent formal languages generated by grammars from different classes of the Chomsky Hierarchy (e.g. Elman, 1991; Christiansen and Chater, 1999; Rodriguez, 2001; Wiles and Elman, 1995; Rodriguez et al., 1999; Batali, 1994; Weiss et al., 2018b).",
      "startOffset" : 148,
      "endOffset" : 296
    }, {
      "referenceID" : 69,
      "context" : "Such studies consider how well neural networks can represent formal languages generated by grammars from different classes of the Chomsky Hierarchy (e.g. Elman, 1991; Christiansen and Chater, 1999; Rodriguez, 2001; Wiles and Elman, 1995; Rodriguez et al., 1999; Batali, 1994; Weiss et al., 2018b).",
      "startOffset" : 148,
      "endOffset" : 296
    }, {
      "referenceID" : 88,
      "context" : "Such studies consider how well neural networks can represent formal languages generated by grammars from different classes of the Chomsky Hierarchy (e.g. Elman, 1991; Christiansen and Chater, 1999; Rodriguez, 2001; Wiles and Elman, 1995; Rodriguez et al., 1999; Batali, 1994; Weiss et al., 2018b).",
      "startOffset" : 148,
      "endOffset" : 296
    }, {
      "referenceID" : 70,
      "context" : "Such studies consider how well neural networks can represent formal languages generated by grammars from different classes of the Chomsky Hierarchy (e.g. Elman, 1991; Christiansen and Chater, 1999; Rodriguez, 2001; Wiles and Elman, 1995; Rodriguez et al., 1999; Batali, 1994; Weiss et al., 2018b).",
      "startOffset" : 148,
      "endOffset" : 296
    }, {
      "referenceID" : 5,
      "context" : "Such studies consider how well neural networks can represent formal languages generated by grammars from different classes of the Chomsky Hierarchy (e.g. Elman, 1991; Christiansen and Chater, 1999; Rodriguez, 2001; Wiles and Elman, 1995; Rodriguez et al., 1999; Batali, 1994; Weiss et al., 2018b).",
      "startOffset" : 148,
      "endOffset" : 296
    }, {
      "referenceID" : 87,
      "context" : "Such studies consider how well neural networks can represent formal languages generated by grammars from different classes of the Chomsky Hierarchy (e.g. Elman, 1991; Christiansen and Chater, 1999; Rodriguez, 2001; Wiles and Elman, 1995; Rodriguez et al., 1999; Batali, 1994; Weiss et al., 2018b).",
      "startOffset" : 148,
      "endOffset" : 296
    }, {
      "referenceID" : 85,
      "context" : "be extracted from attention patterns (Vig and Belinkov, 2019; Mareček and Rosa, 2018; Lin et al., 2019).",
      "startOffset" : 37,
      "endOffset" : 103
    }, {
      "referenceID" : 54,
      "context" : "be extracted from attention patterns (Vig and Belinkov, 2019; Mareček and Rosa, 2018; Lin et al., 2019).",
      "startOffset" : 37,
      "endOffset" : 103
    }, {
      "referenceID" : 47,
      "context" : "be extracted from attention patterns (Vig and Belinkov, 2019; Mareček and Rosa, 2018; Lin et al., 2019).",
      "startOffset" : 37,
      "endOffset" : 103
    }, {
      "referenceID" : 11,
      "context" : "While this ‘generative’ view of language became popular with Chomsky in the early sixties (Chomsky, 1956), Chomsky himself traces it back to Von Humboldt, who stated that ‘language makes infinite use of finite means’.",
      "startOffset" : 90,
      "endOffset" : 105
    }, {
      "referenceID" : 66,
      "context" : "However, whereas systematicity can be – to some extent – empirically established, productivity cannot, as it is not possible to prove that natural languages in fact contain an infinite number of complex expressions (Pullum and Scholz, 2010).",
      "startOffset" : 215,
      "endOffset" : 240
    }, {
      "referenceID" : 59,
      "context" : "This principle, which finds its origin in philosophical logic, states that if an expression is altered by replacing one of its constituents with another constituent with the same meaning (a synonym), this does not affect the meaning of the expression (Pagin, 2003).",
      "startOffset" : 251,
      "endOffset" : 264
    }, {
      "referenceID" : 60,
      "context" : "When operations are very local (a case also referred to as strong or first-level compositionality), the meaning of a complex expression depends only on its local structure and the meanings of its immediate parts (Pagin and Westerst̊ahl, 2010; Jacobson, 2002).",
      "startOffset" : 212,
      "endOffset" : 258
    }, {
      "referenceID" : 35,
      "context" : "When operations are very local (a case also referred to as strong or first-level compositionality), the meaning of a complex expression depends only on its local structure and the meanings of its immediate parts (Pagin and Westerst̊ahl, 2010; Jacobson, 2002).",
      "startOffset" : 212,
      "endOffset" : 258
    }, {
      "referenceID" : 71,
      "context" : "One of the most well-known examples, which served also as the subject of the famous past-tense debate between symbolism and connectionism (Rumelhart and McClelland, 1986; Marcus et al., 1992), concerns the rule that English past-tense verbs can be formed by appending -ed to the stem of the verb.",
      "startOffset" : 138,
      "endOffset" : 191
    }, {
      "referenceID" : 53,
      "context" : "One of the most well-known examples, which served also as the subject of the famous past-tense debate between symbolism and connectionism (Rumelhart and McClelland, 1986; Marcus et al., 1992), concerns the rule that English past-tense verbs can be formed by appending -ed to the stem of the verb.",
      "startOffset" : 138,
      "endOffset" : 191
    }, {
      "referenceID" : 44,
      "context" : "In this sense, PCFG SET is similar to SCAN (Lake and Baroni, 2018) but differs from a task such as the lookup table task introduced by Lǐska et al.",
      "startOffset" : 43,
      "endOffset" : 66
    }, {
      "referenceID" : 51,
      "context" : "We parse this corpus with a statistical parser (Manning et al., 2014) and extract the distribution of length and depths from the annotated corpus.",
      "startOffset" : 47,
      "endOffset" : 69
    }, {
      "referenceID" : 77,
      "context" : "To showcase our compositionality test suite, we compare three currently popular neural architectures for sequence-to-sequence language processing tasks such as machine translation, speech processing and language understanding: recurrent neural networks (Sutskever et al., 2014), convolutional neural networks (Gehring et al.",
      "startOffset" : 253,
      "endOffset" : 277
    }, {
      "referenceID" : 24,
      "context" : ", 2014), convolutional neural networks (Gehring et al., 2017b) and transformer networks (Vaswani et al.",
      "startOffset" : 39,
      "endOffset" : 62
    }, {
      "referenceID" : 83,
      "context" : ", 2017b) and transformer networks (Vaswani et al., 2017).",
      "startOffset" : 34,
      "endOffset" : 56
    }, {
      "referenceID" : 42,
      "context" : "We use the LSTMS2S implementation of the OpenNMT-py framework (Klein et al., 2017).",
      "startOffset" : 62,
      "endOffset" : 82
    }, {
      "referenceID" : 23,
      "context" : "Convolutional sequenceto-sequence models have obtained competitive results in machine translation (Gehring et al., 2017a)",
      "startOffset" : 98,
      "endOffset" : 121
    }, {
      "referenceID" : 3,
      "context" : "However, convolutional networks have been found to maintain a much longer effective history than their recurrent counterparts (Bai et al., 2018).",
      "startOffset" : 126,
      "endOffset" : 144
    }, {
      "referenceID" : 18,
      "context" : "Within ConvS2S, distant portions in the input sequence may be combined primarily through the multi-step attention, which has been shown to improve the generalisation abilities of the model compared to single-step attention (Dess̀ı and Baroni, 2019).",
      "startOffset" : 223,
      "endOffset" : 248
    }, {
      "referenceID" : 83,
      "context" : "3 Transformer The last architecture we consider is the recently introduced transformer model (Vaswani et al., 2017).",
      "startOffset" : 93,
      "endOffset" : 115
    }, {
      "referenceID" : 42,
      "context" : "We use OpenNMT-py(10) (Klein et al., 2017) to train the model according to the guidelines provided by the framework(11): with the Adam optimiser (β1 = 0.",
      "startOffset" : 22,
      "endOffset" : 42
    } ],
    "year" : 2020,
    "abstractText" : "Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality of language and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests for models that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models’ composition operations are local or global (iv) if models’ predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-to-sequence models: a recurrent, a convolution-based and a transformer model. We provide an in-depth analysis of the results, which uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.",
    "creator" : "LaTeX with hyperref package"
  }
}