{
  "name" : "Capacity, Bandwidth, and Compositionality in Emergent Language Learning.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Capacity, Bandwidth, and Compositionality in Emergent Language Learning",
    "authors" : [ "Cinjon Resnick", "Abhinav Gupta", "Jakob Foerster", "Andrew M. Dai", "Kyunghyun Cho" ],
    "emails" : [ "cinjon@nyu.edu", "abhinavg@nyu.edu", "jnf@fb.com", "adai@google.com", "kyunghyun.cho@nyu.edu" ],
    "sections" : [ {
      "heading" : null,
      "text" : "KEYWORDS Multi-agent communication; Compositionality; Emergent languages ACM Reference Format: Cinjon Resnick*, AbhinavGupta*, Jakob Foerster, AndrewM.Dai, and Kyunghyun Cho. 2020. Capacity, Bandwidth, and Compositionality in Emergent Language Learning. In Proc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020), Auckland, New Zealand, May 9–13, 2020, IFAAMAS, 9 pages."
    }, {
      "heading" : "1 INTRODUCTION",
      "text" : "Compositional language learning in the context of multi agent emergent communication has been extensively studied [3, 12, 23, 37]. These works have found that while most emergent languages do not tend to be compositional, they can be guided towards this attribute through artificial task-specific constraints [20, 25].\nIn this paper, we focus on how a neural network, specifically a generative one, can learn a compositional language. Moreover,\n∗The first two authors contributed equally. 1Code is available at https://github.com/backpropper/cbc-emecom.\nProc. of the 19th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2020), B. An, N. Yorke-Smith, A. El Fallah Seghrouchni, G. Sukthankar (eds.), May 9–13, 2020, Auckland, New Zealand. © 2020 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.\nwe ask how this can occur without task-specific constraints. To accomplish this, we first define what is a language and what we mean by compositionality. In tandem, we introduce precision and recall, two metrics that help us measure how well a generative model at large has learned a grammar from a finite set of training instances. We then use a variational autoencoder with a discrete sequence bottleneck to investigate how well the model learns a compositional language, in addition to what affects that learning. This allows us to derive residual entropy, a third metric that reliably measures compositionality in our particular environment. We use this metric to cross-validate precision and recall. Finally, while there may be a connection between our study and human-level languages, we do not experiment with human data or languages in this work.\nOur environment lets us experiment with a syntactic, compositional language while varying the channel width and the number of parameters, our surrogate for the capacity of a model. Our experiments reveal that our smallest models are only able to solve the task when the channel is wide enough to allow for a surfacelevel compositional representation. In contrast, large models learn a language as long as the channel is large enough. However, large models also have the ability to memorize the training set. We hypothesize that this memorization would lead to non-compositional representations and overfitting, albeit this does not yet manifest empirically. This setup allows us to test our hypothesis that there is a network capacity above which models will tend to produce languages with non-compositional structure."
    }, {
      "heading" : "2 RELATEDWORK",
      "text" : "There has recently been renewed interest in studies of emergent language [12, 13, 23] that originated with works such as [35, 36]. Some of these approaches use referential games [11, 22, 29] to produce an emergent language that ideally has properties of human languages, with compositionality being a commonly sought after property [3, 4, 6].\nOur paper is most similar to [20], which showed that compositional language arose only when certain constraints on the agents are satisfied. While the constraints they examined were either making their models memoryless or having a minimal vocabulary in the language, we hypothesized about the importance for agents to have small capacity relative to the number of disentangled representations (concepts) to which they are exposed. This is more general because both of the scenarios they described fall under\nar X\niv :1\n91 0.\n11 42\n4v 2\n[ cs\n.C L\n] 1\nF eb\n2 02\nthe umbrella of reducing model capacity. To ask this, we built a much bigger dataset to illuminate how capacity and channel width effect the resulting compositionality in the language. Liska[28] suggests that the average training run for recurrent neural networks does not converge to a compositional solution, but that a large random search will produce compositional solutions. This implies that the optimization approach biases learning, which is also confirmed in our experiments. However, we further analyze other biases. Spike[34] describes three properties that bias models towards successful learned signaling: the creation and transmission of referential information, a bias against ambiguity, and information loss. This lies on a similar spectrum to our work, but pursues a different intent in that they study biases that lead to optimal signaling; we seek compositionality. Each of [19, 38, 40] examine the trade-off between expression and compression in both emergent and natural languages, in addition to how that trade-off affects the learners. We differ in that we target a specific aspect of the agent (capacity) and ask how that aspect biases the learning. Chen[8] describes how the probability distribution on the set of all strings produced by a recurrent model can be interpreted as a weighted language; this is relevant to our formulation of the language.\nMost other works studying compositionality in emergent languages [1, 7, 24, 31] have focused on learning interpretable representations. See [15] for a broad survey of the different approaches. By and large, these are orthogonal to our work because none pose the question we ask - how does an agent’s capacity effect the resulting language’s compositionality?"
    }, {
      "heading" : "3 COMPOSITIONAL LANGUAGE AND LEARNING",
      "text" : "We start by defining a language and what it means for a language to be compositional. We then discuss what it means for a network\nto learn a compositional language, based on which we derive evaluation metrics."
    }, {
      "heading" : "3.1 Compositional Language",
      "text" : "A language L is a subset of Σ∗, where Σ denotes an alphabet and s denotes a string:\nΣ∗ = {s | ∀i = 1, . . . , |s | si ∈ Σ ∧ |s | ≥ 0} . In this paper, we constrain a language to contain only finite-length strings, i.e., |s | < ∞, implying that L is a finite language. We use Kmax to denote the maximum length of any s ∈ L.\nWe define a generator G from which we can sample one valid string s ∈ L at a time. It never generates an invalid string and generates all the valid strings in L in finite time such that\ns ∼ G ⇔ s ∈ L. (1) We define the length |G | of the description of G as the sum of the number of non-terminal symbols N and the number of production rules P, where |N | < ∞ and |P | < ∞. Each production rule ρ ∈ P takes as input an intermediate string s ′ ∈ (Σ ∪ N)∗ and outputs another string s ′′ ∈ (Σ ∪N)∗. The generator starts from an empty string ∅ and applies an applicable production rule (uniformly selected at random) until the output string consists only of characters from the alphabet (terminal symbols).\nLanguages and compositionality. When the number of such production rules |P | plus the number of intermediate symbols |N | is smaller than the size |L| of the language thatG generates, we call L a compositional language. In other words, L is compositional if and only if |L| > |P | + |N |.\nOne such example is when we have sixty characters in the alphabet, Σ = {0, 1, 2, . . . , 59}, and six intermediate symbols, N = {C1,C2, . . . ,C6}, for a total of 6 × 10 + 1 production rules P:\n• ∅ → C1C2C3C4C5C6. • For each i ∈ [1, 6],Ci → w , wherew ∈ {10i − 10, . . . , 10i − 1}\nFrom these production rules and intermediate symbols, we obtain a language of size 106 ≫ 67 = |P | + |N |. We thus consider this language to be compositional and will use it in our experiments."
    }, {
      "heading" : "3.2 Learning a language",
      "text" : "We consider the problem of learning an underlying language L⋆ from a finite set of training strings randomly drawn from it:\nD = { s |s ∼ G⋆ } whereG⋆ is the minimal length generator associated with L⋆. We assume |D | ≪ |L⋆ | and our goal is to use D to learn a language L that approximates L⋆ as well as possible. We know that there exists an equivalent generatorG for L, and so our problem becomes estimating a generator from this finite set rather than reconstructing an entire set of strings belonging to the original language L∗.\nWe cast the problem of estimating a generator G as density modeling, in which case the goal is to estimate a distribution p(s). Sampling from p(s) is equivalent to generating a string from the generator G. Language learning is then\nmax p∈M 1 |D | |D |∑ n=1 logp(sn ) + λR(p), (2)\nwhere R is a regularization term, λ its strength, andM is a model space.\nEvaluation metrics. When the language was learned perfectly, any string sampled from the learned distribution p(s) must belong to L⋆. Also, any string in L⋆ must be assigned a non-zero probability under p(s). Otherwise, the set of strings generated from this generator, implicitly defined via p(s), is not identical to the original language L⋆. This observation leads to two metrics for evaluating the quality of the estimated language with the distribution p(s), precision and recall:\nPrecision(L⋆,p) = 1|L⋆ | ∑ s ∈L I(s ∈ L⋆) (3)\nRecall(L⋆,p) = ∑ s ∈L⋆ logp(s) (4)\nwhere I(x) is the indicator function. Thesemetrics are designed to be fit for any compositional structure rather than one-off evaluation approaches. Because these are often intractable to compute, we approximate them using Monte-Carlo by sampling N samples from p(s) for calculating precision andM uniform samples from L⋆ for calculating recall.\nPrecision(L⋆,p) ≈ 1 N N∑ n=1 I(sn ∈ L⋆) (5)\nRecall(L⋆,p) ≈ M∑\nm=1 logp(sm ), (6)\nwhere sn ∼ p(s) and sm is a uniform sample from L⋆."
    }, {
      "heading" : "3.3 Compositionality, learning, and capacity",
      "text" : "When learning an underlying compositional language L⋆, there are three possible outcomes:\nOverfitting: p(s) could memorize all the strings that were presented when solving Eq. (2) and assign non-zero probabilities to those strings and zero probabilities to all others. This would maximize precision, but recall will be low as the estimated generator does not cover L⋆.\nSystematic generalization: p(s) could capture the underlying compositional structures of L⋆ characterized by the production rules P and intermediate symbols N . In this case, p(s) will assign non-zero probabilities to all the strings that are reachable via these production rules (and zero probability to all others) and generalize to strings from L⋆ that were unseen during training, leading to high precision and recall. This behavior was characterized in Lake [21].\nFailure: p(s) may neither memorize the entire training set nor capture the underlying production rules and intermediate symbols, resulting in both low precision and recall.\nWe hypothesize that a major factor determining the compositionality of the resulting language is the capacity of the most complicated distribution p(s) within the model space M.2 When the model capacity is too high, the first case of total memorization is likely. When it is too low, the third case of catastrophic failure will happen. Only when the model capacity is just right will language learning correctly capture the compositional structure underlying the original language L⋆ and exhibit systematic generalization [2]. We empirically investigate this hypothesis using a neural network\n2 As the definition of a model’s capacity heavily depends on the specific construction, we do not concretely define it here but do later when we introduce a specific family of models with which we run experiments.\nas a language learner, in particular a variational autoencoder with a discrete sequence bottleneck. This admits the interpretation of a two-player ReferIt game [26] in addition to being a density estimator of p(s). Together, these let us use recall and precision to analyze the resulting emergent language."
    }, {
      "heading" : "4 VARIATIONAL AUTOENCODERS AND THEIR CAPACITY",
      "text" : "A variational autoencoder [18] consists of two neural networks which are often referred to as an encoder fθ , a decoder дϕ , and a prior pλ . These two networks are jointly updated to maximize the variational lower bound to the marginal log-probability of training instances s:\nL(θ ,ϕ, λ; s) = Ez∼fθ (z |s) [ logдϕ (s |z) ] − KL(fθ (z |s)∥pλ). (7)\nWe use L as a proxy to the true p(s) captured by this model. Once trained, we can efficiently sample z̃ from the prior pλ and then sample a string from дϕ (s |z̃).\nThe usual formulation of variational autoencoders uses a continuous latent variable z, which conveniently admits reparametrization that reduces the variance in the gradient estimate. However, this infinitely large latent variable space makes it difficult to understand the resulting capacity of the model. We thus constrain the latent variable to be a binary string of a fixed length l , i.e., z ∈ {0, 1}l . Assuming deterministic decoding, i.e., argmaxs logдϕ (s |z), this puts a strict upperbound of 2l on the size of the language |L| captured by the variational autoencoder."
    }, {
      "heading" : "4.1 Variational autoencoder as a communication channel",
      "text" : "As described above, using variational autoencoders with a discrete sequence bottleneck allows us to analyze the capacity of the model in terms of computation and bandwidth. We can now interpret this variational autoencoder as a communication channel in which a novel protocol must emerge as a by-product of learning. We will refer to the encoder as the speaker and the decoder as the listener when deployed in this communication game. If each string s ∈ L⋆ in the original language is a description of underlying concepts, then the goal of the speaker fθ is to encode those concepts in a binary string z following an emergent communication protocol. The listener дϕ receives this string and must interpret which set of concepts were originally seen by the speaker.\nOur setup. We simplify and assume that each of the characters in the string s ∈ L⋆ correspond to underlying concepts. While the inputs are ordered according to the sequential concepts, our model encodes them using a bag of words (BoW) representation.\nThe speaker fθ is parameterized using a recurrent policy which receives the sequence of concatenated one-hot input tokens of s and converts each of them to an embedding. It then runs an LSTM [14] non-autoregressively for l timesteps taking the flattened representation of the input embeddings as its input and linearly projecting each result to a probability distribution over {0, 1}. This\nresults in a sequential Bernoulli distribution over l latent variables:\nfθ (z |s) = l∏\nt=1 p(zt |s;θ )\nFrom this distribution, we can sample a latent string z = (z1, . . . , zl ). The listener дϕ receives z and uses a BoW representation to encode them into its own embedding space. Taking the flattened representation of these embeddings as input, we run an LSTM for |N | time steps, each time outputting a probability distribution over the full alphabet Σ:\nдϕ (s |z) = |N |∏ j=1 p(sj |z;ϕ)\nTo train the whole system end-to-end [31, 37] via backpropogation, we apply a continuous approximation to zt that depends on a learned temperature parameter τ . We use the ‘straight-through‘ version of Gumbel-Softmax [16, 30] to convert the continuous distribution to a discrete distribution for each zt . This corresponds to"
    }, {
      "heading" : "5.2 for detailed analysis. Panels (a) and (f) show the accuracy of the training data, (b) and (d) show entropy, (e) and (g) show recall over the test data, and (c) plots the max difference in accuracy between training and test.",
      "text" : "the original discrete distribution in the zero-temperature limit. The final sequence of one hot vectors encoding z is our message, which is passed to the listener дϕ . If G j ∼ Gumbel(0, 1) and the Bernoulli random variable corresponding to zt has class probabilities zt0 and zt1 , then zt = one_hot(argmax\nj [G j + log ztj ]).\nThe prior pλ encodes the message z using a BoW representation. It gives the probability of z according to the prior (binary) distribution for each zt and is defined as:\npλ(z) = l∏\nt=1 p(zt |λ).\nThis can be used both to compute the prior probability of a latent string and also to efficiently sample from pλ using ancestral sampling. Penalizing the KL divergence between the speaker’s distribution and the prior distribution in Eq. (7) encourages the emergent protocol to use latent strings that are as diverse as possible."
    }, {
      "heading" : "4.2 Capacity of a variational autoencoder with discrete sequence bottleneck",
      "text" : "This view of a variational autoencoder with discrete sequence bottleneck presents an opportunity for us to separate the model’s capacity into two parts. The first part is the capacity of the communication channel, imposed by the size of the latent variable. As described earlier, the size of the original language L⋆ that can be perfectly captured by this model is strictly upper bounded by 2l , where l is the preset length of the latent string z. If l < log2 |L⋆ |, the model will not be able to learn the language completely, although it may memorize all the training strings s ∈ D if l ≥ log2 |D |. A resulting question is whether 2l ≥ |L⋆ | is a sufficient condition for the model to learn L⋆ from a finite set of training strings.\nThe second part involves the capacity of the speaker and listener to map between the latent variable z and a string s in the original language L⋆. Taking the parameter count as a proxy to the number of patterns that could be memorized by a neural network,3 we can argue that the problem can be solved if the speaker and listener\n3 This is true in certain scenarios such as radial-basis function networks.\neach have Ω(l |L⋆ |) parameters, in which case they can implement a hashmap between a string in the original language L⋆ and that of the learned latent language defined by the z strings.\nHowever, when the underlying language is compositional as defined in §3.1, we can have a much more compact representation of the entire language than a hashmap. Given the status quo understanding of neural networks, it is impossible to correlate the parameter count with the language specification (production rules and intermediate symbols) and the complexity of using that language. It is however reasonable to assume that there is a monotonic relationship between the number of parameters |θ |, or |ϕ |, and the capacity of the network to encode the compositional structures underlying the original language [9]. Thus, we use the parameter count as a proxy to measure the capacity.\nIn summary, there are two axes in determining the capacity of the proposed model: the length of the latent sequence l and the number of parameters |θ | (|ϕ |) in the speaker (listener).4 We vary these two quantities in the experiments and investigate how they affect compositional language learning by the model."
    }, {
      "heading" : "4.3 Implications and hypotheses on compositionality",
      "text" : "Under this framework for language learning, we can make the following observations:\n• If the length of the latent sequence l < log2 |L⋆ |, it is impossible for the model to avoid the failure case because there will be |L⋆ | − 2l strings in L⋆ that cannot be generated from the trained model. Consequently, recall cannot be maximized. However, this may be difficult to check using the sample-based estimate as the chance of sampling s ∈ L⋆\\ ∫ дϕ (s |z)pλ(z)dz decreases proportionally to the\nsize of L⋆. This is especially true when the gap |L⋆ | − 2l is narrow. • When l ≥ log2 |L⋆ |, there are three cases. The first is when there are not enough parameters θ to learn the underlying\n4 We design the variational autoencoder to be symmetric so that the parameter counts of the speaker and the listener are roughly the same.\ncompositional grammar given by P,N , and Σ, in which case L⋆ cannot be learned. The second case is when the number of parameters |θ | is greater than that required to store all the training strings, i.e., |θ | = O(l |D |). Here, it is highly likely for the model to overfit as it can map each training string with a unique latent string without having to learn any of L⋆’s compositional structure. Lastly, when the number of parameters lies in between these two poles, we hypothesize that the model will capture the underlying compositional structure and exhibit systematic generalization.\nIn short, we hypothesize that the effectiveness of compositional language learning is maximized when both the length of the latent sequence is large enough (l ≥ log |L⋆ |), and the number of parameters |θ | is between k(|P| + |N | + |Σ|) and kl |D | for some positive integer k . Our experiments test this by varying the length of the latent sequence l and the number of parameters |θ | while checking the sample-based estimates of precision and recall (Eq. (5)–(6))."
    }, {
      "heading" : "5 EXPERIMENTS",
      "text" : "Data. As described in §3.1, we run experiments where the size of the language is much larger than the number of production rules. The task is to communicate 6 concepts, each of which have 10 possible values with a total dataset size of 106. We build three finite datasets Dtrain, Dval, Dtest:\nStrain = { s ∈ L⋆ | s , (∗Cval1 ∗C val 2 ∗) ∧ s , (∗C test 1 ∗C test 2 ∗) } Sval = { s = (∗Cval1 ∗C val 2 ∗)\n} Stest = { s = (∗Ctest1 ∗C test 2 ∗) ∧ s < Dval\n} Dtrain = subsample (Strain,Ntrain) Dval = subsample (Sval,Nval) Dtest = subsample (Stest,Nval) ,\nwhere subsample(S,N ) uniformly selects N random items from S without replacement. The randomly selected concept values (Cval1 ,C val 2 ) and (C test 1 ,C test 2 ) ensure that concept combinations are unique to each set. The ∗ symbol refers to any number of concepts, as in regular expressions.\nModels and Learning. We train the proposed variational autoencoder (described in §4.1) on Dtrain, using the Adam optimizer [17] with a learning rate of 3×10−3, weight decay coefficient of 10−4 and a batch size of 1000. The Gumbel-Softmax temperature parameter τ in is initialized to 1. Since systematic generalization may only\nhappen in some training runs [39], each model is trained for each of 10 seeds over 200k steps. We trained our models using [33].\nWe train two sets of models. Each set is built from an independent base model, the architectures of which are described in Table 1. We gradually decrease the number of LSTM units from the base model by a factor α ∈ (0, 1]. This is how we control the number of parameters (|θ | and |ϕ |), a factor we hypothesize to influence the resulting compositionality. We obtain seven models from each of these by varying the length of the latent sequence l from {19, 20, 21, 22, 23, 24, 25}. These were chosen because we both wanted to show a range of bits and because we need at least 20 bits to cover the 106 strings in L∗ (⌈log2 106⌉ = 20).\nNote that results for the two models are similar and so we arbitrarily spotlight one of them - model A. We additionally show the results for model B in Figs. 5, 6, and 8, but do not dive into its results in the main section."
    }, {
      "heading" : "5.1 Evaluation: Residual Entropy",
      "text" : "Our setup allows us to design a metric by which we can check the compositionality of the learned language L by examining how the underlying concepts are described by a string. For instance, (2, 11, 24, 31, 44, 56) ∈ L⋆ describes that C1 = 2, C2 = 11, C3 = 24, C4 = 31, C5 = 44 and C6 = 56. Furthermore, we know that the value of a concept Ci is independent of the other concepts Cj,i , and so our custom generative setup with a discrete latent sequence allows us to inspect a learned language L by considering z.\nLet p be a sequence of partitions of {1, 2, . . . , l}. We define the degree of compositionality as the ratio between the variability of each concept Ci and the variability explained by a latent subsequence z[pi ] indexed by an associated partition pi . More formally, the degree of compositionality given the partition sequence p is defined as a residual entropy\nre(p,L,L⋆) = 1|N | |N |∑ i=1 HL(Ci |z[pi ])/HL⋆ (Ci )\nwhere there are |N | concepts by the definition of our language. When each term inside the summation is close to zero, it implies that a subsequence z[pi ] explains most of the variability of the specific concept Ci , and we consider this situation compositional. The residual entropy of a trained model is then the smallest re(p) over all possible sequences of partitions P and spans from 0 (compositional) to 1 (non-compositional).\nre(L,L⋆) = min p∈P re(p,L,L⋆)."
    }, {
      "heading" : "5.2 Results",
      "text" : "Fig. 4 shows the main findings of our research. In plot (a), we see the parameter counts at the threshold. Below these values, the model cannot solve the task but above these, it can solve it. Further, observe the curve delineated by the lower left corner of the shift from unsuccessful to successful models. This inverse relationship between bits and parameters shows that the more parameters in the model, the fewer bits it needs to solve the task. Note however that it could only solve the task with fewer bits if it was forming a non-compositional code, suggesting that higher parameter models are able to do so while lower parameter ones cannot.\nObserve further that all of ourmodels above theminimum threshold (72,400) have the capacity to learn a compositional code. This is shown by the perfect training accuracy achieved by all of those models in plot (a) for 24 bits and by the perfect compositionality (zero entropy) in plot (b) for 24 bits. Together with the above, this validates that learning compositional codes requires less capacity than learning non-compositional codes.\nPlot (c) confirms our hypothesis that large models can memorize the entire dataset. The 24 bit model with 971,400 parameters achieves a train accuracy of 1.0 and a validation accuracy of 0.0. Cross-validating this with plots (d) and (g), we find that a member of the same parameter class is non-compositional and that there is one that achieves unusually low recall. We verified that these are all the same seed, which shows that the agents in this model are memorizing the dataset. This is supplemented by Fig. 7 where we see the non-compositional behavior by plotting residual entropy versus overfitting.\nPlots (b) and (e) show that our compositionality metrics pass two sanity checks - high recall and perfect entropy can only be achieved with a channel that is sufficiently large (i.e. 24 bits) to allow for a compositional latent representation.\nPlot (f) shows that while the capacity does not affect the ability to learn a compositional language across the model range, it does change the learnability. Here we find that smaller models can fail to solve the task for any bandwidth, which coincides with literature suggesting a link between overparameterization and learnability [10, 27]. This is supported by the efficacy results in Fig. 8.\nFinally, as expected, we find that no model learns to solve the task with < 20 bits, validating that the minimum required number of bits for learning a language of size |L| is ⌈log(|L|)⌉. We also see that no model learns to solve it for 20 bits, which is likely due to optimization difficulties.\nIn Fig. 2, we present histograms showing precision, recall and residual entropy measured for each bit and parameter combination over the test set. The histograms show the distributions of these metrics, upon which we make a number of observations.\nWe first confirm the effectiveness of training by observing that almost all the models achieve perfect precision (Fig. 2 (a)), implying that L ⊆ L⋆, where L is the language learned by the model. This occurs even with our learning objective in Eq. (7) encouraging the model to capture all training strings rather than to focus on only a few training strings.\nA natural follow-up question is how large is L⋆\\L. We measure this with recall in Fig. 2 (b), which shows a clear phase transition according to the model capacity when l ≥ 22. This agrees with what we saw in Fig. 4 and is equivalent to saying |L⋆\\L| ≫ 0 at a value that is close to our predicted boundary of l = ⌈log2 106⌉ = 20. We attribute this gap to the difficulty in learning a perfectlyparameterized neural network.\nIn Fig. 7 we show the empirical relation between entropy and overfitting over the parameter range. While it was hard for the models to overfit, there were some in bits 23 and 24 that did do so. For those models, the entropy was also found to be higher relative to the other models.\nEven when l ≥ 22, we observe training runs that fail to achieve optimal recall when the number of parameters is ≤ 365800 (Fig. 2). Due to insufficient understanding of the relationship between the number of parameters and the capacity in a deep neural network, we cannot make a rigorous conclusion. We however conjecture that this is the upperbound to the minimal model capacity necessary to capture the tested compositional language. Above this threshold, the recall is almost always perfect, implying that the model has likely captured the compositional structure underlying L⋆ from a finite set of training strings. We run further experiments up to 1.5M parameters, but do not observe the expected overfitting.\nAs shown in Fig. 3, we also run experiments with the number of categories reduced from 6 to 4 and similarly do not find the upperbound. It is left for future studies to determinewhy. Two conjectures that we have are that it is either due to insufficient model capacity to memorize the hash map between all the strings in L⋆ and 2l latent strings or due to an inclination towards compositionality in our variational autoencoder.\nThese results clearly confirm the first part of our hypothesis - the latent sequence length must be at least as large as log |L⋆ |. They also confirm that there is a lowerbound on the number of parameters over which this model can successfully learn the underlying language. We have not been able to verify the upper bound in our experiments, which may require either a more (computationally) extensive set of experiments with even more parameters or a better theoretical understanding of the inherent biases behind learning with this architecture, such as from recent work on overparameterized models [5, 32]."
    }, {
      "heading" : "6 CONCLUSION",
      "text" : "In this paper, we hypothesize a thus far ignored connection between learnability, capacity, bandwidth, and compositionality for language learning. We empirically verfiy that learning the underlying compositional structure requires less capacity than memorizing a dataset. We also introduce a set of metrics to analyze the compositional properties of a learned language. These metrics are not only well motivated by theoretical insights, but are cross-validated by our task-specific metric.\nThis paper opens the door for a vast amount of follow-up research. All our models were sufficiently large to represent the compositional structure of the language when given sufficient bandwidth, however there should be an upper bound for representing this compositional structure that we did not reach. We consider answering that to be the foremost question.\nFurthermore, while large models did overfit, this was an exception rather than the rule. We hypothesize that this is due to the large number of examples in our language, which almost forces the model to generalize, but note that there are likely additional biases at play that warrant further investigation."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "We would like to thank Marco Baroni and Angeliki Lazaridou for their comments on an earlier version of the paper. We would also like to thank the anonymous reviewers for giving insightful feedback in turn enhancing this work, particularly reviewer two for their thoroughness. Special thanks to Adam Roberts, Doug Eck, Mohammad Norouzi, and Jesse Engel."
    } ],
    "references" : [ {
      "title" : "Measuring Compositionality in Representation Learning",
      "author" : [ "Jacob Andreas" ],
      "venue" : "In International Conference on Learning Representations. https://openreview.net/",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2019
    }, {
      "title" : "Systematic Generalization: What Is Required and Can It Be Learned",
      "author" : [ "Dzmitry Bahdanau", "Shikhar Murty", "Michael Noukhovitch", "Thien Huu Nguyen", "Harm de Vries", "Aaron Courville" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2019
    }, {
      "title" : "Linguistic generalization and compositionality in modern artificial neural networks",
      "author" : [ "Marco Baroni" ],
      "venue" : "Philosophical Transactions of the Royal Society B: Biological Sciences",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2020
    }, {
      "title" : "Hierarchical Models for the Evolution of Compositional Language",
      "author" : [ "Jeffrey A. Barrett", "Brian Skyrms", "Calvin Cochran" ],
      "venue" : null,
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2018
    }, {
      "title" : "Reconciling modern machine-learning practice and the classical bias–variance trade-off",
      "author" : [ "Mikhail Belkin", "Daniel Hsu", "Siyuan Ma", "Soumik Mandal" ],
      "venue" : "Proceedings of the National Academy of Sciences 116,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2019
    }, {
      "title" : "Anti-efficient encoding in emergent communication",
      "author" : [ "Rahma Chaabouni", "Eugene Kharitonov", "Emmanuel Dupoux", "Marco Baroni" ],
      "venue" : "In Advances in Neural Information Processing Systems 32,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 2019
    }, {
      "title" : "Word-order biases in deep-agent emergent communication. arXiv:1905.12330 [cs] (May 2019)",
      "author" : [ "Rahma Chaabouni", "Eugene Kharitonov", "Alessandro Lazaric", "Emmanuel Dupoux", "Marco Baroni" ],
      "venue" : null,
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2019
    }, {
      "title" : "Recurrent Neural Networks as Weighted Language Recognizers",
      "author" : [ "Yining Chen", "Sorcha Gilroy", "Andreas Maletti", "Jonathan May", "Kevin Knight" ],
      "venue" : null,
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 2018
    }, {
      "title" : "Capacity and Trainability in Recurrent Neural Networks",
      "author" : [ "Jasmine Collins", "Jascha Sohl-Dickstein", "David Sussillo" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2017
    }, {
      "title" : "Gradient Descent Provably Optimizes Over-parameterized Neural Networks",
      "author" : [ "Simon S. Du", "Xiyu Zhai", "Barnabas Poczos", "Aarti Singh" ],
      "venue" : "In International Conference on Learning Representations. https://openreview.net/forum?id=",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 2019
    }, {
      "title" : "Emergent Communication in a Multi-Modal, Multi-Step Referential Game",
      "author" : [ "Katrina Evtimova", "Andrew Drozdov", "Douwe Kiela", "Kyunghyun Cho" ],
      "venue" : "In International Conference on Learning Representations. https://openreview.net/",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2018
    }, {
      "title" : "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
      "author" : [ "Jakob Foerster", "Ioannis Alexandros Assael", "Nando de Freitas", "Shimon Whiteson" ],
      "venue" : "In Advances in Neural Information Processing Systems 29,",
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2016
    }, {
      "title" : "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols",
      "author" : [ "Serhii Havrylov", "Ivan Titov" ],
      "venue" : "In Advances in Neural Information Processing Systems 30,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2017
    }, {
      "title" : "Long Short-Term Memory",
      "author" : [ "Sepp Hochreiter", "Jürgen Schmidhuber" ],
      "venue" : "Neural Computation 9,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1997
    }, {
      "title" : "The compositionality of neural networks: integrating symbolism and connectionism",
      "author" : [ "Dieuwke Hupkes", "Verna Dankers", "Mathijs Mul", "Elia Bruni" ],
      "venue" : "Journal of Artificial Intelligence Research",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 2020
    }, {
      "title" : "Categorical Reparameterization with Gumbel-Softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2017
    }, {
      "title" : "Adam: A Method for Stochastic Optimization",
      "author" : [ "Diederik P. Kingma", "Jimmy Ba" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2015
    }, {
      "title" : "Auto-encoding Variational Bayes",
      "author" : [ "Diederik P Kingma", "Max Welling" ],
      "venue" : "In International Conference on Learning Representations. https://openreview.net/",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2014
    }, {
      "title" : "Compression and Communication in the Cultural Evolution of Linguistic Structure",
      "author" : [ "Simon Kirby", "Monica Tamariz", "Hannah Cornish", "Kenny Smith" ],
      "venue" : "Cognition",
      "citeRegEx" : "19",
      "shortCiteRegEx" : "19",
      "year" : 2015
    }, {
      "title" : "Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog",
      "author" : [ "Satwik Kottur", "José Moura", "Stefan Lee", "Dhruv Batra" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
      "citeRegEx" : "20",
      "shortCiteRegEx" : "20",
      "year" : 2017
    }, {
      "title" : "Compositional generalization through meta sequence-to-sequence learning",
      "author" : [ "Brenden M Lake" ],
      "venue" : "In Advances in Neural Information Processing Systems 32,",
      "citeRegEx" : "21",
      "shortCiteRegEx" : "21",
      "year" : 2019
    }, {
      "title" : "Emergence of Linguistic Communication from Referential Games with Symbolic and Pixel Input",
      "author" : [ "Angeliki Lazaridou", "Karl Moritz Hermann", "Karl Tuyls", "Stephen Clark" ],
      "venue" : "In International Conference on Learning Representations. https: //openreview.net/forum?id=HJGv1Z-AW",
      "citeRegEx" : "22",
      "shortCiteRegEx" : "22",
      "year" : 2018
    }, {
      "title" : "2017. Multi- Agent Cooperation and the Emergence of (Natural) Language",
      "author" : [ "Angeliki Lazaridou", "Alexander Peysakhovich", "Marco Baroni" ],
      "venue" : "In International Conference on Learning Representations. https://openreview.net/forum?id=",
      "citeRegEx" : "23",
      "shortCiteRegEx" : "23",
      "year" : 2017
    }, {
      "title" : "Countering Language Drift via Visual Grounding",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Douwe Kiela" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association",
      "citeRegEx" : "24",
      "shortCiteRegEx" : "24",
      "year" : 2019
    }, {
      "title" : "Emergent Translation in Multi-Agent Communication",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Jason Weston", "Douwe Kiela" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "25",
      "shortCiteRegEx" : "25",
      "year" : 2018
    }, {
      "title" : "Convention: A philosophical study",
      "author" : [ "David Lewis" ],
      "venue" : null,
      "citeRegEx" : "26",
      "shortCiteRegEx" : "26",
      "year" : 1969
    }, {
      "title" : "Learning Overparameterized Neural Networks via Stochastic Gradient Descent on Structured Data",
      "author" : [ "Yuanzhi Li", "Yingyu Liang" ],
      "venue" : "In Advances in Neural Information Processing Systems",
      "citeRegEx" : "27",
      "shortCiteRegEx" : "27",
      "year" : 2018
    }, {
      "title" : "Memorize or generalize? Searching for a compositional RNN in a haystack",
      "author" : [ "Adam Liska", "Germán Kruszewski", "Marco Baroni" ],
      "venue" : null,
      "citeRegEx" : "28",
      "shortCiteRegEx" : "28",
      "year" : 2018
    }, {
      "title" : "On the interaction between supervision and self-play in emergent communication",
      "author" : [ "Ryan Lowe", "Abhinav Gupta", "Jakob Foerster", "Douwe Kiela", "Joelle Pineau" ],
      "venue" : "In International Conference on Learning Representations. https: //openreview.net/forum?id=rJxGLlBtwH",
      "citeRegEx" : "29",
      "shortCiteRegEx" : "29",
      "year" : 2020
    }, {
      "title" : "The Concrete Distribution: A Continuous Relaxation of Discrete RandomVariables",
      "author" : [ "Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh" ],
      "venue" : "In International Conference on Learning Representations",
      "citeRegEx" : "30",
      "shortCiteRegEx" : "30",
      "year" : 2017
    }, {
      "title" : "Emergence of Grounded Compositional Language in Multi-Agent Populations",
      "author" : [ "Igor Mordatch", "Pieter Abbeel" ],
      "venue" : "In AAAI Conference on Artificial Intelligence",
      "citeRegEx" : "31",
      "shortCiteRegEx" : "31",
      "year" : 2018
    }, {
      "title" : "Deep Double Descent:Where BiggerModels andMore Data Hurt",
      "author" : [ "Preetum Nakkiran", "Gal Kaplun", "Yamini Bansal", "Tristan Yang", "Boaz Barak", "Ilya Sutskever" ],
      "venue" : "In International Conference on Learning Representations. https://openreview.net/",
      "citeRegEx" : "32",
      "shortCiteRegEx" : "32",
      "year" : 2020
    }, {
      "title" : "PyTorch: An Imperative Style, High-Performance Deep Learning Library",
      "author" : [ "Adam Paszke", "Sam Gross", "Francisco Massa" ],
      "venue" : null,
      "citeRegEx" : "33",
      "shortCiteRegEx" : "33",
      "year" : 2019
    }, {
      "title" : "Minimal Requirements for the Emergence of Learned Signaling",
      "author" : [ "Matthew Spike", "Kevin Stadler", "Simon Kirby", "Kenny Smith" ],
      "venue" : "In Cognitive Science",
      "citeRegEx" : "34",
      "shortCiteRegEx" : "34",
      "year" : 2017
    }, {
      "title" : "The Synthetic Modeling of Language Origins",
      "author" : [ "Luc Steels" ],
      "venue" : "Evolution of Communication",
      "citeRegEx" : "35",
      "shortCiteRegEx" : "35",
      "year" : 1997
    }, {
      "title" : "AIBO’s first words: The social learning of language and meaning",
      "author" : [ "Luc Steels", "Frédéric Kaplan" ],
      "venue" : "Evolution of Communication",
      "citeRegEx" : "36",
      "shortCiteRegEx" : "36",
      "year" : 2000
    }, {
      "title" : "Learning Multiagent Communication with Backpropagation",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus" ],
      "venue" : null,
      "citeRegEx" : "37",
      "shortCiteRegEx" : "37",
      "year" : 2016
    }, {
      "title" : "Iconicity and the Emergence of Combinatorial Structure in Language",
      "author" : [ "Tessa Verhoef", "Simon Kirby", "Bart de Boer" ],
      "venue" : "Cognitive Science 40,",
      "citeRegEx" : "38",
      "shortCiteRegEx" : "38",
      "year" : 2016
    }, {
      "title" : "The Fine Line between Linguistic Generalization and Failure in Seq2Seq-Attention Models",
      "author" : [ "Noah Weber", "Leena Shekhar", "Niranjan Balasubramanian" ],
      "venue" : "In Proceedings of the Workshop on Generalization in the Age of Deep Learning. Association for Computational Linguistics,",
      "citeRegEx" : "39",
      "shortCiteRegEx" : "39",
      "year" : 2018
    }, {
      "title" : "Efficient compression in color naming and its evolution",
      "author" : [ "Noga Zaslavsky", "Charles Kemp", "Terry Regier", "Naftali Tishby" ],
      "venue" : "Proceedings of the National Academy of Sciences 115,",
      "citeRegEx" : "40",
      "shortCiteRegEx" : "40",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 2,
      "context" : "Compositional language learning in the context of multi agent emergent communication has been extensively studied [3, 12, 23, 37].",
      "startOffset" : 114,
      "endOffset" : 129
    }, {
      "referenceID" : 11,
      "context" : "Compositional language learning in the context of multi agent emergent communication has been extensively studied [3, 12, 23, 37].",
      "startOffset" : 114,
      "endOffset" : 129
    }, {
      "referenceID" : 22,
      "context" : "Compositional language learning in the context of multi agent emergent communication has been extensively studied [3, 12, 23, 37].",
      "startOffset" : 114,
      "endOffset" : 129
    }, {
      "referenceID" : 36,
      "context" : "Compositional language learning in the context of multi agent emergent communication has been extensively studied [3, 12, 23, 37].",
      "startOffset" : 114,
      "endOffset" : 129
    }, {
      "referenceID" : 19,
      "context" : "These works have found that while most emergent languages do not tend to be compositional, they can be guided towards this attribute through artificial task-specific constraints [20, 25].",
      "startOffset" : 178,
      "endOffset" : 186
    }, {
      "referenceID" : 24,
      "context" : "These works have found that while most emergent languages do not tend to be compositional, they can be guided towards this attribute through artificial task-specific constraints [20, 25].",
      "startOffset" : 178,
      "endOffset" : 186
    }, {
      "referenceID" : 11,
      "context" : "There has recently been renewed interest in studies of emergent language [12, 13, 23] that originated with works such as [35, 36].",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 12,
      "context" : "There has recently been renewed interest in studies of emergent language [12, 13, 23] that originated with works such as [35, 36].",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 22,
      "context" : "There has recently been renewed interest in studies of emergent language [12, 13, 23] that originated with works such as [35, 36].",
      "startOffset" : 73,
      "endOffset" : 85
    }, {
      "referenceID" : 34,
      "context" : "There has recently been renewed interest in studies of emergent language [12, 13, 23] that originated with works such as [35, 36].",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 35,
      "context" : "There has recently been renewed interest in studies of emergent language [12, 13, 23] that originated with works such as [35, 36].",
      "startOffset" : 121,
      "endOffset" : 129
    }, {
      "referenceID" : 10,
      "context" : "Some of these approaches use referential games [11, 22, 29] to produce an emergent language that ideally has properties of human languages, with compositionality being a commonly sought after property [3, 4, 6].",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 21,
      "context" : "Some of these approaches use referential games [11, 22, 29] to produce an emergent language that ideally has properties of human languages, with compositionality being a commonly sought after property [3, 4, 6].",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 28,
      "context" : "Some of these approaches use referential games [11, 22, 29] to produce an emergent language that ideally has properties of human languages, with compositionality being a commonly sought after property [3, 4, 6].",
      "startOffset" : 47,
      "endOffset" : 59
    }, {
      "referenceID" : 2,
      "context" : "Some of these approaches use referential games [11, 22, 29] to produce an emergent language that ideally has properties of human languages, with compositionality being a commonly sought after property [3, 4, 6].",
      "startOffset" : 201,
      "endOffset" : 210
    }, {
      "referenceID" : 3,
      "context" : "Some of these approaches use referential games [11, 22, 29] to produce an emergent language that ideally has properties of human languages, with compositionality being a commonly sought after property [3, 4, 6].",
      "startOffset" : 201,
      "endOffset" : 210
    }, {
      "referenceID" : 5,
      "context" : "Some of these approaches use referential games [11, 22, 29] to produce an emergent language that ideally has properties of human languages, with compositionality being a commonly sought after property [3, 4, 6].",
      "startOffset" : 201,
      "endOffset" : 210
    }, {
      "referenceID" : 19,
      "context" : "Our paper is most similar to [20], which showed that compositional language arose only when certain constraints on the agents are satisfied.",
      "startOffset" : 29,
      "endOffset" : 33
    }, {
      "referenceID" : 27,
      "context" : "Liska[28] suggests that the average training run for recurrent neural networks does not converge to a compositional solution, but that a large random search will produce compositional solutions.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 33,
      "context" : "Spike[34] describes three properties that bias models towards successful learned signaling: the creation and transmission of referential information, a bias against ambiguity, and information loss.",
      "startOffset" : 5,
      "endOffset" : 9
    }, {
      "referenceID" : 18,
      "context" : "Each of [19, 38, 40] examine the trade-off between expression and compression in both emergent and natural languages, in addition to how that trade-off affects the learners.",
      "startOffset" : 8,
      "endOffset" : 20
    }, {
      "referenceID" : 37,
      "context" : "Each of [19, 38, 40] examine the trade-off between expression and compression in both emergent and natural languages, in addition to how that trade-off affects the learners.",
      "startOffset" : 8,
      "endOffset" : 20
    }, {
      "referenceID" : 39,
      "context" : "Each of [19, 38, 40] examine the trade-off between expression and compression in both emergent and natural languages, in addition to how that trade-off affects the learners.",
      "startOffset" : 8,
      "endOffset" : 20
    }, {
      "referenceID" : 7,
      "context" : "Chen[8] describes how the probability distribution on the set of all strings produced by a recurrent model can be interpreted as a weighted language; this is relevant to our formulation of the language.",
      "startOffset" : 4,
      "endOffset" : 7
    }, {
      "referenceID" : 0,
      "context" : "Most other works studying compositionality in emergent languages [1, 7, 24, 31] have focused on learning interpretable representations.",
      "startOffset" : 65,
      "endOffset" : 79
    }, {
      "referenceID" : 6,
      "context" : "Most other works studying compositionality in emergent languages [1, 7, 24, 31] have focused on learning interpretable representations.",
      "startOffset" : 65,
      "endOffset" : 79
    }, {
      "referenceID" : 23,
      "context" : "Most other works studying compositionality in emergent languages [1, 7, 24, 31] have focused on learning interpretable representations.",
      "startOffset" : 65,
      "endOffset" : 79
    }, {
      "referenceID" : 30,
      "context" : "Most other works studying compositionality in emergent languages [1, 7, 24, 31] have focused on learning interpretable representations.",
      "startOffset" : 65,
      "endOffset" : 79
    }, {
      "referenceID" : 14,
      "context" : "See [15] for a broad survey of the different approaches.",
      "startOffset" : 4,
      "endOffset" : 8
    }, {
      "referenceID" : 0,
      "context" : "• For each i ∈ [1, 6],Ci → w , wherew ∈ {10i − 10, .",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 5,
      "context" : "• For each i ∈ [1, 6],Ci → w , wherew ∈ {10i − 10, .",
      "startOffset" : 15,
      "endOffset" : 21
    }, {
      "referenceID" : 20,
      "context" : "This behavior was characterized in Lake [21].",
      "startOffset" : 40,
      "endOffset" : 44
    }, {
      "referenceID" : 1,
      "context" : "Only when the model capacity is just right will language learning correctly capture the compositional structure underlying the original language L⋆ and exhibit systematic generalization [2].",
      "startOffset" : 186,
      "endOffset" : 189
    }, {
      "referenceID" : 25,
      "context" : "This admits the interpretation of a two-player ReferIt game [26] in addition to being a density estimator of p(s).",
      "startOffset" : 60,
      "endOffset" : 64
    }, {
      "referenceID" : 17,
      "context" : "A variational autoencoder [18] consists of two neural networks which are often referred to as an encoder fθ , a decoder дφ , and a prior pλ .",
      "startOffset" : 26,
      "endOffset" : 30
    }, {
      "referenceID" : 13,
      "context" : "It then runs an LSTM [14] non-autoregressively for l timesteps taking the flattened representation of the input embeddings as its input and linearly projecting each result to a probability distribution over {0, 1}.",
      "startOffset" : 21,
      "endOffset" : 25
    }, {
      "referenceID" : 30,
      "context" : "To train the whole system end-to-end [31, 37] via backpropogation, we apply a continuous approximation to zt that depends on a learned temperature parameter τ .",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 36,
      "context" : "To train the whole system end-to-end [31, 37] via backpropogation, we apply a continuous approximation to zt that depends on a learned temperature parameter τ .",
      "startOffset" : 37,
      "endOffset" : 45
    }, {
      "referenceID" : 15,
      "context" : "We use the ‘straight-through‘ version of Gumbel-Softmax [16, 30] to convert the continuous distribution to a discrete distribution for each zt .",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 29,
      "context" : "We use the ‘straight-through‘ version of Gumbel-Softmax [16, 30] to convert the continuous distribution to a discrete distribution for each zt .",
      "startOffset" : 56,
      "endOffset" : 64
    }, {
      "referenceID" : 8,
      "context" : "It is however reasonable to assume that there is a monotonic relationship between the number of parameters |θ |, or |φ |, and the capacity of the network to encode the compositional structures underlying the original language [9].",
      "startOffset" : 226,
      "endOffset" : 229
    }, {
      "referenceID" : 16,
      "context" : "1) on Dtrain, using the Adam optimizer [17] with a learning rate of 3×10−3, weight decay coefficient of 10−4 and a batch size of 1000.",
      "startOffset" : 39,
      "endOffset" : 43
    }, {
      "referenceID" : 38,
      "context" : "Since systematic generalization may only happen in some training runs [39], each model is trained for each of 10 seeds over 200k steps.",
      "startOffset" : 70,
      "endOffset" : 74
    }, {
      "referenceID" : 9,
      "context" : "Here we find that smaller models can fail to solve the task for any bandwidth, which coincides with literature suggesting a link between overparameterization and learnability [10, 27].",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 26,
      "context" : "Here we find that smaller models can fail to solve the task for any bandwidth, which coincides with literature suggesting a link between overparameterization and learnability [10, 27].",
      "startOffset" : 175,
      "endOffset" : 183
    }, {
      "referenceID" : 4,
      "context" : "We have not been able to verify the upper bound in our experiments, which may require either a more (computationally) extensive set of experiments with even more parameters or a better theoretical understanding of the inherent biases behind learning with this architecture, such as from recent work on overparameterized models [5, 32].",
      "startOffset" : 327,
      "endOffset" : 334
    }, {
      "referenceID" : 31,
      "context" : "We have not been able to verify the upper bound in our experiments, which may require either a more (computationally) extensive set of experiments with even more parameters or a better theoretical understanding of the inherent biases behind learning with this architecture, such as from recent work on overparameterized models [5, 32].",
      "startOffset" : 327,
      "endOffset" : 334
    } ],
    "year" : 2020,
    "abstractText" : "Many recent works have discussed the propensity, or lack thereof, for emergent languages to exhibit properties of natural languages. A favorite in the literature is learning compositionality. We note that most of those works have focused on communicative bandwidth as being of primary importance. While important, it is not the only contributing factor. In this paper, we investigate the learning biases that affect the efficacy and compositionality in multi-agent communication. Our foremost contribution is to explore how the capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.1",
    "creator" : "LaTeX with hyperref package"
  }
}