{
  "name" : "The Emergence of Compositional Languages.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "The Emergence of Compositional Languages for Numeric Concepts Through Iterated Learning in Neural Agents",
    "authors" : [ "Shangmin Guo", "Yi Ren", "Serhii Havrylov", "Stella Frank", "Ivan Titov", "Kenny Smith" ],
    "emails" : [ "s1798190@ed.ac.uk" ],
    "sections" : [ {
      "heading" : "1 Introduction",
      "text" : "With recent advances in deep learning (DL), it has been shown that computational agents can master a variety of complex cognitive tasks [12, 16]. Recent work in grounded language learning [5, 4] applied DL techniques to enable agents to discover through learning communication protocols exhibiting language-like properties, e.g. hierarchy and compositionality. Using DL methods allow us to overcome the language pre-defining issue present in current computer simulation methods in evolutionary linguistics as in [18] and [2]. The issue consists in having all basic linguistic elements (such as symbols and rules of generating phrases) to be pre-specified instead of being invented from scratch. In contrast to previous works [13, 3] which focus on the emergence of referential signalling systems, we explore the emergent compositionality of the non-referential concept of numerals (which will be explained in Section 2.2) by designing a referential game in which agents need to transmit numerical concepts to communicate successfully.\nInspired by [9], we model the emergence of communication protocols in dyads (i.e. the smallest possible social group of two agents) that are nodes in iterated learning chain [8]. We observe that iterated learning can facilitate the emergence of compositional languages for numeric concepts. However, the emergence of languages with such properties depends on the representation of numerical concepts present in the objects observed by the agents during the training. To be specific, compositional languages emerge when numeric concepts are: i) represented as a concatenation of one-hot vectors directly representing numbers; ii) implied in images of scenes featuring different number of objects. Further, we show that input representations influence the difficulty of learning a particular language by the agents, which explains the different results in case of iterated learning. For numerical concepts, we, therefore, argue that one necessary condition for the emergence of compositional languages in\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n91 0.\n05 29\n1v 1\n[ cs\n.C L\n] 1\n1 O\nct 2\niterated learning is that these languages can be fully learnt 1 with less iterations for agents (especially listeners), compared with holistic languages and emergent languages from dyads."
    }, {
      "heading" : "2 Model Methods",
      "text" : ""
    }, {
      "heading" : "2.1 The Bag-Select Game",
      "text" : "To test whether computational agents can learn to transmit numerical concepts, we propose a referential game called as “Bag-Select” game which is briefly illustrated in Figure 1.\nNote that there are always 15 candidates for listeners to choose from in our game. More details about our game setting are given in Section A in Appendix."
    }, {
      "heading" : "2.2 Input Representations of Bags to be Communicated",
      "text" : "The overall architecture of our implementation is similar to communication models proposed by [4]. However, unlike theirs, in our game, an input bi can be\n1. Concatenation: a concatenation of one-hot vectors that represent numbers of each kind of objects, e.g. “2A3B” (a bag containing 2 As and 3 Bs) would be represented as [001000; 000100] and “2A0B” would be represented as [001000; 100000].\n2. Image: an image containing different numbers of objects, e.g. “0A0B”, “0A2B”, “2A0B”, “2A3B”, “5A5B” would be represented as Figure 2 (a-e) respectively.\n3. Bag: a bag of one-hot vectors that represent the quantity of different types of objects, e.g. “2A3B” and “2A0B” would be represented as {[01], [01], [10], [10], [10]} and {[01], [01]} respectively.\nAs there is no specific value that can be referred to as numbers of an object in our Image and Bag representations, numeric concepts are non-referential in our games.\nDifferent types of inputs require different encoders, thus we use: i) multilayer perceptron (MLP) for concatenations; ii) the convolutional neural network (CNN) which shares the same architecture of LeNet-5 proposed by [10] for images; iii) Bag-Encoder for bags.\nOur bag-encoder shares almost the same architecture as the set encoder proposed by [20], except that we replace the softmax function in equation (5) of [20] with the sigmoid function. Thus, we\n1A language is said to be fully learnt if: i) a speaker can always reproduce same messages as in the language given the inputs; ii) a listener could always obtain 100% accuracy given only the messages in it.\ncould keep the feature representation invariant under reordering of the vectors in bags, and avoid introducing normalizing bias (i.e. softmax output has to sum to one) which allows proper encoding of the numbers in the distributed representation of the bag.\nTo keep both meaning space and message space limited and thus analysable, there are only 2 different types of objects in our game and the maximum number of each kind of objects is 5. Therefore, the size of our Concatenation/Image dataset is 36, and the size of Bag dataset is 35 (excluding the empty bag). Messages are strings of characters of maximum length 2, where there is an available vocabulary of 10 characters."
    }, {
      "heading" : "2.3 Iterated Learning for Deep Learning Models",
      "text" : "We contrast two types of the population model. Following [4], we model dyads, pairs of agents who interact repeatedly and update their network parameters to maximise communicative success. Following [9], we contrast the communication systems that emerge in dyads with those that develop in iterated learning transmission chains. In the latter case, each generation in the chain consists of a pair of agents who are first trained on input-message pairs produced by the previous generation, then update their network parameters during communication with each other to maximise communicative success, before finally generating more data to pass to the next generation. More details about iterated learning for deep learning models are given in Section B in Appendix.\nBesides, the metrics and evaluation methods used in the following experiments are illustrated in Section C in Appendix."
    }, {
      "heading" : "3 Emergence of Compositional Languages",
      "text" : "In this section, we show that compositional languages can emerge under iterated learning, but only for the Concatenation and Image representations. As training iterated learning on deep learning models is extremely time-consuming, we report results for only one run per condition. During the exploratory phases of our research, we conducted multiple runs and found that the variance of resulting patterns of emergent languages is small, which gives us confidence that these results are representative.\nTo verify that iterated learning could successfully amplify the probability density of languages having high compositionality, we track the change of topological similarity of languages having greatest probability density over generations. The results for the Concatenation, Image and Bag input representations are shown in Figure 3. As can be seen from the graphs, dyads do not converge on compositional languages under any input representation. However, in iterated learning models, the topological similarity of emergent languages keep increasing on Concatenation and Image representation. We also track the how posterior probability of languages change over generations, and the results as well as corresponding final emergent languages are given in Figure 5 in Appendix."
    }, {
      "heading" : "4 Learnability of Compositional and Emergent Languages",
      "text" : "According to [9], the structure of natural languages is a trade-off between expressivity that arises during communication and compressibility that arises during learning. Meanwhile, [11] propose a hypothesis that compositional languages should be easier for listeners to learn than other less structured languages. Inspired by both of them, we hypothesise that the different effectiveness of iterated learning for different input representations observed in the above experiments is caused by different learnability of compositional languages for different input representations.\nTo test this hypothesis, we examine the learnability of three language types (compositional, emergent, holistic) for speakers and listeners. The establishment of these different types of languages are illustrated in Section E in Appendix. Meanwhile, training listeners separately is also illustrated there.\nThe learning curves of both listeners and speakers on different input representations are shown in Figure 4.\nIt is clear from Figure 4 that compositional languages require fewer training iterations than the other 2 kinds of languages in almost all the cases, with two exceptions: i) emergent languages has better learnability for listeners on the Bag representation; ii) compositional and emergent languages have almost the same learnability for speakers on the Image representation. Further explainations are provided in Section F in Appendix."
    }, {
      "heading" : "5 Conclusion",
      "text" : "We use the Bag-Select game to demonstrate that iterated learning leads to the emergence of compositional languages for transmitting numeric concepts. However, this result is dependent on the representations of inputs, and its effectiveness depends on that compositional languages have the optimal learnability for listeners in the communication game. While our findings confirm that structure of languages emerges under the pressure of both expressivity and learnability, at least for deep learning agents, the representation of the input representations affects on learnability and therefore on the structure of the emergent languages."
    }, {
      "heading" : "A: Game Description",
      "text" : "In our game settings, there are two different kinds of agents: i) speaker S that observes the input bi at the beginning of round i and then generates a message mi; ii) listener L that receives mi and then selects b̂i among candidates cki where k ∈ {1, 2, . . . , 15}. In our experiments, there are always 15 candidates, among which one would be bi and the other fourteen would be uniformly sampled from the whole meaning space excluding bi, for listeners to choose from. The game only succeeds if b̂i matches bi. The speaker does not have access to the entire candidate list, only to the correct bag bi, which implies that the number of each object type has to be encoded in the message in order to reliably succeed in the game."
    }, {
      "heading" : "B: Phases of Iterated Learning for Deep Learning",
      "text" : "To be more specific, each generation in our iterated learning model includes the following three steps:\n1. Learning phase: During this phase, we train speaker St separately to reproduce same messages given the inputs, with the input-message pairs generated by St−1. For example, an input-message pair is “1A0B” → “yw”, then we would train speakers to produce “yw” given the input “1A0B”. To do so, we use stochastic gradient descent (SGD) [14] to update parameters of St. Gradients are computed using the back-propagation [15] algorithm with the cross entropy loss function between speaker’s predictions and the messages generated by St−1. The number of training iterations is fixed such that predefined compositional language can be fully learnt (note that language produced by St−1 is not necessarily compositional). There is no such phase in the first generation of iterated learning chain, as there are no input-message pairs for training S1.\n2. Interaction phase: During this phase, we train St and Lt agents to play the communication game using SGD. The reward is represented by the negative cross entropy between the probability distribution of the listener’s prediction and the one-hot representation of the correct bag. Analogous to linguistic symbols, i.e. words, the messages transmitted between dyad should contain only discrete symbols. However, discrete messages would make learning prohibitively expensive from the computational perspective for computing the gradients would require enumeration of all possible messages. To overcome this limitation, we use the Gumbel-softmax estimator proposed by [7] to train our models. Besides, we set the number of iterations here to be fixed over generations, and number of iterations is obtained by pre-training a dyad to promise that it is long enough for a dyad to obtain 100% communication success rate.\n3. Transmission phase: During this phase, we feed all bi in the training set into St and sample messages mi based on the generated probability distribution over vocabulary. This builds a dataset of input-message pairs for St+1 to learn from. In addition, the number of sampled input-message pairs is 2, 000 so that they effectively reflect the distribution of all possible languages - note that since there are only 35-36 distinct input meanings to be communicated, there is no data bottleneck here, and learners will see signals for the entire space of possible meanings.\nAdditionally, interaction phase is the same as training dyad models like [4]."
    }, {
      "heading" : "C: Metrics and Evaluations",
      "text" : "Following [1], we take the topological similarity between meaning space and message space as the metric for measuring compositionality of languages, and we use Hamming distance and edit distance with respect to meaning space and message space. Equivalently, the topological similarity becomes the correlation coefficient between the Hamming distances between pairs of meanings and the edit distances between their corresponding messages. This measure captures the intuition that, in a compositional language, similar meanings will be conveyed using similar signals. We denote this measure of topological similarity as ρ; holistic (non-compositional) languages will have ρ scores around 0, a perfectly compositional language will have a ρ score of close to 1.\nAdditionally, we also need to measure the learning performance of new learners in order to compare the learnability of different languages, which is illustrated in Section 4. To do so, we use the accuracy of reproducing messages (both sequence-level and token-level) for speakers and accuracy of choosing the correct candidate for listeners respectively."
    }, {
      "heading" : "D: Experiments on the Emergence of Compositional Languages",
      "text" : "In our experiments illustrated in Section 3, we tracked: i) changes of topological similarity of emergent languages over generations; ii) changes of posterior probability of languages having different compositionality over generations. The results are shown in Figure 5 as follow, which also includes the final emergent languages (i.e. the languages having greatest probability density after training).\nAs can be seen from the above figure, in iterated learning models, the probability of languages with high compositionality (ρ > 0.6) keeps increasing over generations and gradually dominates all other languages, for the Concatenation and Image input representations; compositional languages do not develop in the Bag input representation. The compositional structure in the languages that emerge under the Concatenation input is clear from the example language given in Figure 5 (rightmost column), as is the absence of compositionality in the example language that develops under the Bag encoding; the final emergent language on Image representation is not perfectly compositional but contains a high degree of regularity."
    }, {
      "heading" : "E: Establish and Train Different Types of Languages",
      "text" : "Our compositional test language was hand-designed and resembled the compositional languages that emerge under iterated learning in the Concatenation condition. Our holistic language was generated by randomly mapping messages from compositional languages (so that it shares same expressivity as compositional language) to inputs that constitute the whole meaning space. Our emergent test languages came from a dyad, trained to communicate as per the dyad models, once that dyad obtained 100% performance – as such, we would expect them to be largely holistic.\nWith these languages, we train speakers separately, which is illustrated in Section B in Appendix. At the same time, we train listeners separately to correctly complete the game with only messages in a language. For example, an input-message pair in a language is “1A0B” → “yw”, then we would train listeners to select “1A0B” among the 15 candidates after taking “yw” as input. To do so, we still take the cross entropy between the correct candidate and listener’s predicted probability distribution as the loss and apply SGD to update the parameters of listeners."
    }, {
      "heading" : "F: Further Explaination about Learnability Experiments",
      "text" : "Based on the results shown in Figure 4, considering that the topological similarity of final emergent languages given the Bag representation is much lower than Concatenation/Image representations, we argue that iterated learning will amplify the probability of compositional languages only if less training iterations are necessary for listeners to learn the compositional languages. As we can theoretically prove that compositional languages always have lower sample complexity than any other non-degenerate languages and thus better learnability for speakers (based on statistical learning theory [19]), we actually only need to care about learnability for listeners here, instead of both speakers and listeners as before. Otherwise, iterated learning does not show lead to an increase in compositionality. Moreover, our results could also support the hypothesis that compositionality (which is an aspect of linguistic structure) emerges under the pressure of both expressivity and learnability [17], considering that emergent languages have better learnability on Bag representation than compositional languages; as such, those languages still represent a trade-off between learnability and expressivity, but under a slightly different learnability constraint. We are currently investigating why the Bag input encoding makes non-compositional languages more learnable."
    } ],
    "references" : [ {
      "title" : "Understanding linguistic evolution by visualizing the emergence of topographic mappings",
      "author" : [ "Henry Brighton", "Simon Kirby" ],
      "venue" : "Artificial life,",
      "citeRegEx" : "1",
      "shortCiteRegEx" : "1",
      "year" : 2006
    }, {
      "title" : "Simulating the evolution of language",
      "author" : [ "Angelo Cangelosi", "Domenico Parisi" ],
      "venue" : "Springer Science & Business Media,",
      "citeRegEx" : "2",
      "shortCiteRegEx" : "2",
      "year" : 2012
    }, {
      "title" : "Emergent communication through negotiation",
      "author" : [ "Kris Cao", "Angeliki Lazaridou", "Marc Lanctot", "Joel Z Leibo", "Karl Tuyls", "Stephen Clark" ],
      "venue" : "arXiv preprint arXiv:1804.03980,",
      "citeRegEx" : "3",
      "shortCiteRegEx" : "3",
      "year" : 2018
    }, {
      "title" : "Emergence of language with multi-agent games: Learning to communicate with sequences of symbols",
      "author" : [ "Serhii Havrylov", "Ivan Titov" ],
      "venue" : "In Advances in neural information processing systems,",
      "citeRegEx" : "4",
      "shortCiteRegEx" : "4",
      "year" : 2017
    }, {
      "title" : "Grounded language learning in a simulated 3d world",
      "author" : [ "Karl Moritz Hermann", "Felix Hill", "Simon Green", "Fumin Wang", "Ryan Faulkner", "Hubert Soyer", "David Szepesvari", "Wojciech Marian Czarnecki", "Max Jaderberg", "Denis Teplyashin" ],
      "venue" : "arXiv preprint arXiv:1706.06551,",
      "citeRegEx" : "5",
      "shortCiteRegEx" : "5",
      "year" : 2017
    }, {
      "title" : "Biological evolution of the saussurean sign as a component of the language acquisition",
      "author" : [ "James R Hurford" ],
      "venue" : "device. Lingua,",
      "citeRegEx" : "6",
      "shortCiteRegEx" : "6",
      "year" : 1989
    }, {
      "title" : "Categorical reparameterization with gumbel-softmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole" ],
      "venue" : "arXiv preprint arXiv:1611.01144,",
      "citeRegEx" : "7",
      "shortCiteRegEx" : "7",
      "year" : 2016
    }, {
      "title" : "Function, selection, and innateness: The emergence of language universals",
      "author" : [ "Simon Kirby" ],
      "venue" : "OUP Oxford,",
      "citeRegEx" : "8",
      "shortCiteRegEx" : "8",
      "year" : 1999
    }, {
      "title" : "Compression and communication in the cultural evolution of linguistic structure",
      "author" : [ "Simon Kirby", "Monica Tamariz", "Hannah Cornish", "Kenny Smith" ],
      "venue" : "Cognition, 141:87–102,",
      "citeRegEx" : "9",
      "shortCiteRegEx" : "9",
      "year" : 2015
    }, {
      "title" : "Gradient-based learning applied to document recognition",
      "author" : [ "Yann LeCun", "Léon Bottou", "Yoshua Bengio", "Patrick Haffner" ],
      "venue" : "Proceedings of the IEEE,",
      "citeRegEx" : "10",
      "shortCiteRegEx" : "10",
      "year" : 1998
    }, {
      "title" : "Ease-of-teaching and language structure from emergent communication",
      "author" : [ "Fushan Li", "Michael Bowling" ],
      "venue" : "arXiv preprint arXiv:1906.02403,",
      "citeRegEx" : "11",
      "shortCiteRegEx" : "11",
      "year" : 2019
    }, {
      "title" : "Human-level control through deep reinforcement learning",
      "author" : [ "Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski" ],
      "venue" : null,
      "citeRegEx" : "12",
      "shortCiteRegEx" : "12",
      "year" : 2015
    }, {
      "title" : "Emergence of grounded compositional language in multiagent populations",
      "author" : [ "Igor Mordatch", "Pieter Abbeel" ],
      "venue" : "In Thirty-Second AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "13",
      "shortCiteRegEx" : "13",
      "year" : 2018
    }, {
      "title" : "A stochastic approximation method",
      "author" : [ "Herbert Robbins", "Sutton Monro" ],
      "venue" : "The annals of mathematical statistics,",
      "citeRegEx" : "14",
      "shortCiteRegEx" : "14",
      "year" : 1951
    }, {
      "title" : "Learning representations by back-propagating errors",
      "author" : [ "David E Rumelhart", "Geoffrey E Hinton", "Ronald J Williams" ],
      "venue" : "Cognitive modeling,",
      "citeRegEx" : "15",
      "shortCiteRegEx" : "15",
      "year" : 1988
    }, {
      "title" : "Mastering the game of go without human knowledge",
      "author" : [ "David Silver", "Julian Schrittwieser", "Karen Simonyan", "Ioannis Antonoglou", "Aja Huang", "Arthur Guez", "Thomas Hubert", "Lucas Baker", "Matthew Lai", "Adrian Bolton" ],
      "venue" : null,
      "citeRegEx" : "16",
      "shortCiteRegEx" : "16",
      "year" : 2017
    }, {
      "title" : "Linguistic structure is an evolutionary tradeoff between simplicity and expressivity",
      "author" : [ "Kenny Smith", "Monica Tamariz", "Simon Kirby" ],
      "venue" : "In Proceedings of the Annual Meeting of the Cognitive Science Society,",
      "citeRegEx" : "17",
      "shortCiteRegEx" : "17",
      "year" : 2013
    }, {
      "title" : "The emergence and evolution of linguistic structure: from lexical to grammatical communication systems",
      "author" : [ "Luc Steels" ],
      "venue" : "Connection science,",
      "citeRegEx" : "18",
      "shortCiteRegEx" : "18",
      "year" : 2005
    } ],
    "referenceMentions" : [ {
      "referenceID" : 5,
      "context" : "Since first introduced by [6], computer simulation has been an increasingly important tool in evolutionary linguistics.",
      "startOffset" : 26,
      "endOffset" : 29
    }, {
      "referenceID" : 11,
      "context" : "With recent advances in deep learning (DL), it has been shown that computational agents can master a variety of complex cognitive tasks [12, 16].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 15,
      "context" : "With recent advances in deep learning (DL), it has been shown that computational agents can master a variety of complex cognitive tasks [12, 16].",
      "startOffset" : 136,
      "endOffset" : 144
    }, {
      "referenceID" : 4,
      "context" : "Recent work in grounded language learning [5, 4] applied DL techniques to enable agents to discover through learning communication protocols exhibiting language-like properties, e.",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 3,
      "context" : "Recent work in grounded language learning [5, 4] applied DL techniques to enable agents to discover through learning communication protocols exhibiting language-like properties, e.",
      "startOffset" : 42,
      "endOffset" : 48
    }, {
      "referenceID" : 17,
      "context" : "Using DL methods allow us to overcome the language pre-defining issue present in current computer simulation methods in evolutionary linguistics as in [18] and [2].",
      "startOffset" : 151,
      "endOffset" : 155
    }, {
      "referenceID" : 1,
      "context" : "Using DL methods allow us to overcome the language pre-defining issue present in current computer simulation methods in evolutionary linguistics as in [18] and [2].",
      "startOffset" : 160,
      "endOffset" : 163
    }, {
      "referenceID" : 12,
      "context" : "In contrast to previous works [13, 3] which focus on the emergence of referential signalling systems, we explore the emergent compositionality of the non-referential concept of numerals (which will be explained in Section 2.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 2,
      "context" : "In contrast to previous works [13, 3] which focus on the emergence of referential signalling systems, we explore the emergent compositionality of the non-referential concept of numerals (which will be explained in Section 2.",
      "startOffset" : 30,
      "endOffset" : 37
    }, {
      "referenceID" : 8,
      "context" : "Inspired by [9], we model the emergence of communication protocols in dyads (i.",
      "startOffset" : 12,
      "endOffset" : 15
    }, {
      "referenceID" : 7,
      "context" : "the smallest possible social group of two agents) that are nodes in iterated learning chain [8].",
      "startOffset" : 92,
      "endOffset" : 95
    }, {
      "referenceID" : 3,
      "context" : "2 Input Representations of Bags to be Communicated The overall architecture of our implementation is similar to communication models proposed by [4].",
      "startOffset" : 145,
      "endOffset" : 148
    }, {
      "referenceID" : 9,
      "context" : "“2A3B” and “2A0B” would be represented as {[01], [01], [10], [10], [10]} and {[01], [01]} respectively.",
      "startOffset" : 55,
      "endOffset" : 59
    }, {
      "referenceID" : 9,
      "context" : "“2A3B” and “2A0B” would be represented as {[01], [01], [10], [10], [10]} and {[01], [01]} respectively.",
      "startOffset" : 61,
      "endOffset" : 65
    }, {
      "referenceID" : 9,
      "context" : "“2A3B” and “2A0B” would be represented as {[01], [01], [10], [10], [10]} and {[01], [01]} respectively.",
      "startOffset" : 67,
      "endOffset" : 71
    }, {
      "referenceID" : 9,
      "context" : "Different types of inputs require different encoders, thus we use: i) multilayer perceptron (MLP) for concatenations; ii) the convolutional neural network (CNN) which shares the same architecture of LeNet-5 proposed by [10] for images; iii) Bag-Encoder for bags.",
      "startOffset" : 219,
      "endOffset" : 223
    }, {
      "referenceID" : 3,
      "context" : "Following [4], we model dyads, pairs of agents who interact repeatedly and update their network parameters to maximise communicative success.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "Following [9], we contrast the communication systems that emerge in dyads with those that develop in iterated learning transmission chains.",
      "startOffset" : 10,
      "endOffset" : 13
    }, {
      "referenceID" : 8,
      "context" : "According to [9], the structure of natural languages is a trade-off between expressivity that arises during communication and compressibility that arises during learning.",
      "startOffset" : 13,
      "endOffset" : 16
    }, {
      "referenceID" : 10,
      "context" : "Meanwhile, [11] propose a hypothesis that compositional languages should be easier for listeners to learn than other less structured languages.",
      "startOffset" : 11,
      "endOffset" : 15
    } ],
    "year" : 2019,
    "abstractText" : "Since first introduced by [6], computer simulation has been an increasingly important tool in evolutionary linguistics. Recently, with the development of deep learning techniques, research in grounded language learning has also started to focus on facilitating the emergence of compositional languages without pre-defined elementary linguistic knowledge. In this work, we explore the emergence of compositional languages for numeric concepts in multi-agent communication systems. We demonstrate that compositional language for encoding numeric concepts can emerge through iterated learning in populations of deep neural network agents. However, language properties greatly depend on the input representations given to agents. We found that compositional languages only emerge if they require less iterations to be fully learnt than other non-degenerate languages for agents on a given input representation.",
    "creator" : "LaTeX with hyperref package"
  }
}