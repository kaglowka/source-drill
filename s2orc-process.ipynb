{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T10:11:44.116289Z",
     "start_time": "2020-06-24T10:11:44.076962Z"
    }
   },
   "outputs": [],
   "source": [
    "# setup variables\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "from s2orc.config import CURRENT_VERSION\n",
    "\n",
    "LOCAL_S2ORC_DIR = 's2orc-data'\n",
    "local_manifest_file = os.path.join(LOCAL_S2ORC_DIR, CURRENT_VERSION, 'manifest.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the corpus chunks and join with citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T10:11:44.133341Z",
     "start_time": "2020-06-24T10:11:44.118776Z"
    }
   },
   "outputs": [],
   "source": [
    "# ---- CONFIG ----- #\n",
    "\n",
    "# jsonlines https://jsonlines.readthedocs.io/en/latest/#api\n",
    "import jsonlines\n",
    "import gzip\n",
    "\n",
    "paper_dir = os.path.join(LOCAL_S2ORC_DIR, CURRENT_VERSION, 'papers')\n",
    "context_dir = os.path.join(LOCAL_S2ORC_DIR, CURRENT_VERSION, 'contexts')\n",
    "context_file_suffix = 'contexts.jsonl' # filename besides batch number\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T10:12:30.909855Z",
     "start_time": "2020-06-24T10:12:21.529380Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [00:09<07:37,  4.67s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-903a49762788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mcontexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgzip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf_in\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mpapers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjsonlines\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mpaper\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpapers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mcitation_contexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_citation_contexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaper\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoks_in_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/materiały/kogni/Python/env/lib/python3.8/site-packages/jsonlines/jsonlines.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, type, allow_none, skip_empty, skip_invalid)\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m                     yield self.read(\n\u001b[0m\u001b[1;32m    202\u001b[0m                         \u001b[0mtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                         \u001b[0mallow_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_none\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/materiały/kogni/Python/env/lib/python3.8/site-packages/jsonlines/jsonlines.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, type, allow_none, skip_empty)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mskip_empty\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mlineno\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_line_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mreadline\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_not_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/_compression.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreadinto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mview\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"B\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbyte_view\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_view\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mbyte_view\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    499\u001b[0m                                \"end-of-stream marker was reached\")\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 501\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0muncompress\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    502\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muncompress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0muncompress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/gzip.py\u001b[0m in \u001b[0;36m_add_read_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_add_read_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrc32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_crc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stream_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##### extract citation contexts and save them\n",
    "from s2orc.get_citation_contexts import get_citation_contexts\n",
    "\n",
    "start = 0\n",
    "span = 100\n",
    "\n",
    "os.makedirs(context_dir, exist_ok=True)\n",
    "batch_files = sorted(os.listdir(paper_dir), key=lambda f: int(f.split('.')[0]))[start:(start+span)]\n",
    "for batch_file in tqdm.tqdm(batch_files):\n",
    "    batch_number = batch_file.split('.')[0]\n",
    "    contexts = []\n",
    "    with gzip.open(os.path.join(paper_dir, batch_file), 'rb') as f_in:\n",
    "        papers = list(jsonlines.Reader(f_in))\n",
    "        for paper in papers:\n",
    "            citation_contexts = get_citation_contexts(paper, toks_in_context=20)\n",
    "            # remove redundant ids\n",
    "            for item in citation_contexts:\n",
    "                del item['paper_id']\n",
    "            entry = { 'paper_id': paper['paper_id'] }\n",
    "            if len(citation_contexts) > 0:\n",
    "                entry['contexts'] = citation_contexts\n",
    "            contexts.append(entry)\n",
    "    \n",
    "    out_filename = '.'.join([batch_number, context_file_suffix, 'gz'])\n",
    "    with gzip.open(os.path.join(context_dir, out_filename), mode='w') as f_out:\n",
    "        jsonlines.Writer(f_out).write_all(contexts)\n",
    "#         with jsonlines.open(os.path.join(context_dir, out_filename), mode='w') as writer:\n",
    "#             writer.write_all(contexts)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T10:12:52.273883Z",
     "start_time": "2020-06-24T10:12:37.098260Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:15<00:00,  6.61it/s]\n"
     ]
    }
   ],
   "source": [
    "# read citation contexts\n",
    "start = 0\n",
    "span = 100\n",
    "\n",
    "contexts = {} # citation contexts by paper id\n",
    "\n",
    "batch_files = sorted(os.listdir(context_dir), key=lambda f: int(f.split('.')[0]))[start:(start+span)]\n",
    "for batch_file in tqdm.tqdm(batch_files):\n",
    "    batch_number = batch_file.split('.')[0]\n",
    "    with gzip.open(os.path.join(context_dir, batch_file)) as f_in:\n",
    "        reader = jsonlines.Reader(f_in)\n",
    "        contexts.update({item['paper_id']: item for item in reader})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find articles with full grobid parse and intersect them with the papers cited within corpus contexts to get pairs <citation_string, cited full text paper>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T10:14:08.633660Z",
     "start_time": "2020-06-24T10:14:08.631276Z"
    }
   },
   "outputs": [],
   "source": [
    "full_text_pap in tqdm.tqdm(links_random_order):\n",
    "    # find the adequate context pair\n",
    "    glove_similarities = []\n",
    "    flair_similarities = []\n",
    "#     next(context[''] for context in contexts[paper['paper_id']]['contexts'] if context['cited_paper_id'] == paper\n",
    "    citing_string = ''.join([context['pre_context'], context['context_string'], context['post_context']])\n",
    "    s = Sentence(citing_string, use_tokenizer=True)\n",
    "    glove_embedding.embed(s)\n",
    "    glove_citation_embedding = s.embedding.detach()\n",
    "\n",
    "#     s = Sentence(citing_string, use_tokenizer=True)\n",
    "#     flair_embedding.embed(s)\n",
    "#     flair_citation_embedding = s.embedding.detach()\n",
    "    for paper_part, text_chunks in paper['grobid_parse'].items():\n",
    "        if text_chunks is not None:\n",
    "            for text_chunk in text_chunks:\n",
    "                if isinstance(text_chunk, dict):\n",
    "                    text = text_chunk.get('text')\n",
    "                    tokenized_sents = list(split(Tokenizer().tokenize(text)))\n",
    "                    sents = [' '.join(str(token) for token in sent) for sent in tokenized_sents]\n",
    "\n",
    "                    sentences = [s for s in [Sentence(sent, use_tokenizer=True) for sent in sents] if len(s.tokens) > 0]\n",
    "                    embeddings = glove_embedding.embed(sentences)\n",
    "                    for s in sentences:\n",
    "                        e1 = s.embedding.detach()\n",
    "                        e2 = glove_citation_embedding\n",
    "                        sim = np.dot(e1, e2) / (np.sqrt(np.dot(e1, e1)) * np.sqrt(np.dot(e2, e2)))\n",
    "                        glove_similarities.append((sim, s.to_original_text()))\n",
    "\n",
    "\n",
    "#                     sentences = [s for s in [Sentence(sent, use_tokenizer=True) for sent in sents] if len(s.tokens) > 0]\n",
    "#                     embeddings = flair_embedding.embed(sentences)\n",
    "#                     for s in sentences:\n",
    "#                         e1 = s.embedding.detach()\n",
    "#                         e2 = flair_citation_embedding\n",
    "#                         sim = np.dot(e1, e2) / (np.sqrt(np.dot(e1, e1)) * np.sqrt(np.dot(e2, e2)))\n",
    "#                         flair_similarities.append((sim, s.to_original_text()))\n",
    "\n",
    "\n",
    "            print('\\n\\n\\n--- PAPER ---', '\\n', paper['metadata']['title'], file=f_out, end='\\n')\n",
    "            print('\\n\\ncontext: ', citing_string, file=f_out, end='\\n\\n')\n",
    "            print('\\n\\nsimilarities: ', file=f_out, end='\\n\\n')\n",
    "            print(*sorted(glove_similarities, key=lambda x: x[0], reverse=True)[:20], file=f_out, sep='\\n\\n')\n",
    "            just_sim = [sim for sim, text in glove_similarities]\n",
    "            plt.hist(just_sim, bins=np.linspace(0, 1, 20))\n",
    "#             print('FLAIR')\n",
    "#             print(*sorted(flair_similarers = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T10:14:08.633Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 72/100 [11:45<02:59,  6.41s/it]   "
     ]
    }
   ],
   "source": [
    "##### get papers with full text parse\n",
    "\n",
    "start = 0\n",
    "span = 100\n",
    "\n",
    "batch_files = sorted(os.listdir(paper_dir), key=lambda f: int(f.split('.')[0]))[start:(start+span)]\n",
    "for batch_file in tqdm.tqdm(batch_files):\n",
    "    with gzip.open(os.path.join(paper_dir, batch_file), 'rb') as f_in:\n",
    "        papers = list(jsonlines.Reader(f_in))\n",
    "        for paper in papers:\n",
    "            if paper.get('grobid_parse') is not None:\n",
    "                full_text_papers[paper['paper_id']] = paper\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T10:14:08.635Z"
    }
   },
   "outputs": [],
   "source": [
    "len(full_text_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T10:14:08.636Z"
    }
   },
   "outputs": [],
   "source": [
    "# intersect full text papers with paper citations\n",
    "links = []\n",
    "bar = tqdm.tqdm(list(contexts.values()))\n",
    "for paper in bar:\n",
    "    bar.set_description('links found: ' + str(len(links)))\n",
    "#     print(type(paper['paper_id']))\n",
    "#     print(paper['contexts'])\n",
    "    for context in paper.get('contexts', []):\n",
    "#         print(type(context[0]['cited_paper_id']))\n",
    "        cited_id = context['cited_paper_id']\n",
    "        if cited_id in full_text_papers:\n",
    "            links.append((context, full_text_papers[cited_id]))\n",
    "            \n",
    "# len(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T10:14:08.638Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(full_text_papers.values())\n",
    "for context, paper in links[50:100]:\n",
    "    print(context['context_string'], paper['metadata']['title'], sep='\\n', end='\\n\\n')\n",
    "# print(links[0][0])\n",
    "# print(links[0][1])\n",
    "# 17913703"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### observations\n",
    "\n",
    "- embeddings will be applicable more to more \"fuzzy\" ideas and papers?\n",
    "- nominal phrases VS verbal descriptions\n",
    "- broader context of the citing paper would be useful (at least for identifying keywords from abstract)\n",
    "- unknown words must be taken into account (many neologisms, variable names and other very hermetic words in pure science papers).\n",
    "\n",
    "### todo\n",
    "- checkout sciBERT\n",
    "- checkout what kind of embeddings would be useful\n",
    "- what would be useful: extracting precise information, or just fuzzy semantic detection (e.g. topic).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-06-24T10:14:08.642Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from syntok.segmenter import split\n",
    "from syntok.tokenizer import Tokenizer\n",
    "import json\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, StackedEmbeddings, Sentence\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "links_random_order = list(links)\n",
    "np.random.seed(234243)\n",
    "np.random.shuffle(links_random_order)\n",
    "\n",
    "glove_embedding = DocumentPoolEmbeddings([WordEmbeddings('glove')])\n",
    "\n",
    "# flair_embedding = StackedEmbeddings([\n",
    "#                                         FlairEmbeddings('news-forward'),\n",
    "#                                         FlairEmbeddings('news-backward'),\n",
    "#                                        ])\n",
    "\n",
    "\n",
    "# with open('glove_examples.txt', 'w') as f_out:\n",
    "for context, paper in tqdm.tqdm(links_random_order):\n",
    "    # find the adequate context pair\n",
    "    glove_similarities = []\n",
    "    flair_similarities = []\n",
    "#     next(context[''] for context in contexts[paper['paper_id']]['contexts'] if context['cited_paper_id'] == paper\n",
    "    citing_string = ''.join([context['pre_context'], context['context_string'], context['post_context']])\n",
    "    s = Sentence(citing_string, use_tokenizer=True)\n",
    "    glove_embedding.embed(s)\n",
    "    glove_citation_embedding = s.embedding.detach()\n",
    "\n",
    "#     s = Sentence(citing_string, use_tokenizer=True)\n",
    "#     flair_embedding.embed(s)\n",
    "#     flair_citation_embedding = s.embedding.detach()\n",
    "    for paper_part, text_chunks in paper['grobid_parse'].items():\n",
    "        if text_chunks is not None:\n",
    "            for text_chunk in text_chunks:\n",
    "                if isinstance(text_chunk, dict):\n",
    "                    text = text_chunk.get('text')\n",
    "                    tokenized_sents = list(split(Tokenizer().tokenize(text)))\n",
    "                    sents = [' '.join(str(token) for token in sent) for sent in tokenized_sents]\n",
    "\n",
    "                    sentences = [s for s in [Sentence(sent, use_tokenizer=True) for sent in sents] if len(s.tokens) > 0]\n",
    "                    embeddings = glove_embedding.embed(sentences)\n",
    "                    for s in sentences:\n",
    "                        e1 = s.embedding.detach()\n",
    "                        e2 = glove_citation_embedding\n",
    "                        sim = np.dot(e1, e2) / (np.sqrt(np.dot(e1, e1)) * np.sqrt(np.dot(e2, e2)))\n",
    "                        glove_similarities.append((sim, s.to_original_text()))\n",
    "\n",
    "\n",
    "#                     sentences = [s for s in [Sentence(sent, use_tokenizer=True) for sent in sents] if len(s.tokens) > 0]\n",
    "#                     embeddings = flair_embedding.embed(sentences)\n",
    "#                     for s in sentences:\n",
    "#                         e1 = s.embedding.detach()\n",
    "#                         e2 = flair_citation_embedding\n",
    "#                         sim = np.dot(e1, e2) / (np.sqrt(np.dot(e1, e1)) * np.sqrt(np.dot(e2, e2)))\n",
    "#                         flair_similarities.append((sim, s.to_original_text()))\n",
    "\n",
    "\n",
    "            print('\\n\\n\\n--- PAPER ---', '\\n', paper['metadata']['title'], file=f_out, end='\\n')\n",
    "            print('\\n\\ncontext: ', citing_string, file=f_out, end='\\n\\n')\n",
    "            print('\\n\\nsimilarities: ', file=f_out, end='\\n\\n')\n",
    "            print(*sorted(glove_similarities, key=lambda x: x[0], reverse=True)[:20], file=f_out, sep='\\n\\n')\n",
    "            just_sim = [sim for sim, text in glove_similarities]\n",
    "            plt.hist(just_sim, bins=np.linspace(0, 1, 20))\n",
    "#             print('FLAIR')\n",
    "#             print(*sorted(flair_similarities, key=lambda x: x[0], reverse=True)[:10], sep='\\n\\n')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### conclusions\n",
    "- sometimes the context says only what an article \"does\" (not what is its content), e.g.: \"A more detailed description of the study population that participated in the autonomic measurements has been reported elsewhere (Dietrich et al., 2006)\"\n",
    "- the major problem is the citation context, and it seems glove embeddings do find some relevant sentences when the citing sentence describes accurately\n",
    "- distinction between \"nominal\" and \"verbal\" contexts would help a lot, since verbal contexts often need a large context, and the nominal sometimes need probably only 3-4 words to compare with the text.\n",
    "- sometimes, when the whole article seems to correspond to the citing sentence, in fact the task of finding relevant ones seem to be very ambiguous. We should probably see possible clusters of score distribution\n",
    "\n",
    "### TODO:\n",
    "- get flair embeddings to work (define their own metric of similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = {\n",
    "    'bag_of_words': calc_bow,\n",
    "    'glove+bow': itd.\n",
    "}\n",
    "\n",
    "def compare(paper_links, methods):\n",
    "    for name, get_score in methods.items():\n",
    "        for paper_link in paper_links:\n",
    "            get_score(paper_link)\n",
    "            # show\n",
    "            # - histogram of scores in the whole paper?\n",
    "            # - plot location -> score?\n",
    "    # some distribution over all papers?\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-24T22:34:54.689668Z",
     "start_time": "2020-06-24T22:34:54.684012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sdf'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = set()\n",
    "x.update(['sdf'])\n",
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogsci_python",
   "language": "python",
   "name": "cogsci_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
