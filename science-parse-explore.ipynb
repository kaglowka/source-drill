{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:04:55.583027Z",
     "start_time": "2020-05-01T14:04:55.576622Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "with open('science-parse-test/parses/2019.json') as f:\n",
    "    doc = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:28:24.510743Z",
     "start_time": "2020-05-01T14:28:24.503042Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "94"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_data = doc['metadata']\n",
    "len(doc_data['references'])\n",
    "len(doc_data['referenceMentions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:28:25.687346Z",
     "start_time": "2020-05-01T14:28:25.642401Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'title', 'authors', 'emails', 'sections', 'references', 'referenceMentions', 'year', 'abstractText', 'creator'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'source': 'CRF',\n",
       " 'title': 'Extending Machine Language Models toward Human-Level Language Understanding',\n",
       " 'authors': ['James L. McClellanda',\n",
       "  'Felix Hillb',\n",
       "  'Maja Rudolphc',\n",
       "  'Jason Baldridged',\n",
       "  'Hinrich Schützee'],\n",
       " 'emails': [],\n",
       " 'sections': [{'heading': 'Extending Machine Language Models toward Human-Level Language Understanding',\n",
       "   'text': 'James L. McClellanda,b,2, Felix Hillb,2, Maja Rudolphc,2, Jason Baldridged,1,2, and Hinrich Schützee,1,2\\naStanford University, Stanford, CA 94305, USA; bDeepMind, London N1C 4AG, UK; cBosch Center for Artificial Intelligence, Renningen, 71272, Germany; dGoogle Research, Austin, TX 78701, USA; eLMU Munich, Munich, 80538, Germany\\nThis manuscript was compiled on December 13, 2019\\nLanguage is central to human intelligence. We review recent breakthroughs in machine language processing and consider what remains to be achieved. Recent approaches rely on domain general principles of learning and representation captured in artificial neural networks. Most current models, however, focus too closely on language itself. In humans, language is part of a larger system for acquiring, representing, and communicating about objects and situations in the physical and social world, and future machine language models should emulate such a system. We describe existing machine models linking language to concrete situations, and point toward extensions to address more abstract cases. Human language processing exploits complementary learning systems, including a deep neural network-like learning system that learns gradually as machine systems do, as well as a fast-learning system that supports learning new information quickly. Adding such a system to machine language models will be an important further step toward truly human-like language understanding.'},\n",
       "  {'heading': 'Language Understanding | Natural Language Processing | Situation',\n",
       "   'text': ''},\n",
       "  {'heading': 'Models | Machine Language Models | Brain System for Understanding',\n",
       "   'text': 'Many of the most impressive recent successes of machine intelligence have appeared in the domain of language. Machines can now better identify the words we speak and respond in ever more natural sounding voices. More impressive still is modern machine translation (1). Anyone with a smartphone has access to applications that allow them to say a sentence in one language and then see and hear its translation in another. Human ability still far exceeds machines in most language tasks, but these systems work well enough to be used by billions of people everyday.\\nWhat underlies these successes, and what limitations do these systems face? We argue that successes to date come from ever more effective methods for exploiting principles of neural computation that human language users also exploit. We then note that the work remains limited in that it largely treats language separately from the larger task of understanding the world around us. This leads us to propose an integrated approach to building a system that truly understands, in which language plays a key role in concert with other sources of input. We discuss the challenges facing further development of the approach and propose future steps toward addressing these challenges.'},\n",
       "  {'heading': 'Principles of Neural Computation',\n",
       "   'text': 'The principles of neural computation are domain general principles inspired by the human brain. They were first articulated in the 1950s (2) and further developed in the 1980s in the Parallel Distributed Processing (PDP) framework for modeling cognition (3). A central idea of this approach is that structure in language and other cognitive domains is an emergent\\nphenomenon, captured in learned connection weights and resulting in context-sensitive, distributed representations whose characteristics reflect a gradual, input-statistics dependent, learning process (4). The models treat the symbols and rules of classical linguistic theory as consequences of processing and learning, not entities whose structure must be built in. Instead of discrete symbols for linguistic units, these models rely on patterns of activity often called embeddings over arrays of neuron-like processing units. Instead of explicit systems of rules, they rely on learned matrices of connection weights to map patterns on one set of units into patterns on others.\\nAnother key principle is mutual constraint satisfaction (5). For example, the meaning of a sentence depends on its structure (its organization into constituent phrases); but so too can the structure depend on the meaning. Consider the sentence A boy hit a man with a __. If the missing word is bat, with a bat is read as part of a verb phrase headed by hit, and specifies the instrument used to carry out the action. But if beard fills the blank, with a beard is a part of a noun phrase describing who was hit. Even the segmentation of spoken or written language into elementary segments (e.g., letters) depends in part on meaning and context, as illustrated in Fig. 1. Rumelhart (5) sketched an interactive model of language understanding in which estimates of probabilities about all aspects of an input constrain estimates of the probability of every other aspect. The idea was captured in a model of context effects in perception (6). Later work (7, 8) linked these ideas to energy minimization in statistical physics. Neural language modeling research, which we now describe, incorporates these principles.'},\n",
       "  {'heading': 'Neural Language Modeling',\n",
       "   'text': 'An Early Neural Language Model. Elman (9) built on the principles above to demonstrate how neural models can capture key characteristics of language structure through learning, a feat once considered impossible (10). The model provides a starting point for understanding recent developments. Elman used the recurrent neural network shown in Fig. 2a. The network was trained to predict the next word in a sequence (w(t+1)) based on the current word (w(t)) and its own hidden (that is, learned internal) representation from the previous time step (h(t − 1)). These two inputs are each multiplied by a matrix of connection weights (represented by the arrows labeled Whi and Whh) to produce vectors that are added to produce a vector of inputs to the hidden layer of units. The\\nJM, FH, MR, JB, and HS wrote the paper.\\nThe authors declare no conflict of interest.\\n1J.B. and H.S. contributed equally.\\n2 To whom correspondence should be addressed. E-mail: jlmccstanford.edu, felixhill@google.com, marirudolph@gmail.com, jasonbaldridge@google.com or inquiries@cislmu.org\\n1\\nar X\\niv :1\\n91 2.\\n05 87\\n7v 1\\n[ cs\\n.C L\\n] 1\\n2 D\\nec 2\\n01 9\\nelements of this vector undergo a transformation limiting the range of activation values, resulting in the hidden layer representation. This in turn is multiplied with the matrix of weights to the output layer from the hidden layer (Woh) to generate a vector used to predict the probability of each of the possible successor words. Learning in this and other neural models is based on the discrepancy between the network’s output and the actual next word; the values of the connection weights are adjusted by a small amount to reduce the discrepancy. The network is called recurrent because the same connection weights (denoted by arrows in the figure) are used to process each successive word; the hidden representation h(t) becomes the context representation h(t − 1) for the next time step.\\nElman demonstrated two crucial findings. First, after training his network to predict the next word in simple sentences like man eats bread, dog chases cat, and girl sleeps, the network’s representations captured the syntactic distinction between nouns and verbs (9). It also captured interpretable subcategories, as shown by a hierarchical clustering of the hidden-layer pattern (h(t)) for each word in the training materials (Fig. 2b). This illustrates a key feature of learned representations in neural models: they capture specific as well as general or abstract information. By using a different learned representation for each word, the specific predictive consequences of that word can be exploited. Because representations for words that make similar predictions are similar, and because neural networks exploit similarity, the network can share knowledge about predictions among related words. Second, Elman (11) used both simple sentences like boy chases dogs and more complex ones like boy who sees girls chases dogs. In the more complex case, the verb chases must agree with the first noun (boy), not the closest noun (girls), since the sentence contains a main clause (boy chases dogs) interrupted by an embedded clause (boy [who] sees girls). The model learned to predict the verb form correctly despite the intervening clause, showing that it acquired sensitivity to the syntactic structure of language, and not just local co-occurrence statistics in its learned connections and distributed representations.'},\n",
       "  {'heading': 'Scaling Up to Process Natural Text. Elman’s task—predicting',\n",
       "   'text': 'the next word in a sequence—has been central to neural language modeling. However, Elman trained his networks with only tiny fragments of what were effectively toy languages. For many years, it seemed they would not scale up. Beginning about 10 years ago, advances in machine language processing began to overcome this limitation for neural models. We describe two crucial developments next.\\nLong-distance dependencies and pretrained word embeddings. A challenge for language prediction is the indefinite length of\\nthe context that might be relevant. Consider this passage:\\nJohn put some beer in a cooler and went out with his friends to play volleyball. Soon after he left, someone took the beer out of the cooler. John and his friends were thirsty after the game, and went back to his place for some beers. When John opened the cooler, he discovered that the beer was ___.\\nHere a reader expects the missing word to be gone. Yet if we replaced someone took the beer with someone took the ice the expected word would be warm instead. Furthermore, any amount of additional text between beer and gone would not change the predictive relationship. Elman’s network could take only a few words of context into account, reflecting a larger challenge known as the vanishing gradient problem (12). In essence, the magnitude of the learning signal that determines the adjustments to connection weights tends to decrease exponentially as the number of layers of weights between an input and an output increases. The development of neural network modules called Long-Short-Term-Memory (LSTM) modules (13) that partially overcame this limitation was therefore crucial, greatly increasing the contextual range of neural models. In another crucial development, researchers began to use pre-trained word embeddings derived from learning predictive relationships among words (14, 15). When training a neural model for a specific task, such embeddings could then be used to directly represent training words in the model’s input. The embeddings were based on the aggregate statistics of large text corpora, and captured both general and specific predictive relationships, supporting generalization at both general and specific levels. Using these embeddings, task-focused models trained with relatively small data sets could generalize what they learned from training on frequent words (such as sofa) to infrequent words with similar predictive relationships (such as settee).\\nA limitation of the above approach is that the same representation of a word is used every time it occurs, regardless of context. However, in line with the principle of mutual constraint satisfaction, humans interpret words, including ambiguous words like bank, in accordance with the context (16), rapidly assigning a contextually appropriate meaning to each word based on all other words. A fixed embedding also limits predicting other words; the predictive implication of the word bank depends on which kind of bank is involved. Initial steps\\nMcClelland et al.: Language understanding in humans and machines 2\\ntoward context sensitivity (17) recognize this limitation. We consider a fuller solution in the next section.\\nAttention and fully contextualized embeddings. Breakthroughs in neural language modeling have come from recent models that construct fully contextualized word representations (18–22). The models represent words via a mutual constraint satisfaction process in which each word in a text span influences the representation of every other word. BERT (20) is a key model in this class, illustrated in Fig. 3 as it encodes the end of the sentence John reached the bank of the river (example from (23)). An initial context-independent representation of each word is first combined with a positional representation (bottom row of boxes in the figure). Then, a separate copy of the same neural network module updates the representation in each position with input from all other positions. This process uses queries (red) at each position that are compared to keys (yellow) at all positions to form weightings (mauve boxes) that determine how strongly the values (blue) from each position contribute to the combined attention vectors (grey boxes) that provide context-sensitivity. The computation iterates over many layers, allowing words whose representations have been influenced by their context to influence the representations of other words. The process allows selection among alternative distinct meanings of a word like bank as well as graded shading of word meaning by context, for example assigning different emotional valance to the dogs in the dog wagged its tail and the dog snarled.\\nKey ingredients of the contemporary models are (i) bidirectionality of information flow during processing and (ii) an attention mechanism, which replaces the LSTM mechanism to enhance the exploitation of context. Bidirectionality matters because the meaning of a word in context depends on what comes after it as well as what comes before; in the example sentence, the last word, river, determines the meaning of bank. While it is remarkable how much can be done with strictly leftto-right constraint propagation (24), bidirectionality allows neural models to implement a mutual constraint satisfaction process in which the representation of each word depends on all other words. The attention mechanism distinguishes these models from earlier LSTM-based neural language models, and is used in both bidirectional and left-to-right models. Attention has proven to be even more effective than the LSTM mechanism in allowing networks to capture long-distance dependencies. Rather than requiring information about a context word to reach a target word through an iteration of the LSTM\\nfor every intervening word, the context reaches the target directly. Likewise, gradient learning signals skip over the intervening words, avoiding the dissipation of learning signal that would otherwise occur.\\nBERT-based models have produced remarkable improvements on a wide range of language tasks (25, 26). The models can be pre-trained on massive text corpora, providing useful representations for subsequent tasks for which little specific training data exists (27). These models seem to capture syntactic, semantic and world knowledge and they are beginning to address tasks once thought beyond their reach. For example, Winograd Schema challenge (28) requires determining the referent of a pronoun (here it) in a sentence such as The trophy did not fit in the suitcase because it was too ___. For a person, world knowledge tells us that if the missing word is big the referent must be the trophy, but if it is small the referent must be the suitcase. The latest models achieve ever-higher scores on benchmarks including variants of the Winograd challenge (29). However, variants of test materials that do not fool humans continue to stymie even the best models (30), and further refinements in the models and their assessment will be required before it will be clear what such models can achieve.'},\n",
       "  {'heading': 'The Human Integrated Understanding System (IUS)',\n",
       "   'text': 'Situations and objects. Despite the successes of neural language modeling, an important limitation is that these models are purely language based. We need models in which language is a part of an integrated understanding system (IUS) for understanding and communicating about the situations we encounter and the objects that participate in them. Representations of situations constitute our models of our world and guide our behavior and our interpretation of language. Indeed, resolving the referent of the pronoun in a Winograd sentence would follow from building a representation of the situation the sentence describes. In the situation in which a trombone does not fit in a suitcase, the natural reason would be that the trombone is too big or the suitcase too small; the identity of the referent of the pronoun follows from realizing this. Thus, solving the Winograd Schema challenge is a natural byproduct of the human language understanding process. In short, we argue that language evolved for communication about situations and our systems should address this goal.\\nSituations can be concrete and static, such as one where a cat is on a mat, or they may be events such as one where a boy hits a ball. They can be conceptual, social or legal, such as one where a court invalidates a law. They may even be imaginary. The objects may be real or fictitious physical objects or locations; animals, persons, groups or organizations; beliefs or other states of mind; or entities such as theories, laws or constitutions. Here we focus on concrete situations, considering other cases below. Our proposal builds on classic work in linguistics (31, 32), human cognition (33), artificial intelligence (34), and an early PDP model (35) and dovetails with an emerging perspective in cognitive neuroscience (36).\\nAs humans process language, we construct a representation of the situation the language describes from the stream of words and other available information. Words and their sequencing serve as clues to meaning (37) that jointly constrain the understanding of the situation and each object participating in it (35). Consider this passage:\\nJohn spread jam on a slice of bread. The knife had\\nMcClelland et al.: Language understanding in humans and machines 3\\nbeen dipped in poison. John ate the bread and soon began to feel sick.\\nWe can make many inferences here: that the jam was spread with the poisoned knife, that some of the poison was transferred to the bread, and that this may have led to John’s sickness. Note that the entities here are objects, not words, and the situation could instead be conveyed by a silent movie.\\nEvidence that humans construct situation representations comes from classic work by Bransford and colleagues (33, 38). This work demonstrates that (1) we understand and remember texts better when we can relate the statements in the text to a familiar situation; (2) information that conveys aspects of the situation can be provided by a picture accompanying the text; (3) the characteristics of the objects we remember depend on the situations in which they occurred in a text; (4) we represent in memory objects not explicitly mentioned in texts; and (5) after hearing a sentence describing spatial or conceptual relationships among objects, we retain memory for these relationships rather than the linguistic input. Further, evidence from eye movements shows that people use linguistic and non-linguistic input jointly and immediately as they process language in context (39). For example, just after hearing The man will drink ... participants look at a full wine glass rather than an empty beer glass (40). After hearing The man drank, they look at the empty beer glass. Thus, language understanding involves constructing–in real time–a representation of the situation being described by language input, including the objects involved and their spatial relationships with each other, using visual and linguistic inputs.\\nThe understanding system in the brain. Fig. 4 presents a depiction of our proposed integrated understanding system. Our proposal is both a theory of the brain basis of understanding and a proposed architecture for future language understanding research. It is largely consistent with proposals in (36). First, we focus on a part of the system, called the neocortical system, that is sufficient to combine linguistic and non-linguistic input to understand the object and situation referred to upon hearing a sentence containing the word bat while observing a corresponding situation in the world. This system consists of the blue ovals (corresponding to pools of neurons in the brain) and blue arrows (connections between these pools) in the figure. One population subserves a visual representation/embedding of the given situation, and another subserves a non-semantic linguistic representation capturing the sound structure (phonology) of co-occurring spoken language. The third represents objects participating in the situation, and the fourth represents the overall situation itself. Within each pool, and between each connected pair of pools, the neurons are reciprocally interconnected via learning-dependent pathways allowing mutual constraint satisfaction among all of the elements of each of the embedding types. Brain regions for representing visual and linguistic inputs are well-established, and the evidence for their involvement in a mutual constraint satisfaction process is substantial (41). Here we focus on the evidence for object and situation representations in the brain.\\nObject representations. A brain area near the front of the temporal lobe houses neurons whose activity provides an embedding capturing the properties of an object someone is considering (42). Damage to this area impairs the ability to name objects, to grasp objects correctly in service of their intended use, to\\nmatch objects with their names or the sounds that they make, and to pair objects that go together with each other, either from their names or from pictures. Models that capture these findings (43) treat this area as the hidden layer of an interactive, recurrent network with bidirectional connections to other layers corresponding to brain areas that represent different types of object properties including the object’s name. In these models, an input to any of these other layers activates the corresponding pattern in the hidden layer, which in turn activates the corresponding patterns in the other layers, supporting, for example, the ability to produce the name of an object from visual input. Damage (simulated by removal of neurons in the hidden layer) degrades the model’s representations, capturing the patterns of errors made by patients with the condition.\\nSituation representations. The situation representation specifies the event or situation conveyed by visual and/or language input. Evidence from behavioral studies indicates that construction of a situation representation can occur with or without language input (44) or through the convergent influence of both sources of information (40). Cognitive neuroscience research supports the idea that the situation representation arises in a set of interconnected brain areas primarily located in the frontal and parietal lobes (36, 45). In recent work, brain imaging data is used to analyze the time-varying patterns of neural activity that arise during the processing of a temporally extended event sequence. The activity patterns that\\nMcClelland et al.: Language understanding in humans and machines 4\\nrepresent corresponding events in a sequence are largely the same, whether the information about the sequence comes from watching a movie, hearing or reading a narrative description, or recalling the movie after having seen it (46, 47).\\nSituation-specific constraints. An important feature of our braininspired proposal is the use of distinct situation and object representations, and the idea that the constraints on the participating objects are mediated by the situation representation. The advantage of this is that it allows these constraints to be situation-specific. For example, the dogs in the events described by the sentences the boy ran to the dog and the boy ran from the dog are likely to be different, and a comprehender will represent them differently (38). In general, context-sensitivity is best captured by a mediating representation rather than direct associations among constituents (48). While BERT-like models might partially capture such constraints implicitly, an integrated situation representation may be more effective.\\nIn summary, the brain contains distinct areas that represent each input modality and the objects and situations conveyed through them, computing these representations through a mutual constraint satisfaction process combining language and other inputs. Emulating this architecture in machines could contribute to achieving human-level language understanding. What would a computational instantiation of our system look like? It is likely that a biologically realistic version would differ in some ways from the most effective machine version. Contemporary attention-based language models can deploy attention over tens to thousands of words kept in their current system state, but evidence from brain imaging data collected during movie comprehension suggests that activation states in visual, speech, and object areas change rapidly as events unfold, while the brain state tends to be more constant, changing only at event boundaries in brain areas associated with situation representations (47). In humans, spanning longer temporal windows, including multi-event narratives, appears to require the complementary learning system we consider next.\\nComplementary Learning Systems. Learning plays a crucial role in understanding. The knowledge in the connection weights in the neural networks we have described is acquired through the accumulation of very small adjustments based on each experience. The connection weights gradually become sensitive to subtle higher-order statistical relationships, taking more and more context into account as learning continues (49), and exhibiting sensitivity both to general and recurring specific information (e.g., names of close friends and famous people). In our proposed architecture, this gradual process occurs in all the pathways represented by the blue arrows in Fig. 4, just as it does in the artificial neural language models considered above. However, this learning mechanism is not well suited to acquiring new information rapidly, and attempting to learn specific new information quickly by focused repetition leads to catastrophic interference with what is already known (50).\\nYet, humans can often rely on information presented just once at an arbitrary time in the past to inform our current understanding. Returning to the beer John left in the cooler, to anticipate that John will not find the beer when he opens the cooler again, we must rely on information acquired when we first heard about the beer being stolen. Such situations are ubiquitous, and a learning system must be able to exploit such information, but BERT and the other models described\\npreviously are limited in this way. Though some models hold long word sequences in an active state, when one text is replaced with another, only the small connection adjustments described above remain, leaving these systems without access to the specifics of the prior information.\\nThe human brain contains a system that addresses this limitation. Consider a situation in which someone sees a previously unfamiliar object and hears a spoken statement about it, as illustrated in Fig. 4B. The visual input provides one source of information about the object (a previously unfamiliar animal), while the linguistic input provides its name. Humans show robust learning after just two brief exposures to such pairings (51). This form of learning depends on the hippocampus and adjacent areas in the medial temporal lobes (MTL) of the brain (51). While details of the role of the MTL in learning and memory continue to be debated (52, 53), there is consensus that the MTL is crucial for the initial formation of new memories, including memories for specific events and their constituent objects and situations, while general knowledge, the ability to understand language, and previously acquired skills are unaffected by MTL damage.\\nThe evidence from MTL damage suggests there is a fast learning system in the MTL. According to complementary learning systems theory (CLST) (54–56) this system (shown in red in Fig. 4) provides an integrated representation of the understanding system state, and employs modifiable connections within the MTL (red arrow) that can change rapidly to support new learning based on a single experience. The green arrows represent connections that carry information between the neocortical (blue) and MTL (red) systems so the systems can influence each other.\\nLet us consider how, according to CLST, a human can learn about the numbat (see Fig. 4B) from an experience seeing it and hearing a sentence about it (56). The input to the MTL is thought to be an embedding that captures (i.e., can be used to reconstruct) the patterns in the neocortical areas that arise from the experience. Networks within the MTL (not shown) map the MTL input representation to a sparser one deep inside the MTL, maximizing distinctness and minimizing interference among experiences (54). Large connection weight changes within the MTL associate the elements of the sparse representation with each other and with the MTL input representation. When the person hears the word numbat in a later situation, connections to the MTL from the neocortex activate neurons in the MTL input representation. The weight changes that occurred on prior exposure support the reconstruction of the complete MTL representation, and return connections to the neocortex then support approximate reconstruction of the visual, speech, object and situation representations formed during the initial exposure to the numbat. These representations are the explicit memory for the prior experience, allowing the cortical network to use what it learned from one prior exposure to contribute to understanding the new situation.\\nIntegrating information into the neocortex. How can knowledge initially dependent on the MTL be integrated into the neocortex? According to CLST (55), the neocortex learns gradually through interleaved presentations of new and familiar items; this process avoids interference of new items with what is already known. Interleaved learning can occur through ongoing experience, as would happen if, for example, we acquire a pet numbat that we then see every day, while continuing to have\\nMcClelland et al.: Language understanding in humans and machines 5\\nother experiences. Interleaving may also occur during rest or sleep through reactivation and replay of patterns stored in the MTL: Indeed, spontaneous replay of short snippets of previously experienced episodes occurs within the MTL during sleep and between behavioral episodes (see (56) for review).\\nIn summary, the human brain contains complementary learning systems that support the simultaneous use of many sources of information as we seek to understand an experienced situation. One of these systems acquires an integrated system of knowledge gradually through interleaved learning, including our knowledge of the meanings of words, the properties of frequently-encountered objects, and the characteristics of familiar situations. The other complements this system to allow information from specific experiences to be brought to bear on the interpretation of a current situation.'},\n",
       "  {'heading': 'Toward an Artificial Integrated Understanding System',\n",
       "   'text': 'Here we consider current deep learning research that is taking steps consistent with our IUS proposal, and point toward future directions that will be needed to achieve a truly integrated and fully functional understanding system. We begin within the context of language grounded within concrete visual and physical situations, then consider the role of memory, and finally turn to the extension of the approach to address understanding of more abstract objects, situations, and relations.\\nMapping vision and language to representations of objects. How might a model learn about situations that can occur in the world? The need for an artificial system of language understanding to be grounded in the external world has long been discussed. An early example is Winograd’s SHRDLU system (57), which produced and responded to language about a simulated physical world. Deep learning has enabled joint, end-to-end training of perceptual input and language (i.e., in a single synchronous optimization process). Recent advances with such models have greatly improved performance, resulting in applications transforming user experiences. When presented with a photograph, networks can now answer questions such as what is the man holding? or what color is the woman’s shirt? (58), demonstrating an ability to combine information from vision and language to understand a class of situations.\\nA very recent model (59) explicitly represents the objects in a scene, their properties, and their relations to other objects in a designed scene graph with slots for objects, their properties, and relations. It encodes questions as a series of instructions to find a target object or relation by searching the graph to answer a query. For example, the question what is the object beside the yellow bowl? can be answered by finding the yellow bowl, finding an object linked to it with the ‘beside’ relation, and then reading out this object’s identity. The approach advances the state of the art, though a large gap relative to humans remains. The model shares important properties with our proposal in that it explicitly treats language input as querying the model’s representation of the objects in the scene, and their conceptual properties. A natural extension consistent with IUS would be to build up scene representations using a combination of visual input and language, allowing text to enrich the representations of objects and relations.\\nA question this work raises is whether to build structured representations into one’s model. This is advocated in (59), but natural structure exhibits flexible embedding relationships\\nand is often only approximately characterized by explicit taxonomies, motivating use of emergent connection-based rather than hard-coded representational structures (60). A challenge, then, is to achieve comparable performance with models in which these concept-based object and relational representations emerge through learning.\\nEmbodied models for language understanding. Beyond the integration of vision and language, as illustrated in Fig. 4, we see progress coming from an even fuller integration of many additional information sources. Every source provides a basis for distinct learning objectives and enables information that is salient in one source to bootstrap learning and inference in the other. Important additional sources of information include non-language sound, touch and force-sensing, and information about one’s own actions.\\nIncorporating additional information sources will allow an IUS to go beyond answering questions about static images. Since image data has no temporal aspect, such models lack experience of events or processes. While models that jointly process video and language (61) may acquire some sensitivity to event structure and commonplace causal relationships, these systems do not make choices affecting the world they observe. Ultimately, an ability to link one’s actions to their consequences as one intervenes in the observed flow of events and interacts with other agents should provide the strongest basis for acquiring notions of cause and effect, of agency, and of self and others.\\nThese considerations motivate recent work on agentbased language learning in simulated interactive 3D environments (62–65). In (66), an agent was trained to identify, lift, carry and place objects relative to other objects in a virtual room, as specified by simplified language instructions. At each time step, the agent received a first-person visual observation (pixel-based image) that it processed to produce a representation of the scene. This was concatenated to the final state of an LSTM that processed the instruction, then passed to an integrative LSTM whose output was used to select a motor action. The agent gradually learned to follow instructions of the form find a pencil, lift up a basketball and put the teddy bear on the bed, encompassing 50 objects, and requiring up to 70 action steps to complete. Such instructions require the construction of representations based on language stimuli that enable the identification of objects and relations across space and time, and the integration of this information to inform motor behaviors. Importantly, without building in explicit object representations, the system supported the interpretation of novel instructions. For instance, an agent trained to lift a set of 20 objects in the environment, but only trained to put 10 of those in a specific location could place the remaining objects in the same location on command with over 90% accuracy.\\nNeural models often fail to exhibit systematic generalization, leading some to propose that more structure should be built in (67). While the agent’s level of systematicity does not reach human levels, this work suggests that grounding language learning can help support systematicity without building it in. Critically, the agent’s systematicity was contingent on the ego-centric, multimodal and temporally-extended experience of the agent. On the same set of generalization tests, both an alternative agent with a fixed perspective on a 2D grid world and a static neural network classifier that received only individual still image stimuli exhibited significantly worse gen-\\nMcClelland et al.: Language understanding in humans and machines 6\\neralization that a fully situated agent (Fig. 5). This underlines how affording neural networks access to rich, multi-modal interactive environments can stimulate the development of capacities that are essential for language learning.\\nDespite these promising signs, achieving fully human levels of generalization remains an important challenge. We propose that incorporating an MTL-like fast learning system will help address this by allowing new words to be linked to the corresponding object from just a single episode supporting use of the word to refer to the referent in other situations.\\nAn artificial fast learning system. What might a fast learning system in an implementation of an integrated understanding system look like? The memory system in the differentiable neural computer (DNC) (68) is one possibility. These systems store embeddings derived from past episodes in slots that could store Integrated System State representations like those we attribute to the human MTL. Alternatively, they could store the entire ensemble of states across the visual, speech, object, and situation representations. Though we do not believe the brain has a separate slot for each memory, it can be useful to model it as though it does (56), and artificial systems with indefinite capacity could exceed human abilities in this regard. How might the retrieval of relevant information work in such a system? The DNC employs a querying system similar to the one in BERT and to proposals in the human memory literature, whereby the representation retrieved from the MTL is weighted by the degree of match between a query (which we would treat as coming from the neocortex) and each vector stored in memory. Close matches are favored in this computation (69), so that when there is a unique match (such as a single memory containing a once seen word like numbat), the corresponding object and situation representation could be retrieved. Retrieval could be based on a combination of context and item information, similar to human memory (70). Working out the details of such a system presents an exciting research direction for the future.\\nBeyond concrete situations. Our discussion has focused primarily on concrete situations. However, language allows us to discuss abstract ideas and situations, where grounding in the physical world can be very indirect and our learning about it comes primarily from language-based materials. Consider, for example, an understanding of Brexit. Concrete events involving actual people have occurred, but the issues and ques-\\ntions under consideration can only be communicated through language. What is the way forward toward developing models that can understand such a complex situation?\\nLanguage may have evolved in part to support transmission of complex, hierarchical knowledge about tools (71). However, utterances also had to support abstraction and complex dependencies between concrete objects, as well as social relationships between speakers. Words themselves provided a new abstract substrate for characterizing other words (72). Word embeddings are one implementation of this substrate: they can characterize abstract words like justice and represents without directly grounding them. In humans, encyclopedic knowledge grounded in such representations can be acquired in an MTL-dependent way from reading an encyclopedia article just once. For machines, forming integrated system state representations capturing the content and using a DNC-like system for their storage and retrieval might provide a starting place for enabling such knowledge to be acquired and used effectively.\\nThat said, words are uttered in real world contexts and there is a continuum between grounding and language-based linking for different words and different uses of words. For example, career is not only linked to other abstract words like work and specialization but also to more grounded concepts such as path and its extended metaphorical use for discussing the means to achieve goals (72). Embodied, simulation-based approaches to meaning (73, 74) build on this observation to bridge from concrete to abstract situations via metaphor. They posit that understanding words like grasp is directly linked to neural representations of the action of grabbing and that this circuitry is recruited for understanding the word in contexts such as grasping an idea. We consider situated agents as a critical catalyst for learning about how to represent and compose concepts pertaining to spatial, physical and other perceptually immediate phenomena—thereby providing a grounded edifice that can connect to both the low level brain circuitry for motor action and to representations derived primarily from language.\\nConclusion. Language does not stand alone. The integrated understanding system in the brain connects language to representations of objects and situations and enhances language understanding by exploiting the full range of our multi-sensory experience of the world, our representations of our motor actions, and our memory of previous situations. We have argued that the next generation language understanding system should emulate this system in the brain and we have sketched some aspects of the form such a system might take. While we have emphasized understanding of concrete situations, we have argued that understanding more abstract language builds upon this concrete foundation, pointing toward the possibility that it may someday be possible to build artificial systems that understand abstract situations far beyond the concrete and the here-and-now. In sum, we have proposed that modeling the integrated understanding system in the brain will take us closer to capturing human-level language understanding and intelligence.\\nACKNOWLEDGMENTS. This article grew out of a workshop organized by HS at Meaning in Context 3, Stanford University, September 2017. We thank Chris Potts for discussion. HS was supported by ERC Advanced Grant #740516.\\nMcClelland et al.: Language understanding in humans and machines 7\\n1. Wu Y, et al. (2016) Google’s neural machine translation system: Bridging the gap between human and machine translation. -. 2. Rosenblatt F (1961) Principles of neurodynamics. perceptrons and the theory of brain mechanisms. (Spartan Books, Cornell University, Ithaca, New York). 3. Rumelhart DE, McClelland JL, the PDP research group (1986) Parallel Distributed Processing. Explorations in the Microstructure of Cognition. Volume 1: Foundations. (The MIT Press, Cambridge, MA). 4. Rumelhart DE, McClelland JL (1986) On learning the past tenses of English verbs in Parallel Distributed Processing. Explorations in the Microstructure of Cognition. Volume 2: Psychological and Biological Models, eds. McClelland JL, Rumelhart DE, the PDP Research Group. (MIT Press), pp. 216–271. 5. Rumelhart DE (1977) Toward an interactive model of reading in Attention & Performance VI, ed. Dornic S. (LEA, Hillsdale, NJ), pp. 573–603. 6. McClelland JL, Rumelhart DE (1981) An interactive activation model of context effects in letter perception: I. an account of basic findings. Psychological review 88(5):375. 7. Hopfield JJ (1982) Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences 79(8):2554–2558. 8. Ackley DH, Hinton GE, Sejnowski TJ (1985) A learning algorithm for boltzmann machines. Cognitive science 9(1):147–169.\\n9. Elman JL (1990) Finding structure in time. Cognitive Science 14:179–211. 10. Gold EM (1967) Language identification in the limit. Information and control 10(5):447–474. 11. Elman JL (1991) Distributed representations, simple recurrent networks, and grammatical\\nstructure. Mach. Learn. 7(2/3):195–225. 12. Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J (2001) Gradient flow in recurrent nets: the\\ndifficulty of learning long-term dependencies in A Field Guide to Dynamical Recurrent Neural Networks, eds. Kremer SC, Kolen JF. (IEEE Press). 13. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural computation 9(8):1735–1780. 14. Collobert R, et al. (2011) Natural language processing (almost) from scratch. Journal of Machine Learning Research 12(Aug):2493–2537. 15. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013) Distributed representations of words and phrases and their compositionality in Neural Information Processing Systems. pp. 3111–3119. 16. Simpson GB (1981) Meaning dominance and semantic context in the processing of lexical ambiguity. Journal of verbal learning and verbal behavior 20(1):120–136. 17. Rudolph M, Ruiz F, Blei D (2017) Structured embedding models for grouped data in Advances in Neural Information Processing Systems. 18. Peters ME, et al. (2018) Deep contextualized word representations. CoRR abs/1802.05365. 19. Vaswani A, et al. (2017) Attention is all you need in Advances in Neural Information Process-\\ning Systems 30. (Curran Associates, Inc.), pp. 5998–6008. 20. Devlin J, Chang MW, Lee K, Toutanova K (2018) BERT: Pre-training of Deep Bidirectional\\nTransformers for Language Understanding. ArXiv e-prints. 21. Yang Z, et al. (2019) Xlnet: Generalized autoregressive pretraining for language understand-\\ning. CoRR abs/1906.08237. 22. Dehghani M, Gouws S, Vinyals O, Uszkoreit J, Kaiser L (2018) Universal transformers. CoRR\\nabs/1807.03819. 23. Uszkoreit J (2017) Transformer: A novel neural network architecture for language understand-\\ning, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html. 24. Radford A, Narasimhan K, Salimans T, Sutskever I (2018) Improving language understanding\\nby generative pre-training. OpenAI Preprint. 25. Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training of deep bidirectional\\ntransformers for language understanding. arXiv preprint arXiv:1810.04805. 26. Wang A, et al. (2018) Glue: A multi-task benchmark and analysis platform for natural lan-\\nguage understanding. arXiv preprint arXiv:1804.07461. 27. Wang A, et al. (2019) Superglue: A stickier benchmark for general-purpose language under-\\nstanding systems. arXiv preprint arXiv:1905.00537. 28. Levesque H, Davis E, Morgenstern L (2012) The winograd schema challenge in Thirteenth\\nInternational Conference on the Principles of Knowledge Representation and Reasoning. 29. Raffel C, Shazeer N, Roberts A, , et al. (2019) Exploring the limits of transfer learning with a\\nunified text-to-text transformer. arXiv preprint arXiv:1910.10683. 30. Jia R, Liang P (2017) Adversarial examples for evaluating reading comprehension systems in\\nProceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. (Association for Computational Linguistics), pp. 2021–2031. 31. Lakoff G (1987) Women, Fire, and Dangerous Things. (The University of Chicago Press, Chicago). 32. Langacker RW (1987) Foundations of cognitive grammar: Theoretical prerequisites. (Stanford university press) Vol. 1. 33. Bransford JD, Johnson MK (1972) Contextual prerequisites for understanding: Some investigations of comprehension and recall. Journal of verbal learning and verbal behavior 11(6):717–726. 34. Schank RC (1983) Dynamic memory: A theory of reminding and learning in computers and people. (Cambridge University Press). 35. John MFS, McClelland JL (1990) Learning and applying contextual constraints in sentence comprehension. Artificial Intelligence 46(1):217 – 257. 36. Hasson U, Egidi G, Marelli M, Willems RM (2018) Grounding the neurobiology of language in first principles: The necessity of non-language-centric explanations for language comprehension. Cognition 180:135–157. 37. Rumelhart DE (1979) Some problems with the notion that words have literal meanings in Metaphor and thought, ed. Ortony A. (Cambridge Univ. Press, Cambridge, UK), pp. 71–82. 38. Barclay J, Bransford JD, Franks JJ, McCarrell NS, Nitsch K (1974) Comprehension and semantic flexibility. Journal of Verbal Learning and Verbal Behavior 13(4):471 – 481. 39. Tanenhaus MK, Spivey-Knowlton MJ, Eberhard KM, Sedivy JC (1995) Integration of visual and linguistic information in spoken language comprehension. Science pp. 1632–1634. 40. Altmann GT, Kamide Y (2007) The real-time mediation of visual attention by language and\\nworld knowledge: Linking anticipatory (and other) eye movements to linguistic processing. Journal of Memory and Language 57(4):502–518. 41. McClelland JL, Mirman D, Bolger DJ, Khaitan P (2014) Interactive activation and mutual constraint satisfaction in perception and cognition. Cognitive science 38(6):1139–1189. 42. Patterson K, Nestor PJ, Rogers TT (2007) Where do you know what you know? the representation of semantic knowledge in the human brain. Nature Reviews Neuroscience 8(12):976. 43. Rogers TT, et al. (2004) Structure and deterioration of semantic memory: a neuropsychological and computational investigation. Psychological review 111(1):205–235. 44. Zwaan RA, Radvansky GA (1998) Situation models in language comprehension and memory. Psychological bulletin 123(2):162–185. 45. Ranganath C, Ritchey M (2012) Two cortical systems for memory-guided behaviour. Nature Reviews Neuroscience 13:713. Review Article. 46. Zadbood A, Chen J, Leong Y, Norman K, Hasson U (2017) How we transmit memories to other brains: Constructing shared neural representations via communication. Cerebral Cortex 27(10):4988–5000. 47. Baldassano C, et al. (2017) Discovering event structure in continuous narrative perception and memory. Neuron 95(3):709–721. 48. Hinton GE (1981) Implementing semantic networks in parallel hardware in Parallel Models of Associative Memory, eds. Hinton GE, Anderson JA. (Erlbaum), pp. 161–187. 49. Cleeremans A, McClelland JL (1991) Learning the structure of event sequences. Journal of Experimental Psychology: General 120(3):235. 50. McCloskey M, Cohen NJ (1989) Catastrophic interference in connectionist networks: The sequential learning problem in Psychology of learning and motivation. (Elsevier) Vol. 24, pp. 109–165. 51. Warren DE, Duff MC (2019) Fast mappers, slow learners: Word learning without hippocampus is slow and sparse irrespective of methodology. Cognitive neuroscience pp. 1–3. 52. Squire LR (1992) Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans. Psychological review 99(2):195. 53. Yonelinas A, Ranganath C, Ekstrom A, Wiltgen B (2019) A contextual binding theory of episodic memory: systems consolidation reconsidered. Nature Reviews Neuroscience. 54. Marr D (1971) Simple memory: a theory for archicortex. Philosophical Transactions of the Royal Society of London B: Biological Sciences 262(841):23–81. 55. McClelland JL, McNaughton BL, O’Reilly RC (1995) Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological Review 102(3):419–457. 56. Kumaran D, Hassabis D, McClelland JL (2016) What learning systems do intelligent agents need? Complementary learning systems theory updated. Trends in Cognitive Sciences 20(7):512–534. 57. Winograd T (1972) Understanding natural language. Cognitive psychology 3(1):1–191. 58. MacLeod H, Bennett CL, Morris MR, Cutrell E (2017) Understanding blind people’s experi-\\nences with computer-generated captions of social media images in Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. (ACM), pp. 5988–5999. 59. Hudson DA, Manning CD (2019) Learning by abstraction: The neural state machine. arXiv preprint arXiv:1907.03950. 60. Rumelhart DE, Smolensky P, McClelland JL, Hinton G (1986) Pdp models of schemata and sequential thought processes in pdp models. Parallel distributed processing: Explorations in the microstructure of cognition 2:3–57. 61. Yu H, Wang J, Huang Z, Yang Y, Xu W (2016) Video paragraph captioning using hierarchical recurrent neural networks in Proceedings of the IEEE conference on computer vision and pattern recognition. pp. 4584–4593. 62. Hermann KM, et al. (2017) Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551. 63. Das R, Zaheer M, Reddy S, McCallum A (2017) Question answering on knowledge bases and text using universal schema and memory networks in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistic. pp. 358–365. 64. Chaplot DS, Sathyendra KM, Pasumarthi RK, Rajagopal D, Salakhutdinov R (2017) Gated-attention architectures for task-oriented language grounding. arXiv preprint arXiv:1706.07230. 65. Oh J, Singh S, Lee H, Kohli P (2017) Zero-shot task generalization with multi-task deep reinforcement learning in Proceedings of the 34th International Conference on Machine LearningVolume 70. (JMLR. org), pp. 2661–2670. 66. Hill F, et al. (2019) Emergent systematic generalization in a situated agent. arXiv preprint arXiv:1910.00571. 67. Lake BM, Salakhutdinov R, Tenenbaum JB (2015) Human-level concept learning through probabilistic program induction. Science 350(6266):1332–1338. 68. Graves A, et al. (2016) Hybrid computing using a neural network with dynamic external memory. Nature 538(7626):471–476. 69. Hintzman DL (1984) Minerva 2: A simulation model of human memory. Behavior Research Methods, Instruments, & Computers 16(2):96–101. 70. Polyn SM, Norman KA, Kahana MJ (2009) A context maintenance and retrieval model of organizational processes in free recall. Psychological review 116(1):129. 71. Stout D, Chaminade T (2012) Stone tools, language and the brain in human evolution. Philos Trans R Soc Lond B Biol Sci 367(1585):75–87. 72. Bryson J (2008) Embodiment vs. memetics. Mind and Society 7(1):77–94. 73. Lakoff G, Johnson M (1980) Metaphors We Live By. (University of Chicago, Chicago, IL). 74. Feldman J, Narayanan S (2004) Embodied meaning in a neural theory of language. Brain\\nand Language 89:385–392.\\nMcClelland et al.: Language understanding in humans and machines 8'}],\n",
       " 'references': [{'title': '2016) Google’s neural machine translation system: Bridging the gap between human and machine',\n",
       "   'author': ['Y Wu'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '1',\n",
       "   'shortCiteRegEx': '1',\n",
       "   'year': 2016},\n",
       "  {'title': 'Principles of neurodynamics. perceptrons and the theory of brain mechanisms',\n",
       "   'author': ['F Rosenblatt'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '2',\n",
       "   'shortCiteRegEx': '2',\n",
       "   'year': 1961},\n",
       "  {'title': 'Parallel Distributed Processing. Explorations in the Microstructure of Cognition. Volume 1: Foundations',\n",
       "   'author': ['DE Rumelhart', 'JL McClelland'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '3',\n",
       "   'shortCiteRegEx': '3',\n",
       "   'year': 1986},\n",
       "  {'title': 'On learning the past tenses of English verbs in Parallel Distributed Processing',\n",
       "   'author': ['DE Rumelhart', 'JL McClelland'],\n",
       "   'venue': 'Explorations in the Microstructure of Cognition. Volume 2: Psychological and Biological Models, eds. McClelland JL, Rumelhart DE, the PDP Research Group',\n",
       "   'citeRegEx': '4',\n",
       "   'shortCiteRegEx': '4',\n",
       "   'year': 1986},\n",
       "  {'title': 'Toward an interactive model of reading in Attention & Performance VI, ed',\n",
       "   'author': ['DE Rumelhart'],\n",
       "   'venue': 'Dornic S. (LEA,',\n",
       "   'citeRegEx': '5',\n",
       "   'shortCiteRegEx': '5',\n",
       "   'year': 1977},\n",
       "  {'title': 'An interactive activation model of context effects in letter perception: I. an account of basic findings. Psychological review',\n",
       "   'author': ['JL McClelland', 'DE Rumelhart'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '6',\n",
       "   'shortCiteRegEx': '6',\n",
       "   'year': 1981},\n",
       "  {'title': 'Neural networks and physical systems with emergent collective computational abilities',\n",
       "   'author': ['JJ Hopfield'],\n",
       "   'venue': 'Proceedings of the national academy of sciences 79(8):2554–2558',\n",
       "   'citeRegEx': '7',\n",
       "   'shortCiteRegEx': '7',\n",
       "   'year': 1982},\n",
       "  {'title': 'A learning algorithm for boltzmann machines. Cognitive science 9(1):147–169',\n",
       "   'author': ['DH Ackley', 'GE Hinton', 'TJ Sejnowski'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '8',\n",
       "   'shortCiteRegEx': '8',\n",
       "   'year': 1985},\n",
       "  {'title': 'Finding structure in time. Cognitive Science 14:179–211',\n",
       "   'author': ['JL Elman'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '9',\n",
       "   'shortCiteRegEx': '9',\n",
       "   'year': 1990},\n",
       "  {'title': 'Language identification in the limit. Information and control 10(5):447–474',\n",
       "   'author': ['EM Gold'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '10',\n",
       "   'shortCiteRegEx': '10',\n",
       "   'year': 1967},\n",
       "  {'title': 'Distributed representations, simple recurrent networks, and grammatical structure',\n",
       "   'author': ['JL Elman'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '11',\n",
       "   'shortCiteRegEx': '11',\n",
       "   'year': 1991},\n",
       "  {'title': 'Gradient flow in recurrent nets: the difficulty of learning long-term dependencies in A Field Guide to Dynamical Recurrent Neural Networks, eds',\n",
       "   'author': ['S Hochreiter', 'Y Bengio', 'P Frasconi', 'J Schmidhuber'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '12',\n",
       "   'shortCiteRegEx': '12',\n",
       "   'year': 2001},\n",
       "  {'title': 'Long short-term memory. Neural computation 9(8):1735–1780',\n",
       "   'author': ['S Hochreiter', 'J Schmidhuber'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '13',\n",
       "   'shortCiteRegEx': '13',\n",
       "   'year': 1997},\n",
       "  {'title': 'Natural language processing (almost) from scratch',\n",
       "   'author': ['R Collobert'],\n",
       "   'venue': 'Journal of Machine Learning Research 12(Aug):2493–2537',\n",
       "   'citeRegEx': '14',\n",
       "   'shortCiteRegEx': '14',\n",
       "   'year': 2011},\n",
       "  {'title': 'Distributed representations of words and phrases and their compositionality in Neural Information Processing Systems',\n",
       "   'author': ['T Mikolov', 'I Sutskever', 'K Chen', 'GS Corrado', 'J Dean'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '15',\n",
       "   'shortCiteRegEx': '15',\n",
       "   'year': 2013},\n",
       "  {'title': 'Meaning dominance and semantic context in the processing of lexical ambiguity. Journal of verbal learning and verbal behavior 20(1):120–136',\n",
       "   'author': ['GB Simpson'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '16',\n",
       "   'shortCiteRegEx': '16',\n",
       "   'year': 1981},\n",
       "  {'title': 'Structured embedding models for grouped data in Advances in Neural Information Processing Systems',\n",
       "   'author': ['M Rudolph', 'F Ruiz', 'D Blei'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '17',\n",
       "   'shortCiteRegEx': '17',\n",
       "   'year': 2017},\n",
       "  {'title': 'Deep contextualized word representations. CoRR abs/1802.05365',\n",
       "   'author': ['ME Peters'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '18',\n",
       "   'shortCiteRegEx': '18',\n",
       "   'year': 2018},\n",
       "  {'title': 'Attention is all you need in Advances in Neural Information Processing Systems 30',\n",
       "   'author': ['A Vaswani'],\n",
       "   'venue': '(Curran Associates,',\n",
       "   'citeRegEx': '19',\n",
       "   'shortCiteRegEx': '19',\n",
       "   'year': 2017},\n",
       "  {'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. ArXiv e-prints',\n",
       "   'author': ['J Devlin', 'MW Chang', 'K Lee', 'K Toutanova'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '20',\n",
       "   'shortCiteRegEx': '20',\n",
       "   'year': 2018},\n",
       "  {'title': 'Xlnet: Generalized autoregressive pretraining for language understanding',\n",
       "   'author': ['Z Yang'],\n",
       "   'venue': 'CoRR abs/1906.08237',\n",
       "   'citeRegEx': '21',\n",
       "   'shortCiteRegEx': '21',\n",
       "   'year': 2019},\n",
       "  {'title': 'Transformer: A novel neural network architecture for language understanding, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html',\n",
       "   'author': ['J Uszkoreit'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '23',\n",
       "   'shortCiteRegEx': '23',\n",
       "   'year': 2017},\n",
       "  {'title': 'Improving language understanding by generative pre-training',\n",
       "   'author': ['A Radford', 'K Narasimhan', 'T Salimans', 'I Sutskever'],\n",
       "   'venue': 'OpenAI Preprint',\n",
       "   'citeRegEx': '24',\n",
       "   'shortCiteRegEx': '24',\n",
       "   'year': 2018},\n",
       "  {'title': 'Bert: Pre-training of deep bidirectional transformers for language understanding',\n",
       "   'author': ['J Devlin', 'MW Chang', 'K Lee', 'K Toutanova'],\n",
       "   'venue': 'arXiv preprint arXiv:1810.04805',\n",
       "   'citeRegEx': '25',\n",
       "   'shortCiteRegEx': '25',\n",
       "   'year': 2018},\n",
       "  {'title': 'Glue: A multi-task benchmark and analysis platform for natural language understanding',\n",
       "   'author': ['A Wang'],\n",
       "   'venue': 'arXiv preprint arXiv:1804.07461',\n",
       "   'citeRegEx': '26',\n",
       "   'shortCiteRegEx': '26',\n",
       "   'year': 2018},\n",
       "  {'title': 'Superglue: A stickier benchmark for general-purpose language understanding systems. arXiv preprint arXiv:1905.00537',\n",
       "   'author': ['A Wang'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '27',\n",
       "   'shortCiteRegEx': '27',\n",
       "   'year': 2019},\n",
       "  {'title': 'The winograd schema challenge in Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning',\n",
       "   'author': ['H Levesque', 'E Davis', 'L Morgenstern'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '28',\n",
       "   'shortCiteRegEx': '28',\n",
       "   'year': 2012},\n",
       "  {'title': 'Exploring the limits of transfer learning with a unified text-to-text transformer',\n",
       "   'author': ['C Raffel', 'N Shazeer', 'A Roberts'],\n",
       "   'venue': 'arXiv preprint arXiv:1910.10683',\n",
       "   'citeRegEx': '29',\n",
       "   'shortCiteRegEx': '29',\n",
       "   'year': 2019},\n",
       "  {'title': 'Adversarial examples for evaluating reading comprehension systems',\n",
       "   'author': ['R Jia', 'P Liang'],\n",
       "   'venue': 'Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing. (Association for Computational Linguistics),',\n",
       "   'citeRegEx': '30',\n",
       "   'shortCiteRegEx': '30',\n",
       "   'year': 2017},\n",
       "  {'title': 'Women, Fire, and Dangerous Things. (The University of Chicago Press, Chicago)',\n",
       "   'author': ['G Lakoff'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '31',\n",
       "   'shortCiteRegEx': '31',\n",
       "   'year': 1987},\n",
       "  {'title': 'Foundations of cognitive grammar: Theoretical prerequisites',\n",
       "   'author': ['RW Langacker'],\n",
       "   'venue': '(Stanford university press)',\n",
       "   'citeRegEx': '32',\n",
       "   'shortCiteRegEx': '32',\n",
       "   'year': 1987},\n",
       "  {'title': 'Contextual prerequisites for understanding: Some investigations of comprehension and recall. Journal of verbal learning and verbal behavior 11(6):717–726',\n",
       "   'author': ['JD Bransford', 'MK Johnson'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '33',\n",
       "   'shortCiteRegEx': '33',\n",
       "   'year': 1972},\n",
       "  {'title': 'Dynamic memory: A theory of reminding and learning in computers and people',\n",
       "   'author': ['RC Schank'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '34',\n",
       "   'shortCiteRegEx': '34',\n",
       "   'year': 1983},\n",
       "  {'title': 'Learning and applying contextual constraints in sentence comprehension',\n",
       "   'author': ['MFS John', 'JL McClelland'],\n",
       "   'venue': 'Artificial Intelligence',\n",
       "   'citeRegEx': '35',\n",
       "   'shortCiteRegEx': '35',\n",
       "   'year': 1990},\n",
       "  {'title': 'Grounding the neurobiology of language in first principles: The necessity of non-language-centric explanations for language comprehension. Cognition 180:135–157',\n",
       "   'author': ['U Hasson', 'G Egidi', 'M Marelli', 'RM Willems'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '36',\n",
       "   'shortCiteRegEx': '36',\n",
       "   'year': 2018},\n",
       "  {'title': 'Some problems with the notion that words have literal meanings in Metaphor and thought, ed',\n",
       "   'author': ['DE Rumelhart'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '37',\n",
       "   'shortCiteRegEx': '37',\n",
       "   'year': 1979},\n",
       "  {'title': 'Comprehension and semantic flexibility',\n",
       "   'author': ['J Barclay',\n",
       "    'JD Bransford',\n",
       "    'JJ Franks',\n",
       "    'NS McCarrell',\n",
       "    'K Nitsch'],\n",
       "   'venue': 'Journal of Verbal Learning and Verbal Behavior',\n",
       "   'citeRegEx': '38',\n",
       "   'shortCiteRegEx': '38',\n",
       "   'year': 1974},\n",
       "  {'title': 'Integration of visual and linguistic information in spoken language comprehension',\n",
       "   'author': ['MK Tanenhaus',\n",
       "    'MJ Spivey-Knowlton',\n",
       "    'KM Eberhard',\n",
       "    'JC Sedivy'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '39',\n",
       "   'shortCiteRegEx': '39',\n",
       "   'year': 1995},\n",
       "  {'title': 'The real-time mediation of visual attention by language and world knowledge: Linking anticipatory (and other) eye movements to linguistic processing',\n",
       "   'author': ['GT Altmann', 'Y Kamide'],\n",
       "   'venue': 'Journal of Memory and Language',\n",
       "   'citeRegEx': '40',\n",
       "   'shortCiteRegEx': '40',\n",
       "   'year': 2007},\n",
       "  {'title': 'Interactive activation and mutual constraint satisfaction in perception and cognition. Cognitive science 38(6):1139–1189',\n",
       "   'author': ['JL McClelland', 'D Mirman', 'DJ Bolger', 'P Khaitan'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '41',\n",
       "   'shortCiteRegEx': '41',\n",
       "   'year': 2014},\n",
       "  {'title': 'Where do you know what you know? the representation of semantic knowledge in the human brain',\n",
       "   'author': ['K Patterson', 'PJ Nestor', 'TT Rogers'],\n",
       "   'venue': 'Nature Reviews Neuroscience',\n",
       "   'citeRegEx': '42',\n",
       "   'shortCiteRegEx': '42',\n",
       "   'year': 2007},\n",
       "  {'title': 'Structure and deterioration of semantic memory: a neuropsychological and computational investigation',\n",
       "   'author': ['TT Rogers'],\n",
       "   'venue': 'Psychological review 111(1):205–235',\n",
       "   'citeRegEx': '43',\n",
       "   'shortCiteRegEx': '43',\n",
       "   'year': 2004},\n",
       "  {'title': 'Situation models in language comprehension and memory. Psychological bulletin 123(2):162–185',\n",
       "   'author': ['RA Zwaan', 'GA Radvansky'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '44',\n",
       "   'shortCiteRegEx': '44',\n",
       "   'year': 1998},\n",
       "  {'title': 'Two cortical systems for memory-guided behaviour. Nature Reviews Neuroscience 13:713',\n",
       "   'author': ['C Ranganath', 'M Ritchey'],\n",
       "   'venue': 'Review Article',\n",
       "   'citeRegEx': '45',\n",
       "   'shortCiteRegEx': '45',\n",
       "   'year': 2012},\n",
       "  {'title': 'How we transmit memories to other brains: Constructing shared neural representations via communication. Cerebral Cortex 27(10):4988–5000',\n",
       "   'author': ['A Zadbood', 'J Chen', 'Y Leong', 'K Norman', 'U Hasson'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '46',\n",
       "   'shortCiteRegEx': '46',\n",
       "   'year': 2017},\n",
       "  {'title': 'Discovering event structure in continuous narrative perception and memory. Neuron 95(3):709–721',\n",
       "   'author': ['C Baldassano'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '47',\n",
       "   'shortCiteRegEx': '47',\n",
       "   'year': 2017},\n",
       "  {'title': 'Implementing semantic networks in parallel hardware in Parallel Models of Associative Memory',\n",
       "   'author': ['GE Hinton'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '48',\n",
       "   'shortCiteRegEx': '48',\n",
       "   'year': 1981},\n",
       "  {'title': 'Learning the structure of event sequences. Journal of Experimental Psychology: General 120(3):235',\n",
       "   'author': ['A Cleeremans', 'JL McClelland'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '49',\n",
       "   'shortCiteRegEx': '49',\n",
       "   'year': 1991},\n",
       "  {'title': 'Catastrophic interference in connectionist networks: The sequential learning problem in Psychology of learning and motivation',\n",
       "   'author': ['M McCloskey', 'NJ Cohen'],\n",
       "   'venue': '(Elsevier) Vol',\n",
       "   'citeRegEx': '50',\n",
       "   'shortCiteRegEx': '50',\n",
       "   'year': 1989},\n",
       "  {'title': 'Fast mappers, slow learners: Word learning without hippocampus is slow and sparse irrespective of methodology. Cognitive neuroscience',\n",
       "   'author': ['DE Warren', 'MC Duff'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '51',\n",
       "   'shortCiteRegEx': '51',\n",
       "   'year': 2019},\n",
       "  {'title': 'Memory and the hippocampus: a synthesis from findings with rats, monkeys, and humans. Psychological review',\n",
       "   'author': ['LR Squire'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '52',\n",
       "   'shortCiteRegEx': '52',\n",
       "   'year': 1992},\n",
       "  {'title': 'A contextual binding theory of episodic memory: systems consolidation reconsidered',\n",
       "   'author': ['A Yonelinas', 'C Ranganath', 'A Ekstrom', 'B Wiltgen'],\n",
       "   'venue': 'Nature Reviews Neuroscience',\n",
       "   'citeRegEx': '53',\n",
       "   'shortCiteRegEx': '53',\n",
       "   'year': 2019},\n",
       "  {'title': 'Simple memory: a theory for archicortex',\n",
       "   'author': ['D Marr'],\n",
       "   'venue': 'Philosophical Transactions of the Royal Society of London B: Biological Sciences',\n",
       "   'citeRegEx': '54',\n",
       "   'shortCiteRegEx': '54',\n",
       "   'year': 1971},\n",
       "  {'title': 'Why there are complementary learning systems in the hippocampus and neocortex: Insights from the successes and failures of connectionist models of learning and memory. Psychological Review 102(3):419–457',\n",
       "   'author': ['JL McClelland', 'BL McNaughton', 'RC O’Reilly'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '55',\n",
       "   'shortCiteRegEx': '55',\n",
       "   'year': 1995},\n",
       "  {'title': 'What learning systems do intelligent agents need? Complementary learning systems theory updated. Trends in Cognitive Sciences 20(7):512–534',\n",
       "   'author': ['D Kumaran', 'D Hassabis', 'JL McClelland'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '56',\n",
       "   'shortCiteRegEx': '56',\n",
       "   'year': 2016},\n",
       "  {'title': 'Understanding natural language. Cognitive psychology 3(1):1–191',\n",
       "   'author': ['T Winograd'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '57',\n",
       "   'shortCiteRegEx': '57',\n",
       "   'year': 1972},\n",
       "  {'title': 'Understanding blind people’s experiences with computer-generated captions of social media images',\n",
       "   'author': ['H MacLeod', 'CL Bennett', 'MR Morris', 'E Cutrell'],\n",
       "   'venue': 'Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems. (ACM),',\n",
       "   'citeRegEx': '58',\n",
       "   'shortCiteRegEx': '58',\n",
       "   'year': 2017},\n",
       "  {'title': 'Learning by abstraction: The neural state machine. arXiv preprint arXiv:1907.03950',\n",
       "   'author': ['DA Hudson', 'CD Manning'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '59',\n",
       "   'shortCiteRegEx': '59',\n",
       "   'year': 2019},\n",
       "  {'title': 'Pdp models of schemata and sequential thought processes in pdp models. Parallel distributed processing: Explorations in the microstructure of cognition 2:3–57',\n",
       "   'author': ['DE Rumelhart', 'P Smolensky', 'JL McClelland', 'G Hinton'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '60',\n",
       "   'shortCiteRegEx': '60',\n",
       "   'year': 1986},\n",
       "  {'title': 'Video paragraph captioning using hierarchical recurrent neural networks in Proceedings of the IEEE conference on computer vision and pattern recognition',\n",
       "   'author': ['H Yu', 'J Wang', 'Z Huang', 'Y Yang', 'W Xu'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '61',\n",
       "   'shortCiteRegEx': '61',\n",
       "   'year': 2016},\n",
       "  {'title': 'Grounded language learning in a simulated 3d world. arXiv preprint arXiv:1706.06551',\n",
       "   'author': ['KM Hermann'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '62',\n",
       "   'shortCiteRegEx': '62',\n",
       "   'year': 2017},\n",
       "  {'title': 'A (2017) Question answering on knowledge bases and text using universal schema and memory networks in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistic',\n",
       "   'author': ['R Das', 'M Zaheer', 'S Reddy', 'McCallum'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '63',\n",
       "   'shortCiteRegEx': '63',\n",
       "   'year': 2017},\n",
       "  {'title': 'Gated-attention architectures for task-oriented language grounding',\n",
       "   'author': ['DS Chaplot',\n",
       "    'KM Sathyendra',\n",
       "    'RK Pasumarthi',\n",
       "    'D Rajagopal',\n",
       "    'R Salakhutdinov'],\n",
       "   'venue': 'arXiv preprint arXiv:1706.07230',\n",
       "   'citeRegEx': '64',\n",
       "   'shortCiteRegEx': '64',\n",
       "   'year': 2017},\n",
       "  {'title': 'Zero-shot task generalization with multi-task deep reinforcement learning',\n",
       "   'author': ['J Oh', 'S Singh', 'H Lee', 'P Kohli'],\n",
       "   'venue': 'Proceedings of the 34th International Conference on Machine LearningVolume 70. (JMLR. org),',\n",
       "   'citeRegEx': '65',\n",
       "   'shortCiteRegEx': '65',\n",
       "   'year': 2017},\n",
       "  {'title': 'Emergent systematic generalization in a situated agent',\n",
       "   'author': ['F Hill'],\n",
       "   'venue': 'arXiv preprint arXiv:1910.00571',\n",
       "   'citeRegEx': '66',\n",
       "   'shortCiteRegEx': '66',\n",
       "   'year': 2019},\n",
       "  {'title': 'Human-level concept learning through probabilistic program induction',\n",
       "   'author': ['BM Lake', 'R Salakhutdinov', 'JB Tenenbaum'],\n",
       "   'venue': 'Science',\n",
       "   'citeRegEx': '67',\n",
       "   'shortCiteRegEx': '67',\n",
       "   'year': 2015},\n",
       "  {'title': 'Hybrid computing using a neural network with dynamic external memory',\n",
       "   'author': ['A Graves'],\n",
       "   'venue': 'Nature',\n",
       "   'citeRegEx': '68',\n",
       "   'shortCiteRegEx': '68',\n",
       "   'year': 2016},\n",
       "  {'title': 'A context maintenance and retrieval model of organizational processes in free recall. Psychological review',\n",
       "   'author': ['SM Polyn', 'KA Norman', 'MJ Kahana'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '70',\n",
       "   'shortCiteRegEx': '70',\n",
       "   'year': 2009},\n",
       "  {'title': 'Stone tools, language and the brain in human evolution',\n",
       "   'author': ['D Stout', 'T Chaminade'],\n",
       "   'venue': 'Philos Trans R Soc Lond B Biol Sci 367(1585):75–87',\n",
       "   'citeRegEx': '71',\n",
       "   'shortCiteRegEx': '71',\n",
       "   'year': 2012},\n",
       "  {'title': 'Embodiment vs. memetics',\n",
       "   'author': ['J Bryson'],\n",
       "   'venue': 'Mind and Society',\n",
       "   'citeRegEx': '72',\n",
       "   'shortCiteRegEx': '72',\n",
       "   'year': 2008},\n",
       "  {'title': 'Metaphors We Live By',\n",
       "   'author': ['G Lakoff', 'M Johnson'],\n",
       "   'venue': None,\n",
       "   'citeRegEx': '73',\n",
       "   'shortCiteRegEx': '73',\n",
       "   'year': 1980},\n",
       "  {'title': 'Embodied meaning in a neural theory of language. Brain and Language 89:385–392',\n",
       "   'author': ['J Feldman', 'S Narayanan'],\n",
       "   'venue': 'McClelland et al.: Language understanding in humans and machines',\n",
       "   'citeRegEx': '74',\n",
       "   'shortCiteRegEx': '74',\n",
       "   'year': 2004}],\n",
       " 'referenceMentions': [{'referenceID': 0,\n",
       "   'context': 'More impressive still is modern machine translation (1).',\n",
       "   'startOffset': 52,\n",
       "   'endOffset': 55},\n",
       "  {'referenceID': 1,\n",
       "   'context': 'They were first articulated in the 1950s (2) and further developed in the 1980s in the Parallel Distributed Processing (PDP) framework for modeling cognition (3).',\n",
       "   'startOffset': 41,\n",
       "   'endOffset': 44},\n",
       "  {'referenceID': 2,\n",
       "   'context': 'They were first articulated in the 1950s (2) and further developed in the 1980s in the Parallel Distributed Processing (PDP) framework for modeling cognition (3).',\n",
       "   'startOffset': 158,\n",
       "   'endOffset': 161},\n",
       "  {'referenceID': 3,\n",
       "   'context': 'A central idea of this approach is that structure in language and other cognitive domains is an emergent phenomenon, captured in learned connection weights and resulting in context-sensitive, distributed representations whose characteristics reflect a gradual, input-statistics dependent, learning process (4).',\n",
       "   'startOffset': 306,\n",
       "   'endOffset': 309},\n",
       "  {'referenceID': 4,\n",
       "   'context': 'Another key principle is mutual constraint satisfaction (5).',\n",
       "   'startOffset': 56,\n",
       "   'endOffset': 59},\n",
       "  {'referenceID': 4,\n",
       "   'context': 'Rumelhart (5) sketched an interactive model of language understanding in which estimates of probabilities about all aspects of an input constrain estimates of the probability of every other aspect.',\n",
       "   'startOffset': 10,\n",
       "   'endOffset': 13},\n",
       "  {'referenceID': 5,\n",
       "   'context': 'The idea was captured in a model of context effects in perception (6).',\n",
       "   'startOffset': 66,\n",
       "   'endOffset': 69},\n",
       "  {'referenceID': 6,\n",
       "   'context': 'Later work (7, 8) linked these ideas to energy minimization in statistical physics.',\n",
       "   'startOffset': 11,\n",
       "   'endOffset': 17},\n",
       "  {'referenceID': 7,\n",
       "   'context': 'Later work (7, 8) linked these ideas to energy minimization in statistical physics.',\n",
       "   'startOffset': 11,\n",
       "   'endOffset': 17},\n",
       "  {'referenceID': 8,\n",
       "   'context': 'Elman (9) built on the principles above to demonstrate how neural models can capture key characteristics of language structure through learning, a feat once considered impossible (10).',\n",
       "   'startOffset': 6,\n",
       "   'endOffset': 9},\n",
       "  {'referenceID': 9,\n",
       "   'context': 'Elman (9) built on the principles above to demonstrate how neural models can capture key characteristics of language structure through learning, a feat once considered impossible (10).',\n",
       "   'startOffset': 179,\n",
       "   'endOffset': 183},\n",
       "  {'referenceID': 8,\n",
       "   'context': 'First, after training his network to predict the next word in simple sentences like man eats bread, dog chases cat, and girl sleeps, the network’s representations captured the syntactic distinction between nouns and verbs (9).',\n",
       "   'startOffset': 222,\n",
       "   'endOffset': 225},\n",
       "  {'referenceID': 10,\n",
       "   'context': 'Second, Elman (11) used both simple sentences like boy chases dogs and more complex ones like boy who sees girls chases dogs.',\n",
       "   'startOffset': 14,\n",
       "   'endOffset': 18},\n",
       "  {'referenceID': 8,\n",
       "   'context': '(a) Elman’s (1990) simple recurrent network and (b) his hierarchical clustering of the representations it learned, reprinted from (9).',\n",
       "   'startOffset': 130,\n",
       "   'endOffset': 133},\n",
       "  {'referenceID': 11,\n",
       "   'context': 'Elman’s network could take only a few words of context into account, reflecting a larger challenge known as the vanishing gradient problem (12).',\n",
       "   'startOffset': 139,\n",
       "   'endOffset': 143},\n",
       "  {'referenceID': 12,\n",
       "   'context': 'The development of neural network modules called Long-Short-Term-Memory (LSTM) modules (13) that partially overcame this limitation was therefore crucial, greatly increasing the contextual range of neural models.',\n",
       "   'startOffset': 87,\n",
       "   'endOffset': 91},\n",
       "  {'referenceID': 13,\n",
       "   'context': 'In another crucial development, researchers began to use pre-trained word embeddings derived from learning predictive relationships among words (14, 15).',\n",
       "   'startOffset': 144,\n",
       "   'endOffset': 152},\n",
       "  {'referenceID': 14,\n",
       "   'context': 'In another crucial development, researchers began to use pre-trained word embeddings derived from learning predictive relationships among words (14, 15).',\n",
       "   'startOffset': 144,\n",
       "   'endOffset': 152},\n",
       "  {'referenceID': 15,\n",
       "   'context': 'However, in line with the principle of mutual constraint satisfaction, humans interpret words, including ambiguous words like bank, in accordance with the context (16), rapidly assigning a contextually appropriate meaning to each word based on all other words.',\n",
       "   'startOffset': 163,\n",
       "   'endOffset': 167},\n",
       "  {'referenceID': 16,\n",
       "   'context': 'toward context sensitivity (17) recognize this limitation.',\n",
       "   'startOffset': 27,\n",
       "   'endOffset': 31},\n",
       "  {'referenceID': 19,\n",
       "   'context': 'BERT (20) is a key model in this class, illustrated in Fig.',\n",
       "   'startOffset': 5,\n",
       "   'endOffset': 9},\n",
       "  {'referenceID': 21,\n",
       "   'context': '3 as it encodes the end of the sentence John reached the bank of the river (example from (23)).',\n",
       "   'startOffset': 89,\n",
       "   'endOffset': 93},\n",
       "  {'referenceID': 22,\n",
       "   'context': 'While it is remarkable how much can be done with strictly leftto-right constraint propagation (24), bidirectionality allows neural models to implement a mutual constraint satisfaction process in which the representation of each word depends on all other words.',\n",
       "   'startOffset': 94,\n",
       "   'endOffset': 98},\n",
       "  {'referenceID': 23,\n",
       "   'context': 'BERT-based models have produced remarkable improvements on a wide range of language tasks (25, 26).',\n",
       "   'startOffset': 90,\n",
       "   'endOffset': 98},\n",
       "  {'referenceID': 24,\n",
       "   'context': 'BERT-based models have produced remarkable improvements on a wide range of language tasks (25, 26).',\n",
       "   'startOffset': 90,\n",
       "   'endOffset': 98},\n",
       "  {'referenceID': 25,\n",
       "   'context': 'The models can be pre-trained on massive text corpora, providing useful representations for subsequent tasks for which little specific training data exists (27).',\n",
       "   'startOffset': 156,\n",
       "   'endOffset': 160},\n",
       "  {'referenceID': 26,\n",
       "   'context': 'For example, Winograd Schema challenge (28) requires determining the referent of a pronoun (here it) in a sentence such as The trophy did not fit in the suitcase because it was too ___.',\n",
       "   'startOffset': 39,\n",
       "   'endOffset': 43},\n",
       "  {'referenceID': 27,\n",
       "   'context': 'The latest models achieve ever-higher scores on benchmarks including variants of the Winograd challenge (29).',\n",
       "   'startOffset': 104,\n",
       "   'endOffset': 108},\n",
       "  {'referenceID': 28,\n",
       "   'context': 'However, variants of test materials that do not fool humans continue to stymie even the best models (30), and further refinements in the models and their assessment will be required before it will be clear what such models can achieve.',\n",
       "   'startOffset': 100,\n",
       "   'endOffset': 104},\n",
       "  {'referenceID': 29,\n",
       "   'context': 'Our proposal builds on classic work in linguistics (31, 32), human cognition (33), artificial intelligence (34), and an early PDP model (35) and dovetails with an emerging perspective in cognitive neuroscience (36).',\n",
       "   'startOffset': 51,\n",
       "   'endOffset': 59},\n",
       "  {'referenceID': 30,\n",
       "   'context': 'Our proposal builds on classic work in linguistics (31, 32), human cognition (33), artificial intelligence (34), and an early PDP model (35) and dovetails with an emerging perspective in cognitive neuroscience (36).',\n",
       "   'startOffset': 51,\n",
       "   'endOffset': 59},\n",
       "  {'referenceID': 31,\n",
       "   'context': 'Our proposal builds on classic work in linguistics (31, 32), human cognition (33), artificial intelligence (34), and an early PDP model (35) and dovetails with an emerging perspective in cognitive neuroscience (36).',\n",
       "   'startOffset': 77,\n",
       "   'endOffset': 81},\n",
       "  {'referenceID': 32,\n",
       "   'context': 'Our proposal builds on classic work in linguistics (31, 32), human cognition (33), artificial intelligence (34), and an early PDP model (35) and dovetails with an emerging perspective in cognitive neuroscience (36).',\n",
       "   'startOffset': 107,\n",
       "   'endOffset': 111},\n",
       "  {'referenceID': 33,\n",
       "   'context': 'Our proposal builds on classic work in linguistics (31, 32), human cognition (33), artificial intelligence (34), and an early PDP model (35) and dovetails with an emerging perspective in cognitive neuroscience (36).',\n",
       "   'startOffset': 136,\n",
       "   'endOffset': 140},\n",
       "  {'referenceID': 34,\n",
       "   'context': 'Our proposal builds on classic work in linguistics (31, 32), human cognition (33), artificial intelligence (34), and an early PDP model (35) and dovetails with an emerging perspective in cognitive neuroscience (36).',\n",
       "   'startOffset': 210,\n",
       "   'endOffset': 214},\n",
       "  {'referenceID': 35,\n",
       "   'context': 'Words and their sequencing serve as clues to meaning (37) that jointly constrain the understanding of the situation and each object participating in it (35).',\n",
       "   'startOffset': 53,\n",
       "   'endOffset': 57},\n",
       "  {'referenceID': 33,\n",
       "   'context': 'Words and their sequencing serve as clues to meaning (37) that jointly constrain the understanding of the situation and each object participating in it (35).',\n",
       "   'startOffset': 152,\n",
       "   'endOffset': 156},\n",
       "  {'referenceID': 31,\n",
       "   'context': 'Evidence that humans construct situation representations comes from classic work by Bransford and colleagues (33, 38).',\n",
       "   'startOffset': 109,\n",
       "   'endOffset': 117},\n",
       "  {'referenceID': 36,\n",
       "   'context': 'Evidence that humans construct situation representations comes from classic work by Bransford and colleagues (33, 38).',\n",
       "   'startOffset': 109,\n",
       "   'endOffset': 117},\n",
       "  {'referenceID': 0,\n",
       "   'context': 'This work demonstrates that (1) we understand and remember texts better when we can relate the statements in the text to a familiar situation; (2) information that conveys aspects of the situation can be provided by a picture accompanying the text; (3) the characteristics of the objects we remember depend on the situations in which they occurred in a text; (4) we represent in memory objects not explicitly mentioned in texts; and (5) after hearing a sentence describing spatial or conceptual relationships among objects, we retain memory for these relationships rather than the linguistic input.',\n",
       "   'startOffset': 28,\n",
       "   'endOffset': 31},\n",
       "  {'referenceID': 1,\n",
       "   'context': 'This work demonstrates that (1) we understand and remember texts better when we can relate the statements in the text to a familiar situation; (2) information that conveys aspects of the situation can be provided by a picture accompanying the text; (3) the characteristics of the objects we remember depend on the situations in which they occurred in a text; (4) we represent in memory objects not explicitly mentioned in texts; and (5) after hearing a sentence describing spatial or conceptual relationships among objects, we retain memory for these relationships rather than the linguistic input.',\n",
       "   'startOffset': 143,\n",
       "   'endOffset': 146},\n",
       "  {'referenceID': 2,\n",
       "   'context': 'This work demonstrates that (1) we understand and remember texts better when we can relate the statements in the text to a familiar situation; (2) information that conveys aspects of the situation can be provided by a picture accompanying the text; (3) the characteristics of the objects we remember depend on the situations in which they occurred in a text; (4) we represent in memory objects not explicitly mentioned in texts; and (5) after hearing a sentence describing spatial or conceptual relationships among objects, we retain memory for these relationships rather than the linguistic input.',\n",
       "   'startOffset': 249,\n",
       "   'endOffset': 252},\n",
       "  {'referenceID': 3,\n",
       "   'context': 'This work demonstrates that (1) we understand and remember texts better when we can relate the statements in the text to a familiar situation; (2) information that conveys aspects of the situation can be provided by a picture accompanying the text; (3) the characteristics of the objects we remember depend on the situations in which they occurred in a text; (4) we represent in memory objects not explicitly mentioned in texts; and (5) after hearing a sentence describing spatial or conceptual relationships among objects, we retain memory for these relationships rather than the linguistic input.',\n",
       "   'startOffset': 359,\n",
       "   'endOffset': 362},\n",
       "  {'referenceID': 4,\n",
       "   'context': 'This work demonstrates that (1) we understand and remember texts better when we can relate the statements in the text to a familiar situation; (2) information that conveys aspects of the situation can be provided by a picture accompanying the text; (3) the characteristics of the objects we remember depend on the situations in which they occurred in a text; (4) we represent in memory objects not explicitly mentioned in texts; and (5) after hearing a sentence describing spatial or conceptual relationships among objects, we retain memory for these relationships rather than the linguistic input.',\n",
       "   'startOffset': 433,\n",
       "   'endOffset': 436},\n",
       "  {'referenceID': 37,\n",
       "   'context': 'Further, evidence from eye movements shows that people use linguistic and non-linguistic input jointly and immediately as they process language in context (39).',\n",
       "   'startOffset': 155,\n",
       "   'endOffset': 159},\n",
       "  {'referenceID': 38,\n",
       "   'context': 'participants look at a full wine glass rather than an empty beer glass (40).',\n",
       "   'startOffset': 71,\n",
       "   'endOffset': 75},\n",
       "  {'referenceID': 34,\n",
       "   'context': 'It is largely consistent with proposals in (36).',\n",
       "   'startOffset': 43,\n",
       "   'endOffset': 47},\n",
       "  {'referenceID': 39,\n",
       "   'context': 'Brain regions for representing visual and linguistic inputs are well-established, and the evidence for their involvement in a mutual constraint satisfaction process is substantial (41).',\n",
       "   'startOffset': 180,\n",
       "   'endOffset': 184},\n",
       "  {'referenceID': 40,\n",
       "   'context': 'A brain area near the front of the temporal lobe houses neurons whose activity provides an embedding capturing the properties of an object someone is considering (42).',\n",
       "   'startOffset': 162,\n",
       "   'endOffset': 166},\n",
       "  {'referenceID': 41,\n",
       "   'context': 'Models that capture these findings (43) treat this area as the hidden layer of an interactive, recurrent network with bidirectional connections to other layers corresponding to brain areas that represent different types of object properties including the object’s name.',\n",
       "   'startOffset': 35,\n",
       "   'endOffset': 39},\n",
       "  {'referenceID': 42,\n",
       "   'context': 'Evidence from behavioral studies indicates that construction of a situation representation can occur with or without language input (44) or through the convergent influence of both sources of information (40).',\n",
       "   'startOffset': 132,\n",
       "   'endOffset': 136},\n",
       "  {'referenceID': 38,\n",
       "   'context': 'Evidence from behavioral studies indicates that construction of a situation representation can occur with or without language input (44) or through the convergent influence of both sources of information (40).',\n",
       "   'startOffset': 204,\n",
       "   'endOffset': 208},\n",
       "  {'referenceID': 34,\n",
       "   'context': 'Cognitive neuroscience research supports the idea that the situation representation arises in a set of interconnected brain areas primarily located in the frontal and parietal lobes (36, 45).',\n",
       "   'startOffset': 182,\n",
       "   'endOffset': 190},\n",
       "  {'referenceID': 43,\n",
       "   'context': 'Cognitive neuroscience research supports the idea that the situation representation arises in a set of interconnected brain areas primarily located in the frontal and parietal lobes (36, 45).',\n",
       "   'startOffset': 182,\n",
       "   'endOffset': 190},\n",
       "  {'referenceID': 44,\n",
       "   'context': 'represent corresponding events in a sequence are largely the same, whether the information about the sequence comes from watching a movie, hearing or reading a narrative description, or recalling the movie after having seen it (46, 47).',\n",
       "   'startOffset': 227,\n",
       "   'endOffset': 235},\n",
       "  {'referenceID': 45,\n",
       "   'context': 'represent corresponding events in a sequence are largely the same, whether the information about the sequence comes from watching a movie, hearing or reading a narrative description, or recalling the movie after having seen it (46, 47).',\n",
       "   'startOffset': 227,\n",
       "   'endOffset': 235},\n",
       "  {'referenceID': 36,\n",
       "   'context': 'For example, the dogs in the events described by the sentences the boy ran to the dog and the boy ran from the dog are likely to be different, and a comprehender will represent them differently (38).',\n",
       "   'startOffset': 194,\n",
       "   'endOffset': 198},\n",
       "  {'referenceID': 46,\n",
       "   'context': 'In general, context-sensitivity is best captured by a mediating representation rather than direct associations among constituents (48).',\n",
       "   'startOffset': 130,\n",
       "   'endOffset': 134},\n",
       "  {'referenceID': 45,\n",
       "   'context': 'Contemporary attention-based language models can deploy attention over tens to thousands of words kept in their current system state, but evidence from brain imaging data collected during movie comprehension suggests that activation states in visual, speech, and object areas change rapidly as events unfold, while the brain state tends to be more constant, changing only at event boundaries in brain areas associated with situation representations (47).',\n",
       "   'startOffset': 449,\n",
       "   'endOffset': 453},\n",
       "  {'referenceID': 47,\n",
       "   'context': 'The connection weights gradually become sensitive to subtle higher-order statistical relationships, taking more and more context into account as learning continues (49), and exhibiting sensitivity both to general and recurring specific information (e.',\n",
       "   'startOffset': 164,\n",
       "   'endOffset': 168},\n",
       "  {'referenceID': 48,\n",
       "   'context': 'However, this learning mechanism is not well suited to acquiring new information rapidly, and attempting to learn specific new information quickly by focused repetition leads to catastrophic interference with what is already known (50).',\n",
       "   'startOffset': 231,\n",
       "   'endOffset': 235},\n",
       "  {'referenceID': 49,\n",
       "   'context': 'Humans show robust learning after just two brief exposures to such pairings (51).',\n",
       "   'startOffset': 76,\n",
       "   'endOffset': 80},\n",
       "  {'referenceID': 49,\n",
       "   'context': 'This form of learning depends on the hippocampus and adjacent areas in the medial temporal lobes (MTL) of the brain (51).',\n",
       "   'startOffset': 116,\n",
       "   'endOffset': 120},\n",
       "  {'referenceID': 50,\n",
       "   'context': 'While details of the role of the MTL in learning and memory continue to be debated (52, 53), there is consensus that the MTL is crucial for the initial formation of new memories, including memories for specific events and their constituent objects and situations, while general knowledge, the ability to understand language, and previously acquired skills are unaffected by MTL damage.',\n",
       "   'startOffset': 83,\n",
       "   'endOffset': 91},\n",
       "  {'referenceID': 51,\n",
       "   'context': 'While details of the role of the MTL in learning and memory continue to be debated (52, 53), there is consensus that the MTL is crucial for the initial formation of new memories, including memories for specific events and their constituent objects and situations, while general knowledge, the ability to understand language, and previously acquired skills are unaffected by MTL damage.',\n",
       "   'startOffset': 83,\n",
       "   'endOffset': 91},\n",
       "  {'referenceID': 54,\n",
       "   'context': '4B) from an experience seeing it and hearing a sentence about it (56).',\n",
       "   'startOffset': 65,\n",
       "   'endOffset': 69},\n",
       "  {'referenceID': 52,\n",
       "   'context': 'Networks within the MTL (not shown) map the MTL input representation to a sparser one deep inside the MTL, maximizing distinctness and minimizing interference among experiences (54).',\n",
       "   'startOffset': 177,\n",
       "   'endOffset': 181},\n",
       "  {'referenceID': 53,\n",
       "   'context': 'How can knowledge initially dependent on the MTL be integrated into the neocortex? According to CLST (55), the neocortex learns gradually through interleaved presentations of new and familiar items; this process avoids interference of new items with what is already known.',\n",
       "   'startOffset': 101,\n",
       "   'endOffset': 105},\n",
       "  {'referenceID': 54,\n",
       "   'context': 'Interleaving may also occur during rest or sleep through reactivation and replay of patterns stored in the MTL: Indeed, spontaneous replay of short snippets of previously experienced episodes occurs within the MTL during sleep and between behavioral episodes (see (56) for review).',\n",
       "   'startOffset': 264,\n",
       "   'endOffset': 268},\n",
       "  {'referenceID': 55,\n",
       "   'context': 'An early example is Winograd’s SHRDLU system (57), which produced and responded to language about a simulated physical world.',\n",
       "   'startOffset': 45,\n",
       "   'endOffset': 49},\n",
       "  {'referenceID': 56,\n",
       "   'context': 'When presented with a photograph, networks can now answer questions such as what is the man holding? or what color is the woman’s shirt? (58), demonstrating an ability to combine information from vision and language to understand a class of situations.',\n",
       "   'startOffset': 137,\n",
       "   'endOffset': 141},\n",
       "  {'referenceID': 57,\n",
       "   'context': 'A very recent model (59) explicitly represents the objects in a scene, their properties, and their relations to other objects in a designed scene graph with slots for objects, their properties, and relations.',\n",
       "   'startOffset': 20,\n",
       "   'endOffset': 24},\n",
       "  {'referenceID': 57,\n",
       "   'context': 'This is advocated in (59), but natural structure exhibits flexible embedding relationships and is often only approximately characterized by explicit taxonomies, motivating use of emergent connection-based rather than hard-coded representational structures (60).',\n",
       "   'startOffset': 21,\n",
       "   'endOffset': 25},\n",
       "  {'referenceID': 58,\n",
       "   'context': 'This is advocated in (59), but natural structure exhibits flexible embedding relationships and is often only approximately characterized by explicit taxonomies, motivating use of emergent connection-based rather than hard-coded representational structures (60).',\n",
       "   'startOffset': 256,\n",
       "   'endOffset': 260},\n",
       "  {'referenceID': 59,\n",
       "   'context': 'While models that jointly process video and language (61) may acquire some sensitivity to event structure and commonplace causal relationships, these systems do not make choices affecting the world they observe.',\n",
       "   'startOffset': 53,\n",
       "   'endOffset': 57},\n",
       "  {'referenceID': 64,\n",
       "   'context': 'In (66), an agent was trained to identify, lift, carry and place objects relative to other objects in a virtual room, as specified by simplified language instructions.',\n",
       "   'startOffset': 3,\n",
       "   'endOffset': 7},\n",
       "  {'referenceID': 65,\n",
       "   'context': 'Neural models often fail to exhibit systematic generalization, leading some to propose that more structure should be built in (67).',\n",
       "   'startOffset': 126,\n",
       "   'endOffset': 130},\n",
       "  {'referenceID': 66,\n",
       "   'context': 'What might a fast learning system in an implementation of an integrated understanding system look like? The memory system in the differentiable neural computer (DNC) (68) is one possibility.',\n",
       "   'startOffset': 166,\n",
       "   'endOffset': 170},\n",
       "  {'referenceID': 54,\n",
       "   'context': 'Though we do not believe the brain has a separate slot for each memory, it can be useful to model it as though it does (56), and artificial systems with indefinite capacity could exceed human abilities in this regard.',\n",
       "   'startOffset': 119,\n",
       "   'endOffset': 123},\n",
       "  {'referenceID': 67,\n",
       "   'context': 'Retrieval could be based on a combination of context and item information, similar to human memory (70).',\n",
       "   'startOffset': 99,\n",
       "   'endOffset': 103},\n",
       "  {'referenceID': 68,\n",
       "   'context': 'What is the way forward toward developing models that can understand such a complex situation? Language may have evolved in part to support transmission of complex, hierarchical knowledge about tools (71).',\n",
       "   'startOffset': 200,\n",
       "   'endOffset': 204},\n",
       "  {'referenceID': 69,\n",
       "   'context': 'Words themselves provided a new abstract substrate for characterizing other words (72).',\n",
       "   'startOffset': 82,\n",
       "   'endOffset': 86},\n",
       "  {'referenceID': 69,\n",
       "   'context': 'For example, career is not only linked to other abstract words like work and specialization but also to more grounded concepts such as path and its extended metaphorical use for discussing the means to achieve goals (72).',\n",
       "   'startOffset': 216,\n",
       "   'endOffset': 220},\n",
       "  {'referenceID': 70,\n",
       "   'context': 'Embodied, simulation-based approaches to meaning (73, 74) build on this observation to bridge from concrete to abstract situations via metaphor.',\n",
       "   'startOffset': 49,\n",
       "   'endOffset': 57},\n",
       "  {'referenceID': 71,\n",
       "   'context': 'Embodied, simulation-based approaches to meaning (73, 74) build on this observation to bridge from concrete to abstract situations via metaphor.',\n",
       "   'startOffset': 49,\n",
       "   'endOffset': 57},\n",
       "  {'referenceID': 7,\n",
       "   'context': 'Proceedings of the national academy of sciences 79(8):2554–2558.',\n",
       "   'startOffset': 50,\n",
       "   'endOffset': 53},\n",
       "  {'referenceID': 4,\n",
       "   'context': 'Information and control 10(5):447–474.',\n",
       "   'startOffset': 26,\n",
       "   'endOffset': 29},\n",
       "  {'referenceID': 0,\n",
       "   'context': 'Journal of verbal learning and verbal behavior 20(1):120–136.',\n",
       "   'startOffset': 49,\n",
       "   'endOffset': 52},\n",
       "  {'referenceID': 0,\n",
       "   'context': 'Artificial Intelligence 46(1):217 – 257.',\n",
       "   'startOffset': 26,\n",
       "   'endOffset': 29},\n",
       "  {'referenceID': 3,\n",
       "   'context': 'Journal of Verbal Learning and Verbal Behavior 13(4):471 – 481.',\n",
       "   'startOffset': 49,\n",
       "   'endOffset': 52},\n",
       "  {'referenceID': 3,\n",
       "   'context': 'Journal of Memory and Language 57(4):502–518.',\n",
       "   'startOffset': 33,\n",
       "   'endOffset': 36},\n",
       "  {'referenceID': 1,\n",
       "   'context': 'Psychological bulletin 123(2):162–185.',\n",
       "   'startOffset': 26,\n",
       "   'endOffset': 29},\n",
       "  {'referenceID': 2,\n",
       "   'context': 'Journal of Experimental Psychology: General 120(3):235.',\n",
       "   'startOffset': 47,\n",
       "   'endOffset': 50},\n",
       "  {'referenceID': 1,\n",
       "   'context': 'Behavior Research Methods, Instruments, & Computers 16(2):96–101.',\n",
       "   'startOffset': 54,\n",
       "   'endOffset': 57}],\n",
       " 'year': 2019,\n",
       " 'abstractText': 'Many of the most impressive recent successes of machine intelligence have appeared in the domain of language. Machines can now better identify the words we speak and respond in ever more natural sounding voices. More impressive still is modern machine translation (1). Anyone with a smartphone has access to applications that allow them to say a sentence in one language and then see and hear its translation in another. Human ability still far exceeds machines in most language tasks, but these systems work well enough to be used by billions of people everyday. What underlies these successes, and what limitations do these systems face? We argue that successes to date come from ever more effective methods for exploiting principles of neural computation that human language users also exploit. We then note that the work remains limited in that it largely treats language separately from the larger task of understanding the world around us. This leads us to propose an integrated approach to building a system that truly understands, in which language plays a key role in concert with other sources of input. We discuss the challenges facing further development of the approach and propose future steps toward addressing these challenges.',\n",
       " 'creator': 'LaTeX with hyperref package'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_data.keys()\n",
    "doc_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:29:23.556177Z",
     "start_time": "2020-05-01T14:29:23.548975Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Extending Machine Language Models toward Human-Level Language Understanding',\n",
       " 'Language Understanding | Natural Language Processing | Situation',\n",
       " 'Models | Machine Language Models | Brain System for Understanding',\n",
       " 'Principles of Neural Computation',\n",
       " 'Neural Language Modeling',\n",
       " 'Scaling Up to Process Natural Text. Elman’s task—predicting',\n",
       " 'The Human Integrated Understanding System (IUS)',\n",
       " 'Toward an Artificial Integrated Understanding System']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[item['heading'] for item in doc_data['sections']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:15:58.252158Z",
     "start_time": "2020-05-01T14:15:58.243593Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': '2016) Google’s neural machine translation system: Bridging the gap between human and machine',\n",
       " 'author': ['Y Wu'],\n",
       " 'venue': None,\n",
       " 'citeRegEx': '1',\n",
       " 'shortCiteRegEx': '1',\n",
       " 'year': 2016}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'referenceID': 0,\n",
       " 'context': 'More impressive still is modern machine translation (1).',\n",
       " 'startOffset': 52,\n",
       " 'endOffset': 55}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_data['references'][0]\n",
    "\n",
    "doc_data['referenceMentions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:19:50.038907Z",
     "start_time": "2020-05-01T14:19:50.031339Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'referenceID': 0,\n",
       "  'context': 'More impressive still is modern machine translation (1).',\n",
       "  'startOffset': 52,\n",
       "  'endOffset': 55},\n",
       " {'referenceID': 0,\n",
       "  'context': 'This work demonstrates that (1) we understand and remember texts better when we can relate the statements in the text to a familiar situation; (2) information that conveys aspects of the situation can be provided by a picture accompanying the text; (3) the characteristics of the objects we remember depend on the situations in which they occurred in a text; (4) we represent in memory objects not explicitly mentioned in texts; and (5) after hearing a sentence describing spatial or conceptual relationships among objects, we retain memory for these relationships rather than the linguistic input.',\n",
       "  'startOffset': 28,\n",
       "  'endOffset': 31},\n",
       " {'referenceID': 0,\n",
       "  'context': 'Journal of verbal learning and verbal behavior 20(1):120–136.',\n",
       "  'startOffset': 49,\n",
       "  'endOffset': 52},\n",
       " {'referenceID': 0,\n",
       "  'context': 'Artificial Intelligence 46(1):217 – 257.',\n",
       "  'startOffset': 26,\n",
       "  'endOffset': 29}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# referenceMentions have IDs but these don't seem to correspond to the number used in the text\n",
    "[item for item in doc_data['referenceMentions'] if item['referenceID'] == 0 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:27:52.544215Z",
     "start_time": "2020-05-01T14:27:52.537752Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['source', 'title', 'authors', 'emails', 'sections', 'references', 'referenceMentions', 'year', 'abstractText', 'creator'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bulk processing\n",
    "\n",
    "```bash\n",
    "time java -jar cli/target/scala-2.12/science-parse-cli-assembly-3.0.1.jar ../../sample_pdfs/ -o ../parses/ -f sample-pdfs.jsonl\n",
    "```\n",
    "(3 min for 36 papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:45:46.267486Z",
     "start_time": "2020-05-01T14:45:46.229328Z"
    }
   },
   "outputs": [],
   "source": [
    "import os, json\n",
    "\n",
    "DIR = 'science-parse-test/parses'\n",
    "\n",
    "articles = {}\n",
    "for file in os.listdir(DIR):\n",
    "    with open(os.path.join(DIR, file)) as f:\n",
    "        articles[file] = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T14:49:54.219809Z",
     "start_time": "2020-05-01T14:49:54.147840Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019.pdf.json\n",
      "\tExtending Machine Language Models toward Human-Level Language Understanding\n",
      "\tLanguage Understanding | Natural Language Processing | Situation\n",
      "\tModels | Machine Language Models | Brain System for Understanding\n",
      "\tPrinciples of Neural Computation\n",
      "\tNeural Language Modeling\n",
      "\tScaling Up to Process Natural Text. Elman’s task—predicting\n",
      "\tThe Human Integrated Understanding System (IUS)\n",
      "\tToward an Artificial Integrated Understanding System\n",
      "\n",
      "Multi agent cooperation and emergence of natujral langauge.pdf.json\n",
      "\t1 INTRODUCTION\n",
      "\t2 GENERAL FRAMEWORK\n",
      "\t3 EXPERIMENTAL SETUP\n",
      "\t4 LEARNING TO COMMUNICATE\n",
      "\t4.1 OBJECT-LEVEL REFERENCE\n",
      "\t5 GROUNDING AGENTS’ COMMUNICATION IN HUMAN LANGUAGE\n",
      "\t6 DISCUSSION\n",
      "\tACKNOWLEDGMENTS\n",
      "\n",
      "savage-rumbaugh1980.pdf.json\n",
      "\n",
      "Exploiting Deep Semantics and Compositionality of Natural Language.pdf.json\n",
      "\t\n",
      "\tII. PRELIMINARIES AND RELATED WORK\n",
      "\tA. What Makes Natural Language Understanding Hard\n",
      "\tB. Embodied Construction Grammar and Compositionality\n",
      "\tC. A Brief Survey on NLU for Robotics\n",
      "\tIII. SYSTEM OVERVIEW\n",
      "\tA. The ECG semantic analysis framework\n",
      "\tIV. PROOF OF CONCEPT AND EVALUATION\n",
      "\tA. Proof of Concept Scenarios\n",
      "\tB. Comparison of Features of NLU-Systems for HRI\n",
      "\tV. CONCLUSION\n",
      "\n",
      "Enactive AI Froese & Ziemke.pdf.json\n",
      "\t\n",
      "\t1. Introduction – setting the scene\n",
      "\t2. Embodied AI and beyond\n",
      "\t2.1. Foundations of embodied AI\n",
      "\t2.2. The “failure” of embodied AI?\n",
      "\t2.3. The problem of meaning in embodied AI\n",
      "\t2.4. The problem of agency in embodied AI\n",
      "\t2.5. From embodied AI to enactive AI\n",
      "\t2.6. Enactive cognitive science\n",
      "\t3. Biological foundations of enactive AI\n",
      "\t3.1. A view from philosophical biology\n",
      "\t3.1.1. Kant and the notion of ‘natural purpose’\n",
      "\t3.1.2. Von Uexküll and the notion of ‘Umwelt’\n",
      "\t3.1.3. Jonas and the notion of ‘needful freedom’\n",
      "\t3.1.4. Philosophical insights for embodied AI\n",
      "\t3.2. The enactive approach to intentional agency\n",
      "\t3.2.1. Constitutive autonomy is necessary for intrinsic teleology\n",
      "\t3.2.2. Adaptivity is necessary for sense-making\n",
      "\t3.2.3. Constitutive autonomy is necessary for sense-making\n",
      "\t3.3. From life to mind: the enactive framework\n",
      "\t4. Design principles for enactive AI\n",
      "\t4.1. Toward enactive AI (i): constitutive autonomy\n",
      "\t4.2. Toward enactive AI (ii): adaptivity\n",
      "\t4.3. What is ‘enactive AI’?\n",
      "\t4.4. A typography of AI systems\n",
      "\t4.4.1. The typography of Type I systems\n",
      "\t4.4.2. The typography of Type II systems\n",
      "\t5. Conclusion\n",
      "\tAcknowledgements\n",
      "\tAppendix A\n",
      "\tAppendix B\n",
      "\tAppendix C\n",
      "\n",
      "Emergence of Compositional Language with Deep Generational Transmission.pdf.json\n",
      "\t1. Introduction\n",
      "\t2. Approach\n",
      "\t2.1. Goal-Driven Neural Dialog\n",
      "\t2.2. Language Emergence with Cultural Transmission\n",
      "\t3. Experiments\n",
      "\t3.1. Neural Question Answering Details\n",
      "\t3.2. Impact of Cultural Transmission\n",
      "\t4. Cultural Transmission Analysis\n",
      "\t5. Related work\n",
      "\t6. Conclusion\n",
      "\tAcknowledgements\n",
      "\tA. Replacement Strategies\n",
      "\tB. Detailed Results\n",
      "\n",
      "Microblogging_during_two_natural_hazards_events_Wh.pdf.json\n",
      "\t\n",
      "\tAuthor Keywords\n",
      "\tACM Classification Keywords\n",
      "\tGeneral Terms\n",
      "\tINTRODUCTION\n",
      "\tSituational Awareness in Safety-Critical Situations\n",
      "\tSocial Media & Emergencies\n",
      "\tTHE STUDY\n",
      "\tThe Study Events\n",
      "\tRed River Floods, Spring 2009\n",
      "\tOklahoma Grassfires Spring 2009\n",
      "\tMethod & Data\n",
      "\tData Collection Steps\n",
      "\tQualitative Data Coding\n",
      "\tDATA DESCRIPTION\n",
      "\tFurther Analysis of Geo-Location Information\n",
      "\tRelative References to Location: Location-Referencing\n",
      "\tSituational Updates\n",
      "\tDifferences in Situational Updates\n",
      "\tAdditional Characteristics of Tweeted Information\n",
      "\tHigh Yield Twitterers\n",
      "\tMarkedness\n",
      "\tDISCUSSION\n",
      "\tMicroblog-Enhanced Situational Features\n",
      "\tCONCLUSION\n",
      "\tACKNOWLEDGMENTS\n",
      "\n",
      "supervision and self-play in emergence of comunication.pdf.json\n",
      "\t1 INTRODUCTION\n",
      "\t2 RELATED WORK\n",
      "\t3 METHODS\n",
      "\t3.1 PROBLEM DEFINITION\n",
      "\t3.2 SUPERVISED SELF-PLAY (S2P)\n",
      "\t3.3 ALGORITHMS FOR S2P\n",
      "\t4 ENVIRONMENTS & IMPLEMENTATION DETAILS\n",
      "\t5 DO SUPERVISED LEARNING BEFORE SELF-PLAY\n",
      "\t6.2 EXAMINING S2P SCHEDULES\n",
      "\t7 DISCUSSION\n",
      "\tACKNOWLEDGEMENTS\n",
      "\tA HYPERPARAMETERS\n",
      "\tB CALCULATION OF OPTIMAL SAMPLE COMPLEXITY IN OR GAME\n",
      "\tC ADDITIONAL PLOTS\n",
      "\n",
      "Faking Sandy: Characterizing and Identifying Fake Images.pdf.json\n",
      "\t\n",
      "\t1. INTRODUCTION\n",
      "\t2. RELATED WORK\n",
      "\t2.1 Role of OSM during Real World Events\n",
      "\t2.2 Assessing Quality of Information on OSM\n",
      "\t3. METHODOLOGY\n",
      "\t3.1 Data\n",
      "\t3.2 Characterization Analysis\n",
      "\t3.3 Classification Analysis\n",
      "\t4. RESULTS\n",
      "\t4.1 Characterization Results\n",
      "\t4.2 Classification Results\n",
      "\t5. DISCUSSION\n",
      "\t6. FUTURE WORK\n",
      "\t7. ACKNOWLEDGMENTS\n",
      "\t8. REFERENCES\n",
      "\n",
      "Compositionality_Flexibility_and_Context-Dependence5REV7.pdf.json\n",
      "\t\n",
      "\tStanding meaning of ‘big mouse’\n",
      "\n",
      "hanna2003.pdf.json\n",
      "\t\n",
      "\tMethod\n",
      "\tParticipants\n",
      "\tMaterials and design\n",
      "\tProcedure\n",
      "\tResults\n",
      "\tLatency of target choice\n",
      "\tDiscussion\n",
      "\tMethod\n",
      "\tParticipants\n",
      "\tMaterials and design\n",
      "\tProcedure\n",
      "\tResults\n",
      "\tDiscussion\n",
      "\tGeneral discussion\n",
      "\n",
      "Natural Language Does Not Emerge ‘Naturally’ in Multi-Agent Dialog.pdf.json\n",
      "\t\n",
      "\t1 Introduction\n",
      "\t3 Modeling Q-BOT and A-BOT\n",
      "\t4 The Road to Compositionality\n",
      "\t4.1 Overcomplete Vocabularies\n",
      "\t4.2 Attribute-Value Vocabulary\n",
      "\t4.3 Memoryless A-BOT, Minimal Vocabulary\n",
      "\t5 Evolution of Language\n",
      "\t5.1 Dialog Trees\n",
      "\t5.2 Evolution Timeline\n",
      "\t6 Conclusion\n",
      "\n",
      "ghosh1999.pdf.json\n",
      "\t\n",
      "\n",
      "Reference-Aware Language Models.pdf.json\n",
      "\t1 Introduction\n",
      "\t2 Reference-aware language models\n",
      "\t2.1 Reference to lists\n",
      "\t2.2 Reference to databases\n",
      "\t2.2.1 Incorporating Table Reference\n",
      "\t2.3 Reference to document context\n",
      "\t3 Experiments\n",
      "\t3.1 Data sets and preprocessing\n",
      "\t3.2 Baselines, model training and evaluation\n",
      "\t3.3 Results and analysis\n",
      "\t4 Related Work\n",
      "\t5 Conclusion\n",
      "\n",
      "cowley2016.pdf.json\n",
      "\t\n",
      "\t1. Introduction\n",
      "\t2. Linguistic embodiment\n",
      "\t2.1. Local coordination and synergies\n",
      "\t2.2. Non-local and virtual patterns\n",
      "\t3. Biological engagement with social norms\n",
      "\t3.1. Becoming a person: from synergies to stance-taking\n",
      "\t3.2. The ecological functions of wordings\n",
      "\t4. Conclusion: the illusion of common ground\n",
      "\tAcknowledgments\n",
      "\n",
      "Emergence of linguistic conventions in multi-agent reinforcement.pdf.json\n",
      "\t\n",
      "\tIntroduction\n",
      "\tMethods\n",
      "\tReinforcement learning via urn model\n",
      "\tSingle-object version\n",
      "\tMulti-object version\n",
      "\tPopulation renewal\n",
      "\tResults\n",
      "\tSingle-object version\n",
      "\tMulti-object version\n",
      "\tPopulation renewal\n",
      "\tDiscussion and conclusions\n",
      "\tSingle-Object\n",
      "\tMulti-Object\n",
      "\n",
      "XiaAAMAS13.pdf.json\n",
      "\t\n",
      "\t1. INTRODUCTION\n",
      "\t2. GOAL 1: REACHING COMPROMISE\n",
      "\t2.1 Choosing Axioms\n",
      "\t2.2 Challenge: Incorporating Axiomatic Properties to Machine Learning Framework\n",
      "\t3. GOAL 2: REVEALING GROUND TRUTH\n",
      "\t3.1 Condorcet’s probabilistic model\n",
      "\t3.2 The Random Utility Models\n",
      "\t3.3 Challenge: Model Fitting\n",
      "\t4. CONCLUSION\n",
      "\t5. ACKNOWLEDGMENTS\n",
      "\t6. REFERENCES\n",
      "\n",
      "A_vision_for_technology-mediated_support_for_publi.pdf.json\n",
      "\t\n",
      "\t1. INTRODUCTION: A NEW VIEW OF DISASTER TECHNOLOGY\n",
      "\tMultidisciplinary Orientation\n",
      "\tResearch on Citizen Activity\n",
      "\t3. A RESEARCH PROGRAM\n",
      "\tCONCLUSION\n",
      "\tACKNOWLEDGEMENTS\n",
      "\n",
      "2019, ENVIRONMENTAL DRIVERS OF SYSTEMATICITY AND GENERALIZATION IN A SITUATED AGENT.pdf.pdf.json\n",
      "\t1 INTRODUCTION\n",
      "\t1.1 SYSTEMATICITY AND GENERALISATION\n",
      "\t2 A MINIMAL MULTI-MODAL AGENT\n",
      "\t3 DEMONSTRATING GENERALISATION\n",
      "\t3.1 A GENERAL NOTION OF LIFTING\n",
      "\t3.2 A GENERAL NOTION OF PUTTING\n",
      "\t4 UNDERSTANDING THE DRIVERS OF GENERALISATION\n",
      "\t4.1 NUMBER OF TRAINING INSTRUCTIONS\n",
      "\t4.2 3D VERSUS 2D ENVIRONMENT\n",
      "\t4.3 VISUAL INVARIANCES IN AGENTS’ PERSPECTIVES\n",
      "\t4.4 TEMPORAL ASPECT OF PERCEPTION\n",
      "\t4.5 THE ROLE OF LANGUAGE\n",
      "\t5 DISCUSSION\n",
      "\tA LIMITATIONS\n",
      "\tB EXPERIMENT DETAILS\n",
      "\tC AGENT DETAILS\n",
      "\tD SOME CONTROL EXPERIMENTS FOR THE 2D VS. 3D PUTTING TASK\n",
      "\n",
      "10.1.1.28.8702.pdf.json\n",
      "\t\n",
      "\t1. INTRODUCTION\n",
      "\t1.1 Motivation\n",
      "\t1.2 Challenges\n",
      "\t1.3 Our results\n",
      "\t1.4 Organization\n",
      "\t2. PRELIMINARIES\n",
      "\t2.1 Ranking\n",
      "\t2.1.1 Distance measures\n",
      "\t2.1.2 Optimal rank aggregation\n",
      "\t2.2 Basic notions\n",
      "\t2.2.1 Some concepts from graph theory\n",
      "\t2.2.2 Markov chains\n",
      "\t3. SPAM RESISTANCE AND CONDORCET CRITERIA\n",
      "\t3.1 Local Kemenization\n",
      "\t4. RANK AGGREGATION METHODS\n",
      "\t4.1 Borda’s method\n",
      "\t4.2 Footrule and scaled footrule\n",
      "\t4.3 Markov chain methods\n",
      "\t5. APPLICATIONS\n",
      "\t5.1 Meta-search\n",
      "\t5.2 Aggregating ranking functions\n",
      "\t5.3 Spam reduction\n",
      "\t5.4 Word association techniques\n",
      "\t5.5 Search engine comparison\n",
      "\t6. EXPERIMENTS AND RESULTS\n",
      "\t6.1 Infrastructure\n",
      "\t6.2 Results\n",
      "\t6.2.1 Meta-search\n",
      "\t6.2.2 Spam reduction\n",
      "\t6.2.3 Word associations\n",
      "\t6.3 Discussion\n",
      "\t7. CONCLUSIONS AND FURTHER WORK\n",
      "\t8. REFERENCES\n",
      "\n",
      "2000,Beer.pdf.json\n",
      "\t\n",
      "\n",
      "Korbak.pdf.json\n",
      "\t1 Introduction\n",
      "\t2 Related work\n",
      "\t3 Method\n",
      "\t4 Experiments and results\n",
      "\t5 Conclusions\n",
      "\tAcknowledgments\n",
      "\tA Dataset\n",
      "\tB Agent architecture\n",
      "\tD Loss derivation\n",
      "\n",
      "Humanistika 2-G Grzegorczyk kor.07.pdf.json\n",
      "\t\n",
      "\tAbstrakt\n",
      "\tSłowa kluczowe: język, dialogiczność, interakcyjność, współdziałanie, perspektywa rozproszona, ucieleśnienie\n",
      "\tDispelling the Language Myth\n",
      "\tFrom Constructivism to Biology: Humberto Maturana's Contribution\n",
      "\tDiscarding the Code-view of Language in the Bioecological Framework\n",
      "\tConcluding Remarks\n",
      "\n",
      "a2-gupta.pdf.json\n",
      "\t\n",
      "\t1. INTRODUCTION\n",
      "\t2. RELATED WORK\n",
      "\t3. METHODOLOGY\n",
      "\t3.1 Data Collection\n",
      "\t3.2 Events Selection\n",
      "\t3.3 Annotation Scheme\n",
      "\t4. ANALYSIS\n",
      "\t4.1 Types of Features\n",
      "\t4.2 Pseudo Relevance Feedback\n",
      "\t4.3 Evaluation metric\n",
      "\t5. EXPERIMENTAL RESULTS\n",
      "\t5.1 Regression Analysis\n",
      "\t5.2 Evaluation of Ranking\n",
      "\t6. DISCUSSION\n",
      "\t7. REFERENCES\n",
      "\n",
      "Econets_Neural_networks_that_learn_in_an_environme.pdf.json\n",
      "\t\n",
      "\n",
      "fpsyg-02-00355.pdf.json\n",
      "\t\n",
      "\tRick Dale1*, Natasha Z. Kirkham2 and Daniel C. Richardson3\n",
      "\tINTRODUCTION\n",
      "\tEXPERIMENT\n",
      "\tMETHODS\n",
      "\tParticipants\n",
      "\tApparatus\n",
      "\tStimuli\n",
      "\tProcedure\n",
      "\tData and analysis\n",
      "\tRESULTS\n",
      "\tCompletion time\n",
      "\tShuffled vs. non-shuffled lag profile\n",
      "\tDirector–matcher eye-movement synchronization (Deye–Meye)\n",
      "\tGENERAL DISCUSSION\n",
      "\tAPPENDIX\n",
      "\tOBTAINING DISTRIBUTIONS FROM LAG PROFILES\n",
      "\tWHICH BASELINE TO USE?\n",
      "\n",
      "docouto2014.pdf.json\n",
      "\t\n",
      "\t1. Introduction\n",
      "\t2. Ecolinguistics and ecology\n",
      "\t3. Precursors of ecolinguistics\n",
      "\t4. The emergence of ecolinguistics\n",
      "\t5. Ecolinguistics today\n",
      "\t6. Ecolinguistics as a platform\n",
      "\t7. Concluding remarks\n",
      "\n",
      "Compositionality decomposed.pdf.json\n",
      "\t\n",
      "\t1. Introduction\n",
      "\t2. Related work\n",
      "\t2.1 Evaluating compositionality with artificial data\n",
      "\t2.1.1 Arithmetic language and mathematical reasoning\n",
      "\t2.1.2 SCAN\n",
      "\t2.1.3 Lookup tables\n",
      "\t2.1.4 Logical inference\n",
      "\t2.2 Evaluating compositionality with natural data\n",
      "\t2.2.1 Number agreement\n",
      "\t2.2.2 Syntax in machine translation\n",
      "\t2.3 Intermediate conclusions\n",
      "\t3. Testing compositionality\n",
      "\t3.1 Systematicity\n",
      "\t3.1.1 Testing systematicity\n",
      "\t3.2 Productivity\n",
      "\t3.2.1 Testing productivity\n",
      "\t3.3 Substitutivity\n",
      "\t3.3.1 Testing substitutivity\n",
      "\t3.4 Localism\n",
      "\t3.4.1 Testing localism\n",
      "\t3.5 Overgeneralisation\n",
      "\t3.5.1 Testing overgeneralisation\n",
      "\t4. Data\n",
      "\t4.1 Input sequences: syntax\n",
      "\t4.2 Output sequences: semantics\n",
      "\t4.3 Data construction\n",
      "\t4.3.1 Naturalisation of structural properties\n",
      "\t4.3.2 Sentence selection\n",
      "\t5. Architectures\n",
      "\t5.1 LSTMS2S\n",
      "\t5.1.1 Model basics\n",
      "\t5.1.2 Implementation\n",
      "\t5.2 ConvS2S\n",
      "\t5.2.1 Model basics\n",
      "\t5.2.2 Model implementation\n",
      "\t5.3 Transformer\n",
      "\t5.3.1 Model basics\n",
      "\t5.3.2 Implementation\n",
      "\t6. Experiments and results\n",
      "\t6.1 Task accuracy\n",
      "\t6.1.1 Impact of length, depth and number of functions\n",
      "\t6.1.2 Function difficulty\n",
      "\t6.2 Systematicity\n",
      "\t6.2.1 Test details\n",
      "\t6.2.2 Results\n",
      "\t6.3 Productivity\n",
      "\t6.3.1 Test details\n",
      "\t6.3.2 Results\n",
      "\t6.4 Substitutivity\n",
      "\t6.4.1 Test details\n",
      "\t6.4.2 Equally distributed substitutions\n",
      "\t6.4.3 Primitive substitutions\n",
      "\t6.5 Localism\n",
      "\t6.5.1 Test details\n",
      "\t6.5.2 Results\n",
      "\t6.6 Overgeneralisation\n",
      "\t6.6.1 Test details\n",
      "\t6.6.2 Results\n",
      "\t7. Discussion\n",
      "\t7.1 An evaluation framework to evaluate compositionality\n",
      "\t7.2 Summary of results\n",
      "\t7.3 Conclusion and future work\n",
      "\tAcknowledgments\n",
      "\tAppendix A. Naturalisation of artificial data\n",
      "\n",
      "On_the_continuity_of_mind_Toward_a_dynam.pdf.json\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\t\n",
      "\n",
      "Capacity, Bandwidth, and Compositionality in Emergent Language Learning.pdf.json\n",
      "\t\n",
      "\t1 INTRODUCTION\n",
      "\t2 RELATEDWORK\n",
      "\t3 COMPOSITIONAL LANGUAGE AND LEARNING\n",
      "\t3.1 Compositional Language\n",
      "\t3.2 Learning a language\n",
      "\t3.3 Compositionality, learning, and capacity\n",
      "\t4 VARIATIONAL AUTOENCODERS AND THEIR CAPACITY\n",
      "\t4.1 Variational autoencoder as a communication channel\n",
      "\t5.2 for detailed analysis. Panels (a) and (f) show the accuracy of the training data, (b) and (d) show entropy, (e) and (g) show recall over the test data, and (c) plots the max difference in accuracy between training and test.\n",
      "\t4.2 Capacity of a variational autoencoder with discrete sequence bottleneck\n",
      "\t4.3 Implications and hypotheses on compositionality\n",
      "\t5 EXPERIMENTS\n",
      "\t5.1 Evaluation: Residual Entropy\n",
      "\t5.2 Results\n",
      "\t6 CONCLUSION\n",
      "\tACKNOWLEDGEMENTS\n",
      "\n",
      "2013-Fusella.pdf.json\n",
      "\t\n",
      "\tDynamic Systems Theory in Cognitive Science: Major Elements, Applications, and Debates Surrounding a Revolutionary MetaTheory\n",
      "\tIntroduction\n",
      "\tThe Debate and Commentary Published in Behavioral and Brain Sciences in 1998\n",
      "\tThe Debate and Commentary Published in TopiCS in Cognitive Science in 2012\n",
      "\tConclusion\n",
      "\n",
      "dzeroski2001-Relational_Reinforcement_Learning.pdf.json\n",
      "\t\n",
      "\t1. Introduction\n",
      "\t8 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\t2. Problem specification\n",
      "\t2.1. Reinforcement learning\n",
      "\tGiven\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 9\n",
      "\t2.2. Reinforcement learning for planning\n",
      "\t10 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\t2.3. An example\n",
      "\t3. Q-learning and P-learning\n",
      "\t3.1. Q-learning\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 11\n",
      "\t12 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\t3.2. P-learning\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 13\n",
      "\t4. Top-down induction of logical decision trees\n",
      "\t4.1. Decision trees\n",
      "\t14 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\t4.2. Logical decision trees\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 15\n",
      "\t4.3. Declarative bias\n",
      "\t16 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\t4.4. Background knowledge\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 17\n",
      "\t5. Relational reinforcement learning\n",
      "\t5.1. The need for relational representations\n",
      "\t18 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\t5.2. The task of relational reinforcement learning\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 19\n",
      "\t5.3. The Q-RRL algorithm\n",
      "\t20 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 21\n",
      "\t22 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\t5.4. The P-RRL algorithm\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 23\n",
      "\t24 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 25\n",
      "\t6. Experiments\n",
      "\t6.1. Questions addressed\n",
      "\t6.2. Experimental setup\n",
      "\t26 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 27\n",
      "\t6.3. Fixed number of blocks while learning\n",
      "\t28 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 29\n",
      "\t30 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 31\n",
      "\t6.4. Varying the number of blocks while learning\n",
      "\t6.5. Q-learning versus P-learning\n",
      "\t32 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 33\n",
      "\t6.6. When does relational reinforcement learning work?\n",
      "\t34 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 35\n",
      "\t36 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\t6.7. Efficiency\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 37\n",
      "\t38 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\t6.8. Summary of experimental results\n",
      "\t7. Discussion\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 39\n",
      "\t7.1. Scalability\n",
      "\t7.2. Related work\n",
      "\t40 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 41\n",
      "\t7.3. Further work\n",
      "\tAppendix A: TILDE and TILDE-RT algorithms\n",
      "\t42 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tB. Background knowledge for TILDE\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 43\n",
      "\tC. Settings for TILDE and TILDE-RT\n",
      "\tC.1. TILDE-RT settings\n",
      "\tC.2. TILDE settings\n",
      "\tD. Q-policies and P-policies induced in the 4-blocks world by the P-RRL algorithm\n",
      "\tD.1. P-policy for unstack\n",
      "\tD.2. Q-policy for unstack\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 47\n",
      "\tD.3. P-policy for stack\n",
      "\tD.4. Q-policy for stack\n",
      "\tD.5. P-policy for on(A,B)\n",
      "\t48 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tD.6. Q-policy for on(A,B)\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 49\n",
      "\t50 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\tRELATIONAL REINFORCEMENT LEARNING 51\n",
      "\tAcknowledgments\n",
      "\tNote\n",
      "\t52 DŽEROSKI, DE RAEDT AND DRIESSENS\n",
      "\n",
      "The Emergence of Compositional Languages.pdf.json\n",
      "\t1 Introduction\n",
      "\t2 Model Methods\n",
      "\t2.1 The Bag-Select Game\n",
      "\t2.2 Input Representations of Bags to be Communicated\n",
      "\t2.3 Iterated Learning for Deep Learning Models\n",
      "\t3 Emergence of Compositional Languages\n",
      "\t4 Learnability of Compositional and Emergent Languages\n",
      "\t5 Conclusion\n",
      "\tA: Game Description\n",
      "\tB: Phases of Iterated Learning for Deep Learning\n",
      "\tC: Metrics and Evaluations\n",
      "\tD: Experiments on the Emergence of Compositional Languages\n",
      "\tE: Establish and Train Different Types of Languages\n",
      "\tF: Further Explaination about Learnability Experiments\n",
      "\n",
      "taylor2016.pdf.json\n",
      "\t\n",
      "\t1. Naturalistic language science and the Western linguistic imaginary\n",
      "\t3. Folk metalinguistic practices\n",
      "\t4. A thought-experiment\n",
      "\t5. Conclusions\n",
      "\n",
      "Modeling natural language emergence with integral transform theory and.pdf.json\n",
      "\t\n",
      "\tINTRODUCTION AND BACKGROUND\n",
      "\tRESULTS\n",
      "\tSIMULATION\n",
      "\tDATA AND SOURCE CODE\n",
      "\tMODEL STRUCTURE\n",
      "\tTRAINING AND TESTING\n",
      "\tAGENT ARCHITECTURES\n",
      "\tSIMULATION RESULTS\n",
      "\tFUTURE DIRECTIONS\n",
      "\tCONCLUSIONS\n",
      "\n",
      "1-s2.0-S0004370209000460-main.pdf.json\n",
      "\t\n",
      "\t1. Introduction\n",
      "\t2. Preliminaries\n",
      "\t3. Learnability of scoring rules\n",
      "\t3.1. Efficient learnability of S nm\n",
      "\t3.2. Lower bound for the generalized dimension of S nm\n",
      "\t4. Learnability of voting trees\n",
      "\t4.1. Large voting trees\n",
      "\t4.2. Small voting trees\n",
      "\t4.3. Computational complexity\n",
      "\t5. On learning voting rules “close” to target rules\n",
      "\t6. Discussion\n",
      "\tAcknowledgements\n",
      "\n",
      "Symbol Emergence in Cognitive Developmental.pdf.json\n",
      "\t\n",
      "\tII. SEMIOTICS: FROM SIGNS TO SYMBOLS\n",
      "\tIII. PROBLEM HISTORY 1: SYMBOL EMERGENCE IN BIOLOGICAL SYSTEMS\n",
      "\tA. Evolutionary viewpoint: from actions to symbols\n",
      "\tB. Neuroscientific viewpoint: from neural representations to symbols\n",
      "\tC. Cognitive science viewpoint: from concepts to symbols\n",
      "\tD. Developmental psychology viewpoint: from behaviors to symbols\n",
      "\tIV. PROBLEM HISTORY 2: SYMBOL EMERGENCE IN ARTIFICIAL SYSTEMS\n",
      "\tA. Artificial intelligence viewpoint: from tokens to symbols\n",
      "\tB. Pattern recognition viewpoint: from labels to symbols\n",
      "\tC. Unsupervised learning viewpoint: from multimodal categorization to symbols\n",
      "\tD. Reinforcement learning viewpoint: from states and actions to symbols\n",
      "\tE. Dynamical systems viewpoint: from attractors to symbols\n",
      "\tA. Wrapping things up (from the perspective of PSS)\n",
      "\tB. Symbol emergence systems\n",
      "\tC. Redefinition of the terminology\n",
      "\tVI. CHALLENGES\n",
      "\tA. Computational models for symbol emergence and cognitive architecture\n",
      "\tB. Robotic planning with grounded symbols\n",
      "\tC. Language acquisition by a robot\n",
      "\tD. Human–robot communication, situated symbols, and mutual beliefs\n",
      "\tVII. CONCLUSION\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# headings\n",
    "\n",
    "for name, article in articles.items():\n",
    "    data = article['metadata']\n",
    "    print(name)\n",
    "    for section in (data['sections'] or []):\n",
    "        print('\\t' + (section['heading'] or ''))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-01T15:19:45.031798Z",
     "start_time": "2020-05-01T15:19:44.967131Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"2019.pdf.json\": 1,\n",
      "  \"Multi agent cooperation and emergence of natujral langauge.pdf.json\": 4,\n",
      "  \"Exploiting Deep Semantics and Compositionality of Natural Language.pdf.json\": 1,\n",
      "  \"Microblogging_during_two_natural_hazards_events_Wh.pdf.json\": 1,\n",
      "  \"Faking Sandy: Characterizing and Identifying Fake Images.pdf.json\": 1,\n",
      "  \"Natural Language Does Not Emerge \\u2018Naturally\\u2019 in Multi-Agent Dialog.pdf.json\": 2,\n",
      "  \"ghosh1999.pdf.json\": 2,\n",
      "  \"Reference-Aware Language Models.pdf.json\": 1,\n",
      "  \"Emergence of linguistic conventions in multi-agent reinforcement.pdf.json\": 1,\n",
      "  \"2000,Beer.pdf.json\": 1,\n",
      "  \"a2-gupta.pdf.json\": 4,\n",
      "  \"fpsyg-02-00355.pdf.json\": 2,\n",
      "  \"On_the_continuity_of_mind_Toward_a_dynam.pdf.json\": 15,\n",
      "  \"Capacity, Bandwidth, and Compositionality in Emergent Language Learning.pdf.json\": 1,\n",
      "  \"dzeroski2001-Relational_Reinforcement_Learning.pdf.json\": 6,\n",
      "  \"Symbol Emergence in Cognitive Developmental.pdf.json\": 4\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# check how often figure captions are incorporated into main text...\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "fig_counts = defaultdict(int)\n",
    "for name, article in articles.items():\n",
    "    data = article['metadata']\n",
    "#     print(name)\n",
    "    for section in (data['sections'] or {}):\n",
    "        text = section.get('text', '')\n",
    "        \n",
    "        # figures should be at the beginning of a fragment or preceded by a whitespace\n",
    "        m = re.findall(r'(^|\\.\\s)[fF]ig(\\.|ure)?[0-9\\s]', text)\n",
    "        if len(m) > 0:\n",
    "#             print(name)\n",
    "#             print(text)\n",
    "            fig_counts[name] += len(list(m))\n",
    "\n",
    "print(json.dumps(fig_counts, indent=2))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obervations on 36 sample pdfs:\n",
    "\n",
    "#### PROs\n",
    "- references at the end are parsed\n",
    "- one-sentence reference mention contexts are provided\n",
    "- many figures and side notes\n",
    "\n",
    "#### CONs\n",
    "- additional text like bottom notes or figure captions are either incorporated into the main text or discarded...\n",
    "- no information about text size etc.\n",
    "- long application startup (without a running server): processing of a single paper 102s..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogsci_python",
   "language": "python",
   "name": "cogsci_python"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
