<html><head>
<meta http-equiv="Content-Type" content="text/html">
</head><body>
<span style="position:absolute; border: gray 1px solid; left:0px; top:50px; width:612px; height:792px;"></span>
<div style="position:absolute; top:50px;"><a name="1">Page 1</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:16px; top:261px; width:20px; height:348px;"><span style="font-family: Times-Roman; font-size:10px">9
<br>1
<br>0
<br>2
<br></span><span style="font-family: Times-Roman; font-size:5px"> 
<br></span><span style="font-family: Times-Roman; font-size:8px">c
<br>e
<br></span><span style="font-family: Times-Roman; font-size:14px">D
<br></span><span style="font-family: Times-Roman; font-size:5px"> 
<br></span><span style="font-family: Times-Roman; font-size:10px">2
<br>1
<br></span><span style="font-family: Times-Roman; font-size:5px"> 
<br> 
<br></span><span style="font-family: Times-Roman; font-size:6px">]
<br></span><span style="font-family: Times-Roman; font-size:12px">L
<br></span><span style="font-family: Times-Roman; font-size:13px">C
<br></span><span style="font-family: Times-Roman; font-size:5px">.
<br></span><span style="font-family: Times-Roman; font-size:7px">s
<br></span><span style="font-family: Times-Roman; font-size:8px">c
<br></span><span style="font-family: Times-Roman; font-size:6px">[
<br></span><span style="font-family: Times-Roman; font-size:5px"> 
<br> 
<br></span><span style="font-family: Times-Roman; font-size:10px">1
<br>v
<br>7
<br>7
<br>8
<br>5
<br>0
<br></span><span style="font-family: Times-Roman; font-size:5px">.
<br></span><span style="font-family: Times-Roman; font-size:10px">2
<br>1
<br>9
<br>1
<br></span><span style="font-family: Times-Roman; font-size:5px">:
<br></span><span style="font-family: Times-Roman; font-size:10px">v
<br></span><span style="font-family: Times-Roman; font-size:5px">i
<br></span><span style="font-family: Times-Roman; font-size:14px">X
<br></span><span style="font-family: Times-Roman; font-size:6px">r
<br></span><span style="font-family: Times-Roman; font-size:8px">a
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:104px; width:452px; height:45px;"><span style="font-family: LMSans10-Bold; font-size:21px">Extending Machine Language Models toward
<br>Human-Level Language Understanding
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:160px; width:408px; height:10px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">James L. McClelland</span><span style="font-family: LMRoman6-Regular; font-size:5px">a,b,2</span><span style="font-family: NimbusSanL-Bold; font-size:8px">, Felix Hill</span><span style="font-family: LMRoman6-Regular; font-size:5px">b,2</span><span style="font-family: NimbusSanL-Bold; font-size:8px">, Maja Rudolph</span><span style="font-family: LMRoman6-Regular; font-size:5px">c,2</span><span style="font-family: NimbusSanL-Bold; font-size:8px">, Jason Baldridge</span><span style="font-family: LMRoman6-Regular; font-size:5px">d,1,2</span><span style="font-family: NimbusSanL-Bold; font-size:8px">, and Hinrich Schütze</span><span style="font-family: LMRoman6-Regular; font-size:5px">e,1,2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:180px; width:504px; height:16px;"><span style="font-family: LMRoman5-Regular; font-size:4px">a</span><span style="font-family: NimbusSanL-Regu; font-size:6px">Stanford University, Stanford, CA 94305, USA; </span><span style="font-family: LMRoman5-Regular; font-size:4px">b</span><span style="font-family: NimbusSanL-Regu; font-size:6px">DeepMind, London N1C 4AG, UK; </span><span style="font-family: LMRoman5-Regular; font-size:4px">c</span><span style="font-family: NimbusSanL-Regu; font-size:6px">Bosch Center for Artiﬁcial Intelligence, Renningen, 71272, Germany; </span><span style="font-family: LMRoman5-Regular; font-size:4px">d</span><span style="font-family: NimbusSanL-Regu; font-size:6px">Google Research,
<br>Austin, TX 78701, USA; </span><span style="font-family: LMRoman5-Regular; font-size:4px">e</span><span style="font-family: NimbusSanL-Regu; font-size:6px">LMU Munich, Munich, 80538, Germany
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:206px; width:157px; height:6px;"><span style="font-family: NimbusSanL-Regu; font-size:6px">This manuscript was compiled on December 13, 2019
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:227px; width:246px; height:182px;"><span style="font-family: NimbusSanL-Bold; font-size:7px">Language is central to human intelligence. We review recent break-
<br>throughs in machine language processing and consider what re-
<br>mains to be achieved. Recent approaches rely on domain general
<br>principles of learning and representation captured in artiﬁcial neu-
<br>ral networks. Most current models, however, focus too closely on
<br>In humans, language is part of a larger system for
<br>language itself.
<br>acquiring, representing, and communicating about objects and sit-
<br>uations in the physical and social world, and future machine lan-
<br>guage models should emulate such a system. We describe exist-
<br></span><span style="font-family: NimbusSanL-Bold; font-size:7px">ing machine models linking language to concrete situations, and
<br></span><span style="font-family: NimbusSanL-Bold; font-size:7px">point toward extensions to address more abstract cases. Human
<br>language processing exploits complementary learning systems, in-
<br>cluding a deep neural network-like learning system that learns grad-
<br>ually as machine systems do, as well as a fast-learning system that
<br>supports learning new information quickly. Adding such a system to
<br>machine language models will be an important further step toward
<br>truly human-like language understanding.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:420px; width:245px; height:6px;"><span style="font-family: LMRoman7-Regular; font-size:6px">Language Understanding </span><span style="font-family: LMMathSymbols7-Regular; font-size:6px">| </span><span style="font-family: LMRoman7-Regular; font-size:6px">Natural Language Processing </span><span style="font-family: LMMathSymbols7-Regular; font-size:6px">| </span><span style="font-family: LMRoman7-Regular; font-size:6px">Situation
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:431px; width:245px; height:6px;"><span style="font-family: LMRoman7-Regular; font-size:6px">Models </span><span style="font-family: LMMathSymbols7-Regular; font-size:6px">| </span><span style="font-family: LMRoman7-Regular; font-size:6px">Machine Language Models </span><span style="font-family: LMMathSymbols7-Regular; font-size:6px">| </span><span style="font-family: LMRoman7-Regular; font-size:6px">Brain System for Understanding
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:450px; width:246px; height:107px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Many of the most impressive recent successes of machine in-
<br>telligence have appeared in the domain of language. Machines
<br>can now better identify the words we speak and respond in
<br>ever more natural sounding voices. More impressive still is
<br>modern machine translation (1). Anyone with a smartphone
<br>has access to applications that allow them to say a sentence in
<br>one language and then see and hear its translation in another.
<br>Human ability still far exceeds machines in most language
<br>tasks, but these systems work well enough to be used by
<br>billions of people everyday.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:560px; width:247px; height:128px;"><span style="font-family: LMRoman9-Regular; font-size:8px">What underlies these successes, and what limitations do
<br>these systems face? We argue that successes to date come from
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">ever more eﬀective methods for exploiting principles of neural
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">computation that human language users also exploit. We then
<br>note that the work remains limited in that it largely treats
<br>language separately from the larger task of understanding
<br>the world around us. This leads us to propose an integrated
<br>approach to building a system that truly understands, in which
<br>language plays a key role in concert with other sources of input.
<br>We discuss the challenges facing further development of the
<br>approach and propose future steps toward addressing these
<br>challenges.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:702px; width:149px; height:9px;"><span style="font-family: NimbusSanL-Bold; font-size:9px">Principles of Neural Computation
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:718px; width:247px; height:63px;"><span style="font-family: LMRoman9-Regular; font-size:8px">The principles of neural computation are domain general prin-
<br>ciples inspired by the human brain. They were ﬁrst articulated
<br>in the 1950s (2) and further developed in the 1980s in the
<br>Parallel Distributed Processing (PDP) framework for modeling
<br>cognition (3). A central idea of this approach is that struc-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">ture in language and other cognitive domains is an emergent
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:301px; top:798px; width:4px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:226px; width:246px; height:118px;"><span style="font-family: LMRoman9-Regular; font-size:8px">phenomenon, captured in learned connection weights and re-
<br>sulting in context-sensitive, distributed representations whose
<br>characteristics reﬂect a gradual, input-statistics dependent,
<br>learning process (4). The models treat the symbols and rules
<br>of classical linguistic theory as consequences of processing and
<br>learning, not entities whose structure must be built in. Instead
<br>of discrete symbols for linguistic units, these models rely on
<br>patterns of activity often called </span><span style="font-family: LMRoman9-Italic; font-size:8px">embeddings </span><span style="font-family: LMRoman9-Regular; font-size:8px">over arrays of
<br>neuron-like processing units. Instead of explicit systems of
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">rules, they rely on learned matrices of connection weights to
<br>map patterns on one set of units into patterns on others.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:309px; top:346px; width:247px; height:194px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Another key principle is mutual constraint satisfaction (5).
<br>For example, the meaning of a sentence depends on its struc-
<br>ture (its organization into constituent phrases); but so too can
<br>the structure depend on the meaning. Consider the sentence </span><span style="font-family: LMRoman9-Italic; font-size:8px">A
<br>boy hit a man with a __</span><span style="font-family: LMRoman9-Regular; font-size:8px">. If the missing word is </span><span style="font-family: LMRoman9-Italic; font-size:8px">bat</span><span style="font-family: LMRoman9-Regular; font-size:8px">, </span><span style="font-family: LMRoman9-Italic; font-size:8px">with a bat
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">is read as part of a verb phrase headed by </span><span style="font-family: LMRoman9-Italic; font-size:8px">hit</span><span style="font-family: LMRoman9-Regular; font-size:8px">, and speciﬁes the
<br>instrument used to carry out the action. But if </span><span style="font-family: LMRoman9-Italic; font-size:8px">beard </span><span style="font-family: LMRoman9-Regular; font-size:8px">ﬁlls the
<br>blank, </span><span style="font-family: LMRoman9-Italic; font-size:8px">with a beard </span><span style="font-family: LMRoman9-Regular; font-size:8px">is a part of a noun phrase describing who
<br>was hit. Even the segmentation of spoken or written language
<br>into elementary segments (e.g., letters) depends in part on
<br>meaning and context, as illustrated in Fig. 1. Rumelhart (5)
<br>sketched an interactive model of language understanding in
<br>which estimates of probabilities about all aspects of an input
<br>constrain estimates of the probability of every other aspect.
<br>The idea was captured in a model of context eﬀects in per-
<br>ception (6). Later work (7, 8) linked these ideas to energy
<br>minimization in statistical physics. Neural language modeling
<br>research, which we now describe, incorporates these principles.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:556px; width:120px; height:9px;"><span style="font-family: NimbusSanL-Bold; font-size:9px">Neural Language Modeling
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:309px; top:573px; width:247px; height:139px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">An Early Neural Language Model. </span><span style="font-family: LMRoman9-Regular; font-size:8px">Elman (9) built on the prin-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">ciples above to demonstrate how neural models can capture
<br>key characteristics of language structure through learning, a
<br>feat once considered impossible (10). The model provides a
<br>starting point for understanding recent developments. Elman
<br>used the recurrent neural network shown in Fig. 2a. The
<br>network was trained to predict the next word in a sequence
<br>(</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">w</span><span style="font-family: LMRoman9-Regular; font-size:8px">(</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">t </span><span style="font-family: LMRoman9-Regular; font-size:8px">+ 1)) based on the current word (</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">w</span><span style="font-family: LMRoman9-Regular; font-size:8px">(</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">t</span><span style="font-family: LMRoman9-Regular; font-size:8px">)) and its own </span><span style="font-family: LMRoman9-Italic; font-size:8px">hidden
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">(that is, learned internal) representation from the previous
<br>time step (</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">h</span><span style="font-family: LMRoman9-Regular; font-size:8px">(</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">t </span><span style="font-family: LMMathSymbols9-Regular; font-size:8px">− </span><span style="font-family: LMRoman9-Regular; font-size:8px">1)). These two inputs are each multiplied
<br>by a matrix of connection weights (represented by the arrows
<br>labeled </span><span style="font-family: LMMathItalic9-Regular; font-size:8px">W</span><span style="font-family: LMMathItalic6-Regular; font-size:5px">hi </span><span style="font-family: LMRoman9-Regular; font-size:8px">and </span><span style="font-family: LMMathItalic9-Regular; font-size:8px">W</span><span style="font-family: LMMathItalic6-Regular; font-size:5px">hh</span><span style="font-family: LMRoman9-Regular; font-size:8px">) to produce vectors that are added to
<br>produce a vector of inputs to the hidden layer of units. The
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:731px; width:104px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">JM, FH, MR, JB, and HS wrote the paper.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:742px; width:105px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">The authors declare no conﬂict of interest.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:753px; width:86px; height:7px;"><span style="font-family: NimbusSanL-Regu; font-size:4px">1</span><span style="font-family: NimbusSanL-Regu; font-size:5px">J.B. and H.S. contributed equally.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:766px; width:245px; height:14px;"><span style="font-family: NimbusSanL-Regu; font-size:4px">2 </span><span style="font-family: NimbusSanL-Regu; font-size:5px">To whom correspondence should be addressed. E-mail: jlmccstanford.edu, felixhill@google.com,
<br>marirudolph@gmail.com, jasonbaldridge@google.com or inquiries@cislmu.org
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:892px; width:612px; height:792px;"></span>
<div style="position:absolute; top:892px;"><a name="2">Page 2</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:1033px; width:245px; height:33px;"><span style="font-family: NimbusSanL-Bold; font-size:6px">Fig. 1. </span><span style="font-family: NimbusSanL-Regu; font-size:6px">Two handwritten sentences illustrating how context inﬂuences the identiﬁcation
<br>of letters in written text. The visual input we read as </span><span style="font-family: NimbusSanL-ReguItal; font-size:6px">went </span><span style="font-family: NimbusSanL-Regu; font-size:6px">in the ﬁrst sentence and
<br></span><span style="font-family: NimbusSanL-ReguItal; font-size:6px">event </span><span style="font-family: NimbusSanL-Regu; font-size:6px">in the second is the same bit of Rumelhart’s handwriting, cut and pasted into
<br>each of the two contexts. Reprinted from (5).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:1086px; width:247px; height:139px;"><span style="font-family: LMRoman9-Regular; font-size:8px">elements of this vector undergo a transformation limiting the
<br>range of activation values, resulting in the hidden layer repre-
<br>sentation. This in turn is multiplied with the matrix of weights
<br>to the output layer from the hidden layer (</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">W</span><span style="font-family: LMMathItalic6-Regular; font-size:5px">oh</span><span style="font-family: LMRoman9-Regular; font-size:8px">) to generate a
<br>vector used to predict the probability of each of the possible
<br>successor words. Learning in this and other neural models is
<br>based on the discrepancy between the network’s output and
<br>the actual next word; the values of the connection weights
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">are adjusted by a small amount to reduce the discrepancy.
<br>The network is called recurrent because the same connection
<br>weights (denoted by arrows in the ﬁgure) are used to process
<br>each successive word; the hidden representation </span><span style="font-family: LMMathItalic9-Regular; font-size:8px">h</span><span style="font-family: LMRoman9-Regular; font-size:8px">(</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">t</span><span style="font-family: LMRoman9-Regular; font-size:8px">) becomes
<br>the context representation </span><span style="font-family: LMMathItalic9-Regular; font-size:8px">h</span><span style="font-family: LMRoman9-Regular; font-size:8px">(</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">t </span><span style="font-family: LMMathSymbols9-Regular; font-size:8px">− </span><span style="font-family: LMRoman9-Regular; font-size:8px">1) for the next time step.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:55px; top:1228px; width:248px; height:270px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Elman demonstrated two crucial ﬁndings. First, after train-
<br>ing his network to predict the next word in simple sentences like
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">man eats bread</span><span style="font-family: LMRoman9-Regular; font-size:8px">, </span><span style="font-family: LMRoman9-Italic; font-size:8px">dog chases cat</span><span style="font-family: LMRoman9-Regular; font-size:8px">, and </span><span style="font-family: LMRoman9-Italic; font-size:8px">girl sleeps</span><span style="font-family: LMRoman9-Regular; font-size:8px">, the network’s
<br>representations captured the syntactic distinction between
<br>nouns and verbs (9). It also captured interpretable subcate-
<br>gories, as shown by a hierarchical clustering of the hidden-layer
<br>pattern (</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">h</span><span style="font-family: LMRoman9-Regular; font-size:8px">(</span><span style="font-family: LMMathItalic9-Regular; font-size:8px">t</span><span style="font-family: LMRoman9-Regular; font-size:8px">)) for each word in the training materials (Fig. 2b).
<br>This illustrates a key feature of learned representations in neu-
<br>ral models: they capture speciﬁc as well as general or abstract
<br>information. By using a diﬀerent learned representation for
<br>each word, the speciﬁc predictive consequences of that word
<br>can be exploited. Because representations for words that make
<br>similar predictions are similar, and because neural networks
<br>exploit similarity, the network can share knowledge about
<br>predictions among related words. Second, Elman (11) used
<br>both simple sentences like </span><span style="font-family: LMRoman9-Italic; font-size:8px">boy chases dogs </span><span style="font-family: LMRoman9-Regular; font-size:8px">and more complex
<br>ones like </span><span style="font-family: LMRoman9-Italic; font-size:8px">boy who sees girls chases dogs</span><span style="font-family: LMRoman9-Regular; font-size:8px">. In the more complex
<br>case, the verb </span><span style="font-family: LMRoman9-Italic; font-size:8px">chases </span><span style="font-family: LMRoman9-Regular; font-size:8px">must agree with the ﬁrst noun (</span><span style="font-family: LMRoman9-Italic; font-size:8px">boy</span><span style="font-family: LMRoman9-Regular; font-size:8px">), not
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">the closest noun (</span><span style="font-family: LMRoman9-Italic; font-size:8px">girls</span><span style="font-family: LMRoman9-Regular; font-size:8px">), since the sentence contains a main
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">clause (</span><span style="font-family: LMRoman9-Italic; font-size:8px">boy chases dogs</span><span style="font-family: LMRoman9-Regular; font-size:8px">) interrupted by an embedded clause
<br>(</span><span style="font-family: LMRoman9-Italic; font-size:8px">boy [who] sees girls</span><span style="font-family: LMRoman9-Regular; font-size:8px">). The model learned to predict the verb
<br>form correctly despite the intervening clause, showing that it
<br>acquired sensitivity to the syntactic structure of language, and
<br>not just local co-occurrence statistics in its learned connections
<br>and distributed representations.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:1510px; width:246px; height:85px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">Scaling Up to Process Natural Text. </span><span style="font-family: LMRoman9-Regular; font-size:8px">Elman’s task—predicting
<br>the next word in a sequence—has been central to neural lan-
<br>guage modeling. However, Elman trained his networks with
<br>only tiny fragments of what were eﬀectively toy languages.
<br>For many years, it seemed they would not scale up. Beginning
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">about 10 years ago, advances in machine language process-
<br>ing began to overcome this limitation for neural models. We
<br>describe two crucial developments next.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:1604px; width:245px; height:19px;"><span style="font-family: NimbusSanL-BoldItal; font-size:7px">Long-distance dependencies and pretrained word embeddings. </span><span style="font-family: LMRoman9-Regular; font-size:8px">A
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">challenge for language prediction is the indeﬁnite length of
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:1095px; width:245px; height:15px;"><span style="font-family: NimbusSanL-Bold; font-size:6px">Fig. 2. </span><span style="font-family: NimbusSanL-Regu; font-size:6px">(a) Elman’s (1990) simple recurrent network and (b) his hierarchical clustering
<br>of the representations it learned, reprinted from (9).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:1134px; width:235px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">the context that might be relevant. Consider this passage:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:333px; top:1156px; width:209px; height:63px;"><span style="font-family: LMRoman9-Regular; font-size:8px">John put some beer in a cooler and went out with his
<br>friends to play volleyball. Soon after he left, someone
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">took the beer out of the cooler. John and his friends
<br>were thirsty after the game, and went back to his
<br>place for some beers. When John opened the cooler,
<br>he discovered that the beer was ___.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:313px; top:1232px; width:248px; height:292px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Here a reader expects the missing word to be </span><span style="font-family: LMRoman9-Italic; font-size:8px">gone</span><span style="font-family: LMRoman9-Regular; font-size:8px">. Yet if
<br>we replaced </span><span style="font-family: LMRoman9-Italic; font-size:8px">someone took the beer </span><span style="font-family: LMRoman9-Regular; font-size:8px">with </span><span style="font-family: LMRoman9-Italic; font-size:8px">someone took the ice
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">the expected word would be </span><span style="font-family: LMRoman9-Italic; font-size:8px">warm </span><span style="font-family: LMRoman9-Regular; font-size:8px">instead. Furthermore, any
<br>amount of additional text between </span><span style="font-family: LMRoman9-Italic; font-size:8px">beer </span><span style="font-family: LMRoman9-Regular; font-size:8px">and </span><span style="font-family: LMRoman9-Italic; font-size:8px">gone </span><span style="font-family: LMRoman9-Regular; font-size:8px">would not
<br>change the predictive relationship. Elman’s network could
<br>take only a few words of context into account, reﬂecting a
<br>larger challenge known as the </span><span style="font-family: LMRoman9-Italic; font-size:8px">vanishing gradient </span><span style="font-family: LMRoman9-Regular; font-size:8px">problem (12).
<br>In essence, the magnitude of the learning signal that deter-
<br>mines the adjustments to connection weights tends to decrease
<br>exponentially as the number of layers of weights between an
<br>input and an output increases. The development of neural
<br>network modules called Long-Short-Term-Memory (LSTM)
<br>modules (13) that partially overcame this limitation was there-
<br>fore crucial, greatly increasing the contextual range of neural
<br>models. In another crucial development, researchers began
<br>to use pre-trained word embeddings derived from learning
<br>predictive relationships among words (14, 15). When training
<br>a neural model for a speciﬁc task, such embeddings could then
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">be used to directly represent training words in the model’s
<br>input. The embeddings were based on the aggregate statistics
<br>of large text corpora, and captured both general and speciﬁc
<br>predictive relationships, supporting generalization at both gen-
<br>eral and speciﬁc levels. Using these embeddings, task-focused
<br>models trained with relatively small data sets could generalize
<br>what they learned from training on frequent words (such as
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">sofa</span><span style="font-family: LMRoman9-Regular; font-size:8px">) to infrequent words with similar predictive relationships
<br>(such as </span><span style="font-family: LMRoman9-Italic; font-size:8px">settee</span><span style="font-family: LMRoman9-Regular; font-size:8px">).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:1528px; width:247px; height:96px;"><span style="font-family: LMRoman9-Regular; font-size:8px">A limitation of the above approach is that the same rep-
<br>resentation of a word is used every time it occurs, regardless
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">of context. However, in line with the principle of mutual
<br>constraint satisfaction, humans interpret words, including am-
<br>biguous words like </span><span style="font-family: LMRoman9-Italic; font-size:8px">bank</span><span style="font-family: LMRoman9-Regular; font-size:8px">, in accordance with the context (16),
<br>rapidly assigning a contextually appropriate meaning to each
<br>word based on all other words. A ﬁxed embedding also limits
<br>predicting other words; the predictive implication of the word
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">bank </span><span style="font-family: LMRoman9-Regular; font-size:8px">depends on which kind of bank is involved. Initial steps
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:1640px; width:277px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">McClelland et al.: Language understanding in humans and machines
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:555px; top:1640px; width:4px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">2
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:65px; top:949px; width:226px; height:65px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:314px; top:949px; width:240px; height:135px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:1734px; width:612px; height:792px;"></span>
<div style="position:absolute; top:1734px;"><a name="3">Page 3</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:1793px; width:247px; height:41px;"><span style="font-family: LMRoman9-Regular; font-size:8px">for every intervening word, the context reaches the target
<br>directly. Likewise, gradient learning signals skip over the inter-
<br>vening words, avoiding the dissipation of learning signal that
<br>would otherwise occur.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:309px; top:1836px; width:247px; height:194px;"><span style="font-family: LMRoman9-Regular; font-size:8px">BERT-based models have produced remarkable improve-
<br>ments on a wide range of language tasks (25, 26). The models
<br>can be pre-trained on massive text corpora, providing useful
<br>representations for subsequent tasks for which little speciﬁc
<br>training data exists (27). These models seem to capture syn-
<br>tactic, semantic and world knowledge and they are beginning
<br>to address tasks once thought beyond their reach. For ex-
<br>ample, </span><span style="font-family: LMRoman9-Italic; font-size:8px">Winograd Schema challenge </span><span style="font-family: LMRoman9-Regular; font-size:8px">(28) requires determining
<br>the referent of a pronoun (here </span><span style="font-family: LMRoman9-Italic; font-size:8px">it</span><span style="font-family: LMRoman9-Regular; font-size:8px">) in a sentence such as </span><span style="font-family: LMRoman9-Italic; font-size:8px">The
<br>trophy did not ﬁt in the suitcase because it was too ___</span><span style="font-family: LMRoman9-Regular; font-size:8px">. For a
<br>person, world knowledge tells us that if the missing word is </span><span style="font-family: LMRoman9-Italic; font-size:8px">big
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">the referent must be the trophy, but if it is </span><span style="font-family: LMRoman9-Italic; font-size:8px">small </span><span style="font-family: LMRoman9-Regular; font-size:8px">the referent
<br>must be the suitcase. The latest models achieve ever-higher
<br>scores on benchmarks including variants of the Winograd chal-
<br>lenge (29). However, variants of test materials that do not
<br>fool humans continue to stymie even the best models (30), and
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">further reﬁnements in the models and their assessment will be
<br>required before it will be clear what such models can achieve.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:2044px; width:230px; height:9px;"><span style="font-family: NimbusSanL-Bold; font-size:9px">The Human Integrated Understanding System (IUS)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:2060px; width:247px; height:194px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">Situations and objects. </span><span style="font-family: LMRoman9-Regular; font-size:8px">Despite the successes of neural lan-
<br>guage modeling, an important limitation is that these models
<br>are purely language based. We need models in which lan-
<br>guage is a part of an </span><span style="font-family: LMRoman9-Italic; font-size:8px">integrated understanding system </span><span style="font-family: LMRoman9-Regular; font-size:8px">(IUS)
<br>for understanding and communicating about the situations
<br>we encounter and the objects that participate in them. Rep-
<br>resentations of situations constitute our models of our world
<br>and guide our behavior and our interpretation of language.
<br>Indeed, resolving the referent of the pronoun in a Winograd
<br>sentence would follow from building a representation of the
<br>situation the sentence describes. In the situation in which a
<br>trombone does not ﬁt in a suitcase, the natural reason would
<br>be that the trombone is too big or the suitcase too small; the
<br>identity of the referent of the pronoun follows from realizing
<br>this. Thus, solving the Winograd Schema challenge is a natu-
<br>ral byproduct of the human language understanding process.
<br>In short, we argue that language evolved for communication
<br>about situations and our systems should address this goal.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:2256px; width:247px; height:194px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Situations can be concrete and static, such as one where
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">a cat is on a mat, or they may be events such as one where
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">a boy hits a ball. They can be conceptual, social or legal,
<br>such as one where a court invalidates a law. They may even
<br>be imaginary. The objects may be real or ﬁctitious physical
<br>objects or locations; animals, persons, groups or organizations;
<br>beliefs or other states of mind; or entities such as theories,
<br>laws or constitutions. Here we focus on concrete situations,
<br>considering other cases below. Our proposal builds on classic
<br>work in linguistics (31, 32), human cognition (33), artiﬁcial
<br>intelligence (34), and an early PDP model (35) and dovetails
<br>with an emerging perspective in cognitive neuroscience (36).
<br>As humans process language, we construct a representation
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">of the situation the language describes from the stream of
<br>words and other available information. Words and their se-
<br>quencing serve as </span><span style="font-family: LMRoman9-Italic; font-size:8px">clues to meaning </span><span style="font-family: LMRoman9-Regular; font-size:8px">(37) that jointly constrain
<br>the understanding of the situation and each object participat-
<br>ing in it (35). Consider this passage:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:328px; top:2457px; width:208px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">John spread jam on a slice of bread. The knife had
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:1908px; width:246px; height:33px;"><span style="font-family: NimbusSanL-Bold; font-size:6px">Fig. 3. </span><span style="font-family: NimbusSanL-Regu; font-size:6px">High-level depiction of one stage of the bidirectional attention architecture,
<br>shown constructing a contextually appropriate representation of </span><span style="font-family: NimbusSanL-ReguItal; font-size:6px">bank </span><span style="font-family: NimbusSanL-Regu; font-size:6px">based on other
<br>words in the same sentence. Diagonal lines show how inputs from other positions
<br>reach </span><span style="font-family: NimbusSanL-ReguItal; font-size:6px">bank </span><span style="font-family: NimbusSanL-Regu; font-size:6px">’s position; the same occurs at all word positions. See text for details.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:1964px; width:245px; height:19px;"><span style="font-family: LMRoman9-Regular; font-size:8px">toward context sensitivity (17) recognize this limitation. We
<br>consider a fuller solution in the next section.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:50px; top:1998px; width:248px; height:270px;"><span style="font-family: NimbusSanL-BoldItal; font-size:7px">Attention and fully contextualized embeddings. </span><span style="font-family: LMRoman9-Regular; font-size:8px">Breakthroughs in
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">neural language modeling have come from recent models that
<br>construct fully contextualized word representations (18–22).
<br>The models represent words via a mutual constraint satis-
<br>faction process in which each word in a text span inﬂuences
<br>the representation of every other word. BERT (20) is a key
<br>model in this class, illustrated in Fig. 3 as it encodes the end
<br>of the sentence </span><span style="font-family: LMRoman9-Italic; font-size:8px">John reached the bank of the river </span><span style="font-family: LMRoman9-Regular; font-size:8px">(example
<br>from (23)). An initial context-independent representation of
<br>each word is ﬁrst combined with a positional representation
<br>(bottom row of boxes in the ﬁgure). Then, a separate copy of
<br>the same neural network module updates the representation in
<br>each position with input from all other positions. This process
<br>uses queries (red) at each position that are compared to keys
<br>(yellow) at all positions to form weightings (mauve boxes) that
<br>determine how strongly the values (blue) from each position
<br>contribute to the combined attention vectors (grey boxes) that
<br>provide context-sensitivity. The computation iterates over
<br>many layers, allowing words whose representations have been
<br>inﬂuenced by their context to inﬂuence the representations of
<br>other words. The process allows selection among alternative
<br>distinct meanings of a word like </span><span style="font-family: LMRoman9-Italic; font-size:8px">bank </span><span style="font-family: LMRoman9-Regular; font-size:8px">as well as graded shading
<br>of word meaning by context, for example assigning diﬀerent
<br>emotional valance to the dogs in </span><span style="font-family: LMRoman9-Italic; font-size:8px">the dog wagged its tail </span><span style="font-family: LMRoman9-Regular; font-size:8px">and
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">the dog snarled</span><span style="font-family: LMRoman9-Regular; font-size:8px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:2272px; width:247px; height:194px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Key ingredients of the contemporary models are (i) </span><span style="font-family: LMRoman9-Italic; font-size:8px">bidi-
<br>rectionality </span><span style="font-family: LMRoman9-Regular; font-size:8px">of information ﬂow during processing and (ii) an
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">attention </span><span style="font-family: LMRoman9-Regular; font-size:8px">mechanism, which replaces the LSTM mechanism to
<br>enhance the exploitation of context. Bidirectionality matters
<br>because the meaning of a word in context depends on what
<br>comes after it as well as what comes before; in the example
<br>sentence, the last word, </span><span style="font-family: LMRoman9-Italic; font-size:8px">river</span><span style="font-family: LMRoman9-Regular; font-size:8px">, determines the meaning of </span><span style="font-family: LMRoman9-Italic; font-size:8px">bank</span><span style="font-family: LMRoman9-Regular; font-size:8px">.
<br>While it is remarkable how much can be done with strictly left-
<br>to-right constraint propagation (24), bidirectionality allows
<br>neural models to implement a mutual constraint satisfaction
<br>process in which the representation of each word depends on
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">all other words. The attention mechanism distinguishes these
<br>models from earlier LSTM-based neural language models, and
<br>is used in both bidirectional and left-to-right models. Atten-
<br>tion has proven to be even more eﬀective than the LSTM
<br>mechanism in allowing networks to capture long-distance de-
<br>pendencies. Rather than requiring information about a context
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">word to reach a target word through an iteration of the LSTM
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:2482px; width:277px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">McClelland et al.: Language understanding in humans and machines
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:551px; top:2482px; width:4px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">3
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:61px; top:1791px; width:226px; height:98px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:2576px; width:612px; height:792px;"></span>
<div style="position:absolute; top:2576px;"><a name="4">Page 4</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:74px; top:2635px; width:208px; height:19px;"><span style="font-family: LMRoman9-Regular; font-size:8px">been dipped in poison. John ate the bread and soon
<br>began to feel sick.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:55px; top:2661px; width:248px; height:292px;"><span style="font-family: LMRoman9-Regular; font-size:8px">We can make many inferences here: that the jam was spread
<br>with the poisoned knife, that some of the poison was trans-
<br>ferred to the bread, and that this may have led to John’s
<br>sickness. Note that the entities here are objects, not words,
<br>and the situation could instead be conveyed by a silent movie.
<br>Evidence that humans construct situation representations
<br>comes from classic work by Bransford and colleagues (33, 38).
<br>This work demonstrates that (1) we understand and remember
<br>texts better when we can relate the statements in the text
<br>to a familiar situation; (2) information that conveys aspects
<br>of the situation can be provided by a picture accompanying
<br>the text; (3) the characteristics of the objects we remember
<br>depend on the situations in which they occurred in a text;
<br>(4) we represent in memory objects not explicitly mentioned
<br>in texts; and (5) after hearing a sentence describing spatial
<br>or conceptual relationships among objects, we retain mem-
<br>ory for these relationships rather than the linguistic input.
<br>Further, evidence from eye movements shows that people use
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">linguistic and non-linguistic input jointly and immediately as
<br>they process language in context (39). For example, just after
<br>hearing </span><span style="font-family: LMRoman9-Italic; font-size:8px">The man will drink ... </span><span style="font-family: LMRoman9-Regular; font-size:8px">participants look at a full wine
<br>glass rather than an empty beer glass (40). After hearing </span><span style="font-family: LMRoman9-Italic; font-size:8px">The
<br>man drank</span><span style="font-family: LMRoman9-Regular; font-size:8px">, they look at the empty beer glass. Thus, language
<br>understanding involves constructing–in real time–a represen-
<br>tation of the situation being described by language input,
<br>including the objects involved and their spatial relationships
<br>with each other, using visual and linguistic inputs.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:55px; top:2965px; width:247px; height:281px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">The understanding system in the brain. </span><span style="font-family: LMRoman9-Regular; font-size:8px">Fig. 4 presents a de-
<br>piction of our proposed integrated understanding system. </span><span style="font-family: LMRoman9-Italic; font-size:8px">Our
<br>proposal is both a theory of the brain basis of understanding
<br>and a proposed architecture for future language understanding
<br>research. </span><span style="font-family: LMRoman9-Regular; font-size:8px">It is largely consistent with proposals in (36). First,
<br>we focus on a part of the system, called the neocortical sys-
<br>tem, that is suﬃcient to combine linguistic and non-linguistic
<br>input to understand the object and situation referred to upon
<br>hearing a sentence containing the word </span><span style="font-family: LMRoman9-Italic; font-size:8px">bat </span><span style="font-family: LMRoman9-Regular; font-size:8px">while observing
<br>a corresponding situation in the world. This system consists
<br>of the blue ovals (corresponding to pools of neurons in the
<br>brain) and blue arrows (connections between these pools) in
<br>the ﬁgure. One population subserves a visual representa-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">tion/embedding of the given situation, and another subserves
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">a non-semantic linguistic representation capturing the sound
<br>structure (phonology) of co-occurring spoken language. The
<br>third represents objects participating in the situation, and
<br>the fourth represents the overall situation itself. Within each
<br>pool, and between each connected pair of pools, the neurons
<br>are reciprocally interconnected via learning-dependent path-
<br>ways allowing mutual constraint satisfaction among all of the
<br>elements of each of the embedding types. Brain regions for
<br>representing visual and linguistic inputs are well-established,
<br>and the evidence for their involvement in a mutual constraint
<br>satisfaction process is substantial (41). Here we focus on the
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">evidence for object and situation representations in the brain.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:55px; top:3255px; width:247px; height:52px;"><span style="font-family: NimbusSanL-BoldItal; font-size:7px">Object representations. </span><span style="font-family: LMRoman9-Regular; font-size:8px">A brain area near the front of the tempo-
<br>ral lobe houses neurons whose activity provides an embedding
<br>capturing the properties of an object someone is considering
<br>(42). Damage to this area impairs the ability to name objects,
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">to grasp objects correctly in service of their intended use, to
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:2914px; width:245px; height:78px;"><span style="font-family: NimbusSanL-Bold; font-size:6px">Fig. 4. </span><span style="font-family: NimbusSanL-Regu; font-size:6px">Proposed integrated understanding system (IUS). The blue box contains
<br>the neocortical system, with each oval forming an embedding (representation) of a
<br>speciﬁc kind of information. Blue arrows represent learned connections that allow
<br>the embeddings to constrain each other. The red box contains the medial temporal
<br>lobe system, thought to provide a network that stores an integrated embedding of the
<br>neocortical system state. The red arrow represents fast-learning connections that
<br>bind the elements of this embedding together for later reactivation and use. Green
<br>arrows connecting the red and blue ovals support bidirectional inﬂuences between
<br>the two systems. (A) and (B) are two example inputs discussed in the main text.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:3015px; width:246px; height:150px;"><span style="font-family: LMRoman9-Regular; font-size:8px">match objects with their names or the sounds that they make,
<br>and to pair objects that go together with each other, either
<br>from their names or from pictures. Models that capture these
<br>ﬁndings (43) treat this area as the hidden layer of an interac-
<br>tive, recurrent network with bidirectional connections to other
<br>layers corresponding to brain areas that represent diﬀerent
<br>types of object properties including the object’s name. In these
<br>models, an input to any of these other layers activates the cor-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">responding pattern in the hidden layer, which in turn activates
<br>the corresponding patterns in the other layers, supporting, for
<br>example, the ability to produce the name of an object from
<br>visual input. Damage (simulated by removal of neurons in the
<br>hidden layer) degrades the model’s representations, capturing
<br>the patterns of errors made by patients with the condition.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:3179px; width:246px; height:128px;"><span style="font-family: NimbusSanL-BoldItal; font-size:7px">Situation representations. </span><span style="font-family: LMRoman9-Regular; font-size:8px">The situation representation speciﬁes
<br>the event or situation conveyed by visual and/or language
<br>input. Evidence from behavioral studies indicates that con-
<br>struction of a situation representation can occur with or with-
<br>out language input (44) or through the convergent inﬂuence
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">of both sources of information (40). Cognitive neuroscience
<br>research supports the idea that the situation representation
<br>arises in a set of interconnected brain areas primarily located
<br>in the frontal and parietal lobes (36, 45). In recent work, brain
<br>imaging data is used to analyze the time-varying patterns
<br>of neural activity that arise during the processing of a tem-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">porally extended event sequence. The activity patterns that
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:3324px; width:277px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">McClelland et al.: Language understanding in humans and machines
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:555px; top:3324px; width:4px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">4
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:324px; top:2633px; width:226px; height:262px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:3418px; width:612px; height:792px;"></span>
<div style="position:absolute; top:3418px;"><a name="5">Page 5</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:3477px; width:246px; height:41px;"><span style="font-family: LMRoman9-Regular; font-size:8px">represent corresponding events in a sequence are largely the
<br>same, whether the information about the sequence comes from
<br>watching a movie, hearing or reading a narrative description,
<br>or recalling the movie after having seen it (46, 47).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:3529px; width:247px; height:139px;"><span style="font-family: NimbusSanL-BoldItal; font-size:7px">Situation-speciﬁc constraints. </span><span style="font-family: LMRoman9-Regular; font-size:8px">An important feature of our brain-
<br>inspired proposal is the use of distinct situation and object
<br>representations, and the idea that the constraints on the par-
<br>ticipating objects are mediated by the situation representation.
<br>The advantage of this is that it allows these constraints to
<br>be situation-speciﬁc. For example, the dogs in the events de-
<br>scribed by the sentences </span><span style="font-family: LMRoman9-Italic; font-size:8px">the boy ran </span><span style="font-family: LMRoman10-BoldItalic; font-size:8px">to </span><span style="font-family: LMRoman9-Italic; font-size:8px">the dog </span><span style="font-family: LMRoman9-Regular; font-size:8px">and </span><span style="font-family: LMRoman9-Italic; font-size:8px">the boy ran
<br></span><span style="font-family: LMRoman10-BoldItalic; font-size:8px">from </span><span style="font-family: LMRoman9-Italic; font-size:8px">the dog </span><span style="font-family: LMRoman9-Regular; font-size:8px">are likely to be diﬀerent, and a comprehender will
<br>represent them diﬀerently (38). In general, context-sensitivity
<br>is best captured by a mediating representation rather than
<br>direct associations among constituents (48). While BERT-like
<br>models might partially capture such constraints implicitly, an
<br>integrated situation representation may be more eﬀective.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:3671px; width:247px; height:205px;"><span style="font-family: LMRoman9-Regular; font-size:8px">In summary, the brain contains distinct areas that represent
<br>each input modality and the objects and situations conveyed
<br>through them, computing these representations through a
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">mutual constraint satisfaction process combining language and
<br>other inputs. Emulating this architecture in machines could
<br>contribute to achieving human-level language understanding.
<br>What would a computational instantiation of our system look
<br>like? It is likely that a biologically realistic version would
<br>diﬀer in some ways from the most eﬀective machine version.
<br>Contemporary attention-based language models can deploy
<br>attention over tens to thousands of words kept in their current
<br>system state, but evidence from brain imaging data collected
<br>during movie comprehension suggests that activation states in
<br>visual, speech, and object areas change rapidly as events unfold,
<br>while the brain state tends to be more constant, changing only
<br>at event boundaries in brain areas associated with situation
<br>representations (47). In humans, spanning longer temporal
<br>windows, including multi-event narratives, appears to require
<br>the complementary learning system we consider next.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:3890px; width:247px; height:260px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">Complementary Learning Systems. </span><span style="font-family: LMRoman9-Regular; font-size:8px">Learning plays a crucial
<br>role in understanding. The knowledge in the connection
<br>weights in the neural networks we have described is acquired
<br>through the accumulation of very small adjustments based on
<br>each experience. The connection weights gradually become
<br>sensitive to subtle higher-order statistical relationships, taking
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">more and more context into account as learning continues (49),
<br>and exhibiting sensitivity both to general and recurring speciﬁc
<br>information (e.g., names of close friends and famous people).
<br>In our proposed architecture, this gradual process occurs in
<br>all the pathways represented by the blue arrows in Fig. 4, just
<br>as it does in the artiﬁcial neural language models considered
<br>above. However, this learning mechanism is not well suited to
<br>acquiring new information rapidly, and attempting to learn
<br>speciﬁc new information quickly by focused repetition leads
<br>to catastrophic interference with what is already known (50).
<br>Yet, humans can often rely on information presented just
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">once at an arbitrary time in the past to inform our current
<br>understanding. Returning to the beer John left in the cooler,
<br>to anticipate that John will not ﬁnd the beer when he opens
<br>the cooler again, we must rely on information acquired when
<br>we ﬁrst heard about the beer being stolen. Such situations
<br>are ubiquitous, and a learning system must be able to exploit
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">such information, but BERT and the other models described
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:3477px; width:245px; height:52px;"><span style="font-family: LMRoman9-Regular; font-size:8px">previously are limited in this way. Though some models hold
<br>long word sequences in an active state, when one text is
<br>replaced with another, only the small connection adjustments
<br>described above remain, leaving these systems without access
<br>to the speciﬁcs of the prior information.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:3531px; width:246px; height:172px;"><span style="font-family: LMRoman9-Regular; font-size:8px">The human brain contains a system that addresses this
<br>limitation. Consider a situation in which someone sees a pre-
<br>viously unfamiliar object and hears a spoken statement about
<br>it, as illustrated in Fig. 4B. The visual input provides one
<br>source of information about the object (a previously unfamiliar
<br>animal), while the linguistic input provides its name. Humans
<br>show robust learning after just two brief exposures to such
<br>pairings (51). This form of learning depends on the hippocam-
<br>pus and adjacent areas in the medial temporal lobes (MTL)
<br>of the brain (51). While details of the role of the MTL in
<br>learning and memory continue to be debated (52, 53), there is
<br>consensus that the MTL is crucial for the initial formation of
<br>new memories, including memories for speciﬁc events and their
<br>constituent objects and situations, while general knowledge,
<br>the ability to understand language, and previously acquired
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">skills are unaﬀected by MTL damage.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:3706px; width:246px; height:107px;"><span style="font-family: LMRoman9-Regular; font-size:8px">The evidence from MTL damage suggests there is a fast
<br>learning system in the MTL. According to complementary
<br>learning systems theory (CLST) (54–56) this system (shown
<br>in red in Fig. 4) provides an integrated representation of the
<br>understanding system state, and employs modiﬁable connec-
<br>tions within the MTL (red arrow) that can change rapidly to
<br>support new learning based on a single experience. The green
<br>arrows represent connections that carry information between
<br>the neocortical (blue) and MTL (red) systems so the systems
<br>can inﬂuence each other.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:3816px; width:246px; height:238px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Let us consider how, according to CLST, a human can learn
<br>about the numbat (see Fig. 4B) from an experience seeing
<br>it and hearing a sentence about it (56). The input to the
<br>MTL is thought to be an embedding that captures (i.e., can
<br>be used to reconstruct) the patterns in the neocortical areas
<br>that arise from the experience. Networks within the MTL (not
<br>shown) map the MTL input representation to a sparser one
<br>deep inside the MTL, maximizing distinctness and minimizing
<br>interference among experiences (54). Large connection weight
<br>changes within the MTL associate the elements of the sparse
<br>representation with each other and with the MTL input repre-
<br>sentation. When the person hears the word </span><span style="font-family: LMRoman9-Italic; font-size:8px">numbat </span><span style="font-family: LMRoman9-Regular; font-size:8px">in a later
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">situation, connections to the MTL from the neocortex activate
<br>neurons in the MTL input representation. The weight changes
<br>that occurred on prior exposure support the reconstruction
<br>of the complete MTL representation, and return connections
<br>to the neocortex then support approximate reconstruction of
<br>the visual, speech, object and situation representations formed
<br>during the initial exposure to the numbat. These representa-
<br>tions are the explicit memory for the prior experience, allowing
<br>the cortical network to use what it learned from one prior
<br>exposure to contribute to understanding the new situation.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:4065px; width:246px; height:85px;"><span style="font-family: NimbusSanL-BoldItal; font-size:7px">Integrating information into the neocortex. </span><span style="font-family: LMRoman9-Regular; font-size:8px">How can knowledge ini-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">tially dependent on the MTL be integrated into the neocor-
<br>tex? According to CLST (55), the neocortex learns gradually
<br>through interleaved presentations of new and familiar items;
<br>this process avoids interference of new items with what is al-
<br>ready known. Interleaved learning can occur through ongoing
<br>experience, as would happen if, for example, we acquire a pet
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">numbat that we then see every day, while continuing to have
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:4166px; width:277px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">McClelland et al.: Language understanding in humans and machines
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:551px; top:4166px; width:4px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">5
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:4260px; width:612px; height:792px;"></span>
<div style="position:absolute; top:4260px;"><a name="6">Page 6</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:4319px; width:246px; height:162px;"><span style="font-family: LMRoman9-Regular; font-size:8px">other experiences. Interleaving may also occur during rest
<br>or sleep through reactivation and replay of patterns stored
<br>in the MTL: Indeed, spontaneous replay of short snippets of
<br>previously experienced episodes occurs within the MTL during
<br>sleep and between behavioral episodes (see (56) for review).
<br>In summary, the human brain contains complementary
<br>learning systems that support the simultaneous use of many
<br>sources of information as we seek to understand an experienced
<br>situation. One of these systems acquires an integrated system
<br>of knowledge gradually through interleaved learning, includ-
<br>ing our knowledge of the meanings of words, the properties
<br>of frequently-encountered objects, and the characteristics of
<br>familiar situations. The other complements this system to
<br>allow information from speciﬁc experiences to be brought to
<br>bear on the interpretation of a current situation.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:4497px; width:240px; height:9px;"><span style="font-family: NimbusSanL-Bold; font-size:9px">Toward an Artiﬁcial Integrated Understanding System
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:4513px; width:246px; height:85px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Here we consider current deep learning research that is taking
<br>steps consistent with our IUS proposal, and point toward fu-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">ture directions that will be needed to achieve a truly integrated
<br>and fully functional understanding system. We begin within
<br>the context of language grounded within concrete visual and
<br>physical situations, then consider the role of memory, and
<br>ﬁnally turn to the extension of the approach to address under-
<br>standing of more abstract objects, situations, and relations.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:55px; top:4612px; width:247px; height:346px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">Mapping vision and language to representations of objects.
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">How might a model learn about situations that can occur
<br>in the world? The need for an artiﬁcial system of language
<br>understanding to be grounded in the external world has long
<br>been discussed. An early example is Winograd’s SHRDLU
<br>system (57), which produced and responded to language about
<br>a simulated physical world. Deep learning has enabled </span><span style="font-family: LMRoman9-Italic; font-size:8px">joint</span><span style="font-family: LMRoman9-Regular; font-size:8px">,
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">end-to-end </span><span style="font-family: LMRoman9-Regular; font-size:8px">training of perceptual input and language (i.e., in
<br>a single synchronous optimization process). Recent advances
<br>with such models have greatly improved performance, resulting
<br>in applications transforming user experiences. When presented
<br>with a photograph, networks can now answer questions such
<br>as </span><span style="font-family: LMRoman9-Italic; font-size:8px">what is the man holding? </span><span style="font-family: LMRoman9-Regular; font-size:8px">or </span><span style="font-family: LMRoman9-Italic; font-size:8px">what color is the woman’s
<br>shirt? </span><span style="font-family: LMRoman9-Regular; font-size:8px">(58), demonstrating an ability to combine information
<br>from vision and language to understand a class of situations.
<br>A very recent model (59) explicitly represents the objects in
<br>a scene, their properties, and their relations to other objects in
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">a designed </span><span style="font-family: LMRoman9-Italic; font-size:8px">scene graph </span><span style="font-family: LMRoman9-Regular; font-size:8px">with slots for objects, their properties,
<br>and relations. It encodes questions as a series of instructions
<br>to ﬁnd a target object or relation by searching the graph to
<br>answer a query. For example, the question </span><span style="font-family: LMRoman9-Italic; font-size:8px">what is the object
<br>beside the yellow bowl? </span><span style="font-family: LMRoman9-Regular; font-size:8px">can be answered by ﬁnding the yellow
<br>bowl, ﬁnding an object linked to it with the ‘beside’ relation,
<br>and then reading out this object’s identity. The approach
<br>advances the state of the art, though a large gap relative
<br>to humans remains. The model shares important properties
<br>with our proposal in that it explicitly treats language input
<br>as querying the model’s representation of the objects in the
<br>scene, and their conceptual properties. A natural extension
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">consistent with IUS would be to build up scene representations
<br>using a combination of visual input and language, allowing
<br>text to enrich the representations of objects and relations.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:4961px; width:246px; height:30px;"><span style="font-family: LMRoman9-Regular; font-size:8px">A question this work raises is whether to build structured
<br>representations into one’s model. This is advocated in (59),
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">but natural structure exhibits ﬂexible embedding relationships
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:4319px; width:247px; height:63px;"><span style="font-family: LMRoman9-Regular; font-size:8px">and is often only approximately characterized by explicit tax-
<br>onomies, motivating use of emergent connection-based rather
<br>than hard-coded representational structures (60). A challenge,
<br>then, is to achieve comparable performance with models in
<br>which these concept-based object and relational representa-
<br>tions emerge through learning.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:4394px; width:245px; height:96px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">Embodied models for language understanding. </span><span style="font-family: LMRoman9-Regular; font-size:8px">Beyond the
<br>integration of vision and language, as illustrated in Fig. 4, we
<br>see progress coming from an even fuller integration of many
<br>additional information sources. Every source provides a basis
<br>for distinct learning objectives and enables information that
<br>is salient in one source to bootstrap learning and inference in
<br>the other. Important additional sources of information include
<br>non-language sound, touch and force-sensing, and information
<br>about one’s own actions.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:4492px; width:246px; height:128px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Incorporating additional information sources will allow an
<br>IUS to go beyond answering questions about static images.
<br>Since image data has no temporal aspect, such models lack
<br>experience of events or processes. While models that jointly
<br>process video and language (61) may acquire some sensitiv-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">ity to event structure and commonplace causal relationships,
<br>these systems do not make choices aﬀecting the world they
<br>observe. Ultimately, an ability to link one’s actions to their
<br>consequences as one intervenes in the observed ﬂow of events
<br>and interacts with other agents should provide the strongest
<br>basis for acquiring notions of cause and eﬀect, of agency, and
<br>of self and others.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:313px; top:4623px; width:247px; height:248px;"><span style="font-family: LMRoman9-Regular; font-size:8px">These considerations motivate recent work on agent-
<br>based language learning in simulated interactive 3D environ-
<br>ments (62–65). In (66), an agent was trained to identify, lift,
<br>carry and place objects relative to other objects in a virtual
<br>room, as speciﬁed by simpliﬁed language instructions. At each
<br>time step, the agent received a ﬁrst-person visual observation
<br>(pixel-based image) that it processed to produce a representa-
<br>tion of the scene. This was concatenated to the ﬁnal state of
<br>an LSTM that processed the instruction, then passed to an
<br>integrative LSTM whose output was used to select a motor
<br>action. The agent gradually learned to follow instructions of
<br>the form </span><span style="font-family: LMRoman9-Italic; font-size:8px">ﬁnd a pencil</span><span style="font-family: LMRoman9-Regular; font-size:8px">,
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">lift up a basketball </span><span style="font-family: LMRoman9-Regular; font-size:8px">and </span><span style="font-family: LMRoman9-Italic; font-size:8px">put the teddy
<br>bear on the bed</span><span style="font-family: LMRoman9-Regular; font-size:8px">, encompassing 50 objects, and requiring up
<br>to 70 action steps to complete. Such instructions require the
<br>construction of representations based on language stimuli that
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">enable the identiﬁcation of objects and relations across space
<br>and time, and the integration of this information to inform mo-
<br>tor behaviors. Importantly, without building in explicit object
<br>representations, the system supported the interpretation of
<br>novel instructions. For instance, an agent trained to lift a set
<br>of 20 objects in the environment, but only trained to put 10 of
<br>those in a speciﬁc location could place the remaining objects
<br>in the same location on command with over 90% accuracy.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:4874px; width:247px; height:118px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Neural models often fail to exhibit systematic generaliza-
<br>tion, leading some to propose that more structure should be
<br>built in (67). While the agent’s level of systematicity does
<br>not reach human levels, this work suggests that grounding lan-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">guage learning can help support systematicity </span><span style="font-family: LMRoman9-Italic; font-size:8px">without building
<br>it in</span><span style="font-family: LMRoman9-Regular; font-size:8px">. Critically, the agent’s systematicity was contingent on
<br>the ego-centric, multimodal and temporally-extended expe-
<br>rience of the agent. On the same set of generalization tests,
<br>both an alternative agent with a ﬁxed perspective on a 2D grid
<br>world and a static neural network classiﬁer that received only
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">individual still image stimuli exhibited signiﬁcantly worse gen-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:5008px; width:277px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">McClelland et al.: Language understanding in humans and machines
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:555px; top:5008px; width:4px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">6
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:5102px; width:612px; height:792px;"></span>
<div style="position:absolute; top:5102px;"><a name="7">Page 7</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:5161px; width:245px; height:30px;"><span style="font-family: LMRoman9-Regular; font-size:8px">tions under consideration can only be communicated through
<br>language. What is the way forward toward developing models
<br>that can understand such a complex situation?
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:5193px; width:247px; height:172px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Language may have evolved in part to support transmission
<br>of complex, hierarchical knowledge about tools (71). How-
<br>ever, utterances also had to support abstraction and complex
<br>dependencies between concrete objects, as well as social re-
<br>lationships between speakers. Words themselves provided a
<br>new abstract substrate for characterizing other words (72).
<br>Word embeddings are one implementation of this substrate:
<br>they can characterize abstract words like </span><span style="font-family: LMRoman9-Italic; font-size:8px">justice </span><span style="font-family: LMRoman9-Regular; font-size:8px">and </span><span style="font-family: LMRoman9-Italic; font-size:8px">represents
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">without directly grounding them. In humans, encyclopedic
<br>knowledge grounded in such representations can be acquired
<br>in an MTL-dependent way from reading an encyclopedia arti-
<br>cle just once. For machines, forming integrated system state
<br>representations capturing the content and using a DNC-like
<br>system for their storage and retrieval might provide a starting
<br>place for enabling such knowledge to be acquired and used
<br>eﬀectively.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:309px; top:5368px; width:247px; height:194px;"><span style="font-family: LMRoman9-Regular; font-size:8px">That said, words are uttered in real world contexts and there
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">is a continuum between grounding and language-based linking
<br>for diﬀerent words and diﬀerent uses of words. For example,
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">career </span><span style="font-family: LMRoman9-Regular; font-size:8px">is not only linked to other abstract words like </span><span style="font-family: LMRoman9-Italic; font-size:8px">work </span><span style="font-family: LMRoman9-Regular; font-size:8px">and
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">specialization </span><span style="font-family: LMRoman9-Regular; font-size:8px">but also to more grounded concepts such as </span><span style="font-family: LMRoman9-Italic; font-size:8px">path
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">and its extended metaphorical use for discussing the means to
<br>achieve goals (72). Embodied, simulation-based approaches
<br>to meaning (73, 74) build on this observation to bridge from
<br>concrete to abstract situations via metaphor. They posit
<br>that understanding words like </span><span style="font-family: LMRoman9-Italic; font-size:8px">grasp </span><span style="font-family: LMRoman9-Regular; font-size:8px">is directly linked to neural
<br>representations of the action of grabbing and that this circuitry
<br>is recruited for understanding the word in contexts such as
<br></span><span style="font-family: LMRoman9-Italic; font-size:8px">grasping an idea</span><span style="font-family: LMRoman9-Regular; font-size:8px">. We consider situated agents as a critical
<br>catalyst for learning about how to represent and compose
<br>concepts pertaining to spatial, physical and other perceptually
<br>immediate phenomena—thereby providing a grounded ediﬁce
<br>that can connect to both the low level brain circuitry for motor
<br>action and to representations derived primarily from language.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:5574px; width:247px; height:194px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">Conclusion. </span><span style="font-family: LMRoman9-Regular; font-size:8px">Language does not stand alone. The integrated
<br>understanding system in the brain connects language to rep-
<br>resentations of objects and situations and enhances language
<br>understanding by exploiting the full range of our multi-sensory
<br>experience of the world, our representations of our motor
<br>actions, and our memory of previous situations. We have ar-
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">gued that the next generation language understanding system
<br>should emulate this system in the brain and we have sketched
<br>some aspects of the form such a system might take. While
<br>we have emphasized understanding of concrete situations, we
<br>have argued that understanding more abstract language builds
<br>upon this concrete foundation, pointing toward the possibility
<br>that it may someday be possible to build artiﬁcial systems that
<br>understand abstract situations far beyond the concrete and
<br>the here-and-now. In sum, we have proposed that modeling
<br>the integrated understanding system in the brain will take us
<br>closer to capturing human-level language understanding and
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">intelligence.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:310px; top:5781px; width:246px; height:35px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">ACKNOWLEDGMENTS. </span><span style="font-family: LMRoman8-Regular; font-size:7px">This article grew out of a workshop
<br>organized by HS at </span><span style="font-family: LMRoman8-Italic; font-size:7px">Meaning in Context 3</span><span style="font-family: LMRoman8-Regular; font-size:7px">, Stanford University,
<br>September 2017. We thank Chris Potts for discussion. HS was
<br>supported by ERC Advanced Grant #740516.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:5289px; width:245px; height:33px;"><span style="font-family: NimbusSanL-Bold; font-size:6px">Fig. 5. </span><span style="font-family: NimbusSanL-Regu; font-size:6px">Left: the (allocentric) agent perspective in a 2D grid-world. The text indicates
<br>the language instruction, requiring the agent (white striped cell) to visit one of the
<br>red ﬁgures and move it to the white square. Right: the ﬁrst-person perspective of the
<br>situated agent, addressing an equivalent task to the one posed in the grid world.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:5344px; width:245px; height:41px;"><span style="font-family: LMRoman9-Regular; font-size:8px">eralization that a fully situated agent (Fig. 5). This underlines
<br>how aﬀording neural networks access to rich, multi-modal
<br>interactive environments can stimulate the development of
<br>capacities that are essential for language learning.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:5388px; width:246px; height:63px;"><span style="font-family: LMRoman9-Regular; font-size:8px">Despite these promising signs, achieving fully human levels
<br>of generalization remains an important challenge. We propose
<br>that incorporating an MTL-like fast learning system will help
<br>address this by allowing new words to be linked to the corre-
<br>sponding object from just a single episode supporting use of
<br>the word to refer to the referent in other situations.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:50px; top:5465px; width:248px; height:281px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">An artiﬁcial fast learning system. </span><span style="font-family: LMRoman9-Regular; font-size:8px">What might a fast learning
<br>system in an implementation of an integrated understanding
<br>system look like? The memory system in the diﬀerentiable
<br>neural computer (DNC) (68) is one possibility. These systems
<br>store embeddings derived from past episodes in slots that
<br>could store Integrated System State representations like those
<br>we attribute to the human MTL. Alternatively, they could
<br>store the entire ensemble of states across the visual, speech,
<br>object, and situation representations. Though we do not
<br>believe the brain has a separate slot for each memory, it can
<br>be useful to model it as though it does (56), and artiﬁcial
<br>systems with indeﬁnite capacity could exceed human abilities
<br>in this regard. How might the retrieval of relevant information
<br>work in such a system? The DNC employs a querying system
<br>similar to the one in BERT and to proposals in the human
<br>memory literature, whereby the representation retrieved from
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">the MTL is weighted by the degree of match between a query
<br>(which we would treat as coming from the neocortex) and each
<br>vector stored in memory. Close matches are favored in this
<br>computation (69), so that when there is a unique match (such
<br>as a single memory containing a once seen word like </span><span style="font-family: LMRoman9-Italic; font-size:8px">numbat</span><span style="font-family: LMRoman9-Regular; font-size:8px">),
<br>the corresponding object and situation representation could
<br>be retrieved. Retrieval could be based on a combination of
<br>context and item information, similar to human memory (70).
<br>Working out the details of such a system presents an exciting
<br>research direction for the future.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:5760px; width:246px; height:74px;"><span style="font-family: NimbusSanL-Bold; font-size:8px">Beyond concrete situations. </span><span style="font-family: LMRoman9-Regular; font-size:8px">Our discussion has focused pri-
<br>marily on concrete situations. However, language allows us to
<br>discuss abstract ideas and situations, where grounding in the
<br>physical world can be very indirect and our learning about
<br>it comes primarily from language-based materials. Consider,
<br>for example, an understanding of </span><span style="font-family: LMRoman9-Italic; font-size:8px">Brexit</span><span style="font-family: LMRoman9-Regular; font-size:8px">. Concrete events
<br></span><span style="font-family: LMRoman9-Regular; font-size:8px">involving actual people have occurred, but the issues and ques-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:51px; top:5850px; width:277px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">McClelland et al.: Language understanding in humans and machines
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:551px; top:5850px; width:4px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">7
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:53px; top:5159px; width:240px; height:110px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:5944px; width:612px; height:792px;"></span>
<div style="position:absolute; top:5944px;"><a name="8">Page 8</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:61px; top:6005px; width:240px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">1. Wu Y, et al. (2016) Google’s neural machine translation system: Bridging the gap between
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6013px; width:86px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">human and machine translation. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">-</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:328px; top:6005px; width:232px; height:13px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">world knowledge: Linking anticipatory (and other) eye movements to linguistic processing.
<br></span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Journal of Memory and Language </span><span style="font-family: NimbusSanL-Regu; font-size:5px">57(4):502–518.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:61px; top:6021px; width:241px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">2. Rosenblatt F (1961) </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Principles of neurodynamics. perceptrons and the theory of brain mech-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6021px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">41. McClelland JL, Mirman D, Bolger DJ, Khaitan P (2014) Interactive activation and mutual con-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6029px; width:157px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">anisms</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. (Spartan Books, Cornell University, Ithaca, New York).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6029px; width:209px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">straint satisfaction in perception and cognition. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Cognitive science </span><span style="font-family: NimbusSanL-Regu; font-size:5px">38(6):1139–1189.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:61px; top:6037px; width:241px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">3. Rumelhart DE, McClelland JL, the PDP research group (1986) </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Parallel Distributed Process-
<br>ing. Explorations in the Microstructure of Cognition. Volume 1: Foundations</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. (The MIT Press,
<br>Cambridge, MA).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:61px; top:6061px; width:241px; height:29px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">4. Rumelhart DE, McClelland JL (1986) On learning the past tenses of English verbs in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Parallel
<br>Distributed Processing. Explorations in the Microstructure of Cognition. Volume 2: Psycho-
<br>logical and Biological Models</span><span style="font-family: NimbusSanL-Regu; font-size:5px">, eds. McClelland JL, Rumelhart DE, the PDP Research Group.
<br>(MIT Press), pp. 216–271.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6037px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">42. Patterson K, Nestor PJ, Rogers TT (2007) Where do you know what you know? the represen-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6045px; width:229px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">tation of semantic knowledge in the human brain. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Nature Reviews Neuroscience </span><span style="font-family: NimbusSanL-Regu; font-size:5px">8(12):976.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6053px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">43. Rogers TT, et al. (2004) Structure and deterioration of semantic memory: a neuropsycholog-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6061px; width:188px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">ical and computational investigation. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Psychological review </span><span style="font-family: NimbusSanL-Regu; font-size:5px">111(1):205–235.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6069px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">44. Zwaan RA, Radvansky GA (1998) Situation models in language comprehension and memory.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6077px; width:97px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Psychological bulletin </span><span style="font-family: NimbusSanL-Regu; font-size:5px">123(2):162–185.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6085px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">45. Ranganath C, Ritchey M (2012) Two cortical systems for memory-guided behaviour. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Nature
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:61px; top:6093px; width:241px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">5. Rumelhart DE (1977) Toward an interactive model of reading in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Attention &amp; Performance VI</span><span style="font-family: NimbusSanL-Regu; font-size:5px">,
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6093px; width:117px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Reviews Neuroscience </span><span style="font-family: NimbusSanL-Regu; font-size:5px">13:713. Review Article.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6101px; width:120px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">ed. Dornic S. (LEA, Hillsdale, NJ), pp. 573–603.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:61px; top:6109px; width:240px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">6. McClelland JL, Rumelhart DE (1981) An interactive activation model of context effects in letter
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6117px; width:188px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">perception: I. an account of basic ﬁndings. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Psychological review </span><span style="font-family: NimbusSanL-Regu; font-size:5px">88(5):375.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6101px; width:244px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">46. Zadbood A, Chen J, Leong Y, Norman K, Hasson U (2017) How we transmit memories to
<br>other brains: Constructing shared neural representations via communication. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Cerebral Cor-
<br>tex </span><span style="font-family: NimbusSanL-Regu; font-size:5px">27(10):4988–5000.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:61px; top:6125px; width:241px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">7. Hopﬁeld JJ (1982) Neural networks and physical systems with emergent collective computa-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6125px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">47. Baldassano C, et al. (2017) Discovering event structure in continuous narrative perception
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6133px; width:207px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">tional abilities. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Proceedings of the national academy of sciences </span><span style="font-family: NimbusSanL-Regu; font-size:5px">79(8):2554–2558.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6133px; width:93px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">and memory. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Neuron </span><span style="font-family: NimbusSanL-Regu; font-size:5px">95(3):709–721.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:61px; top:6141px; width:241px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">8. Ackley DH, Hinton GE, Sejnowski TJ (1985) A learning algorithm for boltzmann machines.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6141px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">48. Hinton GE (1981) Implementing semantic networks in parallel hardware in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Parallel Models of
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6149px; width:81px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Cognitive science </span><span style="font-family: NimbusSanL-Regu; font-size:5px">9(1):147–169.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6149px; width:192px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Associative Memory</span><span style="font-family: NimbusSanL-Regu; font-size:5px">, eds. Hinton GE, Anderson JA. (Erlbaum), pp. 161–187.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:61px; top:6157px; width:195px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">9. Elman JL (1990) Finding structure in time. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Cognitive Science </span><span style="font-family: NimbusSanL-Regu; font-size:5px">14:179–211.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6157px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">49. Cleeremans A, McClelland JL (1991) Learning the structure of event sequences. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Journal of
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6165px; width:243px; height:13px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">10. Gold EM (1967) Language identiﬁcation in the limit. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Information and control </span><span style="font-family: NimbusSanL-Regu; font-size:5px">10(5):447–474.
<br>11. Elman JL (1991) Distributed representations, simple recurrent networks, and grammatical
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6181px; width:100px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">structure. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Mach. Learn. </span><span style="font-family: NimbusSanL-Regu; font-size:5px">7(2/3):195–225.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6189px; width:243px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">12. Hochreiter S, Bengio Y, Frasconi P, Schmidhuber J (2001) Gradient ﬂow in recurrent nets: the
<br>difﬁculty of learning long-term dependencies in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">A Field Guide to Dynamical Recurrent Neural
<br>Networks</span><span style="font-family: NimbusSanL-Regu; font-size:5px">, eds. Kremer SC, Kolen JF. (IEEE Press).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6165px; width:118px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Experimental Psychology: General </span><span style="font-family: NimbusSanL-Regu; font-size:5px">120(3):235.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6173px; width:244px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">50. McCloskey M, Cohen NJ (1989) Catastrophic interference in connectionist networks: The
<br>sequential learning problem in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Psychology of learning and motivation</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. (Elsevier) Vol. 24, pp.
<br>109–165.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6197px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">51. Warren DE, Duff MC (2019) Fast mappers, slow learners: Word learning without hippocam-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6205px; width:212px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">pus is slow and sparse irrespective of methodology. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Cognitive neuroscience </span><span style="font-family: NimbusSanL-Regu; font-size:5px">pp. 1–3.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6213px; width:183px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">13. Hochreiter S, Schmidhuber J (1997) Long short-term memory.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:250px; top:6213px; width:51px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Neural computation
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6213px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">52. Squire LR (1992) Memory and the hippocampus: a synthesis from ﬁndings with rats, mon-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6220px; width:41px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">9(8):1735–1780.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6228px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">14. Collobert R, et al. (2011) Natural language processing (almost) from scratch. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Journal of
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6236px; width:124px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Machine Learning Research </span><span style="font-family: NimbusSanL-Regu; font-size:5px">12(Aug):2493–2537.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6244px; width:244px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">15. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013) Distributed representations of
<br>words and phrases and their compositionality in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Neural Information Processing Systems</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. pp.
<br>3111–3119.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6268px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">16. Simpson GB (1981) Meaning dominance and semantic context in the processing of lexical
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6276px; width:182px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">ambiguity. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Journal of verbal learning and verbal behavior </span><span style="font-family: NimbusSanL-Regu; font-size:5px">20(1):120–136.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6284px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">17. Rudolph M, Ruiz F, Blei D (2017) Structured embedding models for grouped data in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Advances
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6292px; width:107px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">in Neural Information Processing Systems</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6300px; width:244px; height:13px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">18. Peters ME, et al. (2018) Deep contextualized word representations. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">CoRR </span><span style="font-family: NimbusSanL-Regu; font-size:5px">abs/1802.05365.
<br>19. Vaswani A, et al. (2017) Attention is all you need in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Advances in Neural Information Process-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6316px; width:147px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">ing Systems 30</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. (Curran Associates, Inc.), pp. 5998–6008.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6324px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">20. Devlin J, Chang MW, Lee K, Toutanova K (2018) BERT: Pre-training of Deep Bidirectional
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6332px; width:145px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">Transformers for Language Understanding. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">ArXiv e-prints</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6220px; width:129px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">keys, and humans. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Psychological review </span><span style="font-family: NimbusSanL-Regu; font-size:5px">99(2):195.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6228px; width:243px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">53. Yonelinas A, Ranganath C, Ekstrom A, Wiltgen B (2019) A contextual binding theory of
<br>episodic memory: systems consolidation reconsidered. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Nature Reviews Neuroscience</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br>54. Marr D (1971) Simple memory: a theory for archicortex. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Philosophical Transactions of the
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6252px; width:161px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Royal Society of London B: Biological Sciences </span><span style="font-family: NimbusSanL-Regu; font-size:5px">262(841):23–81.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6260px; width:243px; height:13px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">55. McClelland JL, McNaughton BL, O’Reilly RC (1995) Why there are complementary learning
<br>Insights from the successes and failures of
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6268px; width:243px; height:37px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">systems in the hippocampus and neocortex:
<br>connectionist models of learning and memory. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Psychological Review </span><span style="font-family: NimbusSanL-Regu; font-size:5px">102(3):419–457.
<br>56. Kumaran D, Hassabis D, McClelland JL (2016) What learning systems do intelligent agents
<br>need? Complementary learning systems theory updated. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Trends in Cognitive Sciences
<br></span><span style="font-family: NimbusSanL-Regu; font-size:5px">20(7):512–534.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6308px; width:244px; height:29px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">57. Winograd T (1972) Understanding natural language. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Cognitive psychology </span><span style="font-family: NimbusSanL-Regu; font-size:5px">3(1):1–191.
<br>58. MacLeod H, Bennett CL, Morris MR, Cutrell E (2017) Understanding blind people’s experi-
<br>ences with computer-generated captions of social media images in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Proceedings of the 2017
<br>CHI Conference on Human Factors in Computing Systems</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. (ACM), pp. 5988–5999.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6340px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">21. Yang Z, et al. (2019) Xlnet: Generalized autoregressive pretraining for language understand-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6340px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">59. Hudson DA, Manning CD (2019) Learning by abstraction: The neural state machine. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6356px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">22. Dehghani M, Gouws S, Vinyals O, Uszkoreit J, Kaiser L (2018) Universal transformers. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">CoRR
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6348px; width:70px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">ing. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">CoRR </span><span style="font-family: NimbusSanL-Regu; font-size:5px">abs/1906.08237.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6364px; width:42px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">abs/1807.03819.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6372px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">23. Uszkoreit J (2017) Transformer: A novel neural network architecture for language understand-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6380px; width:194px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">ing, https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6388px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">24. Radford A, Narasimhan K, Salimans T, Sutskever I (2018) Improving language understanding
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6396px; width:110px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">by generative pre-training. OpenAI Preprint.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6348px; width:67px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">preprint arXiv:1907.03950</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6356px; width:243px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">60. Rumelhart DE, Smolensky P, McClelland JL, Hinton G (1986) Pdp models of schemata and
<br>sequential thought processes in pdp models. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Parallel distributed processing: Explorations in
<br>the microstructure of cognition </span><span style="font-family: NimbusSanL-Regu; font-size:5px">2:3–57.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6380px; width:243px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">61. Yu H, Wang J, Huang Z, Yang Y, Xu W (2016) Video paragraph captioning using hierarchical
<br>recurrent neural networks in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Proceedings of the IEEE conference on computer vision and
<br>pattern recognition</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. pp. 4584–4593.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6404px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">25. Devlin J, Chang MW, Lee K, Toutanova K (2018) Bert: Pre-training of deep bidirectional
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6404px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">62. Hermann KM, et al. (2017) Grounded language learning in a simulated 3d world. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6412px; width:187px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">transformers for language understanding. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv preprint arXiv:1810.04805</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6412px; width:67px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">preprint arXiv:1706.06551</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6420px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">26. Wang A, et al. (2018) Glue: A multi-task benchmark and analysis platform for natural lan-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6428px; width:138px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">guage understanding. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv preprint arXiv:1804.07461</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6436px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">27. Wang A, et al. (2019) Superglue: A stickier benchmark for general-purpose language under-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6444px; width:129px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">standing systems. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv preprint arXiv:1905.00537</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6452px; width:243px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">28. Levesque H, Davis E, Morgenstern L (2012) The winograd schema challenge in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Thirteenth
<br>International Conference on the Principles of Knowledge Representation and Reasoning</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br>29. Raffel C, Shazeer N, Roberts A, , et al. (2019) Exploring the limits of transfer learning with a
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6476px; width:160px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">uniﬁed text-to-text transformer. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv preprint arXiv:1910.10683</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6484px; width:244px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">30. Jia R, Liang P (2017) Adversarial examples for evaluating reading comprehension systems in
<br></span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br>(Association for Computational Linguistics), pp. 2021–2031.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6420px; width:243px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">63. Das R, Zaheer M, Reddy S, McCallum A (2017) Question answering on knowledge bases
<br>and text using universal schema and memory networks in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Proceedings of the 55th Annual
<br>Meeting of the Association for Computational Linguistic</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. pp. 358–365.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6444px; width:243px; height:13px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">64. Chaplot DS, Sathyendra KM, Pasumarthi RK, Rajagopal D, Salakhutdinov R (2017)
<br></span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv preprint
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:420px; top:6452px; width:91px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">task-oriented language grounding.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6452px; width:87px; height:13px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">Gated-attention architectures for
<br></span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv:1706.07230</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6468px; width:244px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">65. Oh J, Singh S, Lee H, Kohli P (2017) Zero-shot task generalization with multi-task deep rein-
<br></span><span style="font-family: NimbusSanL-Regu; font-size:5px">forcement learning in </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Proceedings of the 34th International Conference on Machine Learning-
<br>Volume 70</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. (JMLR. org), pp. 2661–2670.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6491px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">66. Hill F, et al. (2019) Emergent systematic generalization in a situated agent. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv preprint
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6499px; width:46px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">arXiv:1910.00571</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6507px; width:153px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">31. Lakoff G (1987) </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Women, Fire, and Dangerous Things</span><span style="font-family: NimbusSanL-Regu; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:215px; top:6507px; width:87px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">(The University of Chicago Press,
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6507px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">67. Lake BM, Salakhutdinov R, Tenenbaum JB (2015) Human-level concept learning through
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6523px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">32. Langacker RW (1987) </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Foundations of cognitive grammar: Theoretical prerequisites</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. (Stan-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6523px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">68. Graves A, et al. (2016) Hybrid computing using a neural network with dynamic external mem-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6515px; width:24px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">Chicago).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6531px; width:69px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">ford university press) Vol. 1.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6515px; width:159px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">probabilistic program induction. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Science </span><span style="font-family: NimbusSanL-Regu; font-size:5px">350(6266):1332–1338.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6531px; width:81px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">ory. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Nature </span><span style="font-family: NimbusSanL-Regu; font-size:5px">538(7626):471–476.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6539px; width:244px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">33. Bransford JD, Johnson MK (1972) Contextual prerequisites for understanding: Some in-
<br>vestigations of comprehension and recall. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Journal of verbal learning and verbal behavior
<br></span><span style="font-family: NimbusSanL-Regu; font-size:5px">11(6):717–726.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6539px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">69. Hintzman DL (1984) Minerva 2: A simulation model of human memory. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Behavior Research
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6547px; width:127px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Methods, Instruments, &amp; Computers </span><span style="font-family: NimbusSanL-Regu; font-size:5px">16(2):96–101.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6555px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">70. Polyn SM, Norman KA, Kahana MJ (2009) A context maintenance and retrieval model of
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6563px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">34. Schank RC (1983) </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Dynamic memory: A theory of reminding and learning in computers and
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6563px; width:182px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">organizational processes in free recall. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Psychological review </span><span style="font-family: NimbusSanL-Regu; font-size:5px">116(1):129.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6571px; width:96px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">people</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. (Cambridge University Press).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6571px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">71. Stout D, Chaminade T (2012) Stone tools, language and the brain in human evolution. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Philos
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6579px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">35. John MFS, McClelland JL (1990) Learning and applying contextual constraints in sentence
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6579px; width:117px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Trans R Soc Lond B Biol Sci </span><span style="font-family: NimbusSanL-Regu; font-size:5px">367(1585):75–87.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:316px; top:6587px; width:243px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">72. Bryson J (2008) Embodiment vs. memetics. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Mind and Society </span><span style="font-family: NimbusSanL-Regu; font-size:5px">7(1):77–94.
<br>73. Lakoff G, Johnson M (1980) </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Metaphors We Live By</span><span style="font-family: NimbusSanL-Regu; font-size:5px">. (University of Chicago, Chicago, IL).
<br>74. Feldman J, Narayanan S (2004) Embodied meaning in a neural theory of language. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Brain
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:329px; top:6611px; width:69px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">and Language </span><span style="font-family: NimbusSanL-Regu; font-size:5px">89:385–392.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6587px; width:136px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">comprehension. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Artiﬁcial Intelligence </span><span style="font-family: NimbusSanL-Regu; font-size:5px">46(1):217 – 257.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6595px; width:244px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">36. Hasson U, Egidi G, Marelli M, Willems RM (2018) Grounding the neurobiology of language
<br>in ﬁrst principles: The necessity of non-language-centric explanations for language compre-
<br>hension. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Cognition </span><span style="font-family: NimbusSanL-Regu; font-size:5px">180:135–157.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6619px; width:243px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">37. Rumelhart DE (1979) Some problems with the notion that words have literal meanings in
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6627px; width:227px; height:5px;"><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Metaphor and thought</span><span style="font-family: NimbusSanL-Regu; font-size:5px">, ed. Ortony A. (Cambridge Univ. Press, Cambridge, UK), pp. 71–82.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6635px; width:244px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">38. Barclay J, Bransford JD, Franks JJ, McCarrell NS, Nitsch K (1974) Comprehension and se-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:70px; top:6643px; width:204px; height:5px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">mantic ﬂexibility. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Journal of Verbal Learning and Verbal Behavior </span><span style="font-family: NimbusSanL-Regu; font-size:5px">13(4):471 – 481.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:57px; top:6651px; width:243px; height:21px;"><span style="font-family: NimbusSanL-Regu; font-size:5px">39. Tanenhaus MK, Spivey-Knowlton MJ, Eberhard KM, Sedivy JC (1995) Integration of visual
<br>and linguistic information in spoken language comprehension. </span><span style="font-family: NimbusSanL-ReguItal; font-size:5px">Science </span><span style="font-family: NimbusSanL-Regu; font-size:5px">pp. 1632–1634.
<br>40. Altmann GT, Kamide Y (2007) The real-time mediation of visual attention by language and
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:56px; top:6692px; width:277px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">McClelland et al.: Language understanding in humans and machines
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:555px; top:6692px; width:4px; height:8px;"><span style="font-family: LMRoman9-Regular; font-size:8px">8
<br></span></div><div style="position:absolute; top:0px;">Page: <a href="#1">1</a>, <a href="#2">2</a>, <a href="#3">3</a>, <a href="#4">4</a>, <a href="#5">5</a>, <a href="#6">6</a>, <a href="#7">7</a>, <a href="#8">8</a></div>
</body></html>
