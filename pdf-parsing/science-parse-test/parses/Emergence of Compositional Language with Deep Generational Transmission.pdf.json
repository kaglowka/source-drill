{
  "name" : "Emergence of Compositional Language with Deep Generational Transmission.pdf",
  "metadata" : {
    "source" : "META",
    "title" : "Emergence of Compositional Language with Deep Generational Transmission",
    "authors" : [ "Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra" ],
    "emails" : [ "<cogswell@gatech.edu>." ],
    "sections" : [ {
      "heading" : "1. Introduction",
      "text" : "Cultural transmission of language occurs when one group of agents passes their language to a new group of agents, e.g. parents who teach their children to speak as they do. Of the many design features which make human language unique, cultural transmission is important partially because it allows language itself to change over time via cultural evolution (Tomasello, 1999; Christiansen & Kirby, 2003a). This helps explain how a modern language like English emerged from some proto-language, an “almost language” precursor lacking the functionality of modern languages.\n1Georgia Tech 2Facebook AI Research. Correspondence to: Michael Cogswell <cogswell@gatech.edu>.\nGeneration N-1\n… … !\" !#\nGeneration N\n… …\nReinitialize !# and $%\n!# and $% learn language from other bots\n!%\n$\"\n$#\n$%\n$\"\n$#\n$%!%\n!#\n!\"\nFigure 1. We introduce cultural transmission into language emergence between neural agents. The starting point of our study is the goal oriented dialogue task of Kottur et al. (2017), summarized in Fig. 2. During learning we periodically replace some agents with new ones (gray agents). These new agents do not know any language, but instead of creating one they learn it from older agents. This creates generations of language that become more compositional over time.\nCompositionality is an important structure of a language, interesting to both linguists and machine learning researchers, which evolutionary linguists explain via cultural transmission (Kirby et al., 2014). In this work, a compositional language is one that expresses concepts by combining simpler elements which each have their own meaning. This kind of structure helps give human language the ability to express infinitely many concepts using finitely many elements, and to generalize in obviously correct ways despite a dearth of training examples (Lake & Baroni, 2018). For example, an agent who understands blue square and purple triangle should also understand purple square without directly experiencing it; we use this sort of generalization to measure compositionality. Existing work has investigated conditions under which compositional languages emerge between neural agents in simple environments (Mordatch & Abbeel, 2018; Kottur et al., 2017), but it only investigates how language changes within a generation.\nSimulating cultural transmission, the iterated learning model (Kirby et al., 2008; Kirby, 2001; Kirby et al., 2014) has found that generational dynamics cause compositional language to emerge using experiments in simulation (Kirby,\nar X\niv :1\n90 4.\n09 06\n7v 1\n[ cs\n.L G\n] 1\n9 A\npr 2\n01 9\n2001) and with human subjects (Kirby et al., 2008). In this model, language is directly but incompletely transmitted (taught) to one generation of agents from the previous generation. Because learning is incomplete and biased, the student language may differ from the teacher language. With the right learning and transmission mechanisms, a noncompositional language becomes compositional after many generations. This is cast as a trade-off between expressivity and compressibility, where a language must be expressive enough to differentiate between all possible meanings (e.g., objects) and compressible enough to be learned (Kirby et al., 2015). The explanation is so prominent that it was somewhat surprising when it was recently found that other factors can cause enough compressibility pressure to get compositional language emergence without generational transmission (Raviv et al., 2018).\nIn AI, emergence work aims to influence efforts to build intelligent agents that communicate with each other and especially with humans using language. On the fully supervised end of the spectrum, agents have been successfully trained to mimic human utterances for applications like machine translation (Bahdanau et al., 2014), image captioning (Xu et al., 2015), visual question answering (Antol et al., 2015), and visual dialog (Das et al., 2017a). In addition to the large amounts of data required, these systems still do not generalize nearly as well as we would like, and are rather hard to evaluate because natural language is open-ended and has strong priors that obscure true understanding (Liu et al., 2016; Agrawal et al., 2018). Other approaches use less supervision, placing multiple agents in carefully designed environments and giving them goals which require communication (Mikolov et al., 2016; Gauthier & Mordatch, 2016; Kiela et al., 2016). If some of the agents in the environment already know a language like English then the other agents can indirectly learn that language. But even this is expensive because it requires some agent that already knows a language.\nOn the other end of the spectrum, and most relevant to us, some work has found that languages will emerge to enable communication-centric tasks to be solved without direct or even indirect language supervision (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017; Das et al., 2017b). Unlike regimes where agents are trained to learn an existing language, languages that emerge in this sort of setting are not necessarily easy to understand for a human. Even attempts to translate these emerged languages for other agents are not completely successful (Andreas et al., 2017), possibly because the target languages can’t express the same concepts as the source languages. This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents (Kottur et al., 2017; Mordatch & Abbeel, 2018; Choi et al., 2018).\nIn this paper, we study the following question – what are the conditions that lead to the emergence of a compositional language? Our key finding is evidence that cultural transmission leads to more compositional language in deep reinforcement learning agents, as it does in evolutionary linguistics. The starting point for our investigation is the recent work of Kottur et al. (2017), which investigates compositionality using a cooperative reference game between two agents. Instead of using the same set of agents throughout training, we replace (re-initialize) some subset of them periodically. The resulting knowledge gap makes it easier for the new agent to learn from the older agents than to create a new language. In this way our approach introduces cultural transmission and thus a compressibility pressure, causing compositionality to emerge.\nOne difference between our approach and evolutionary methods applied elsewhere in deep learning (Stanley & Miikkulainen, 2002; Stanley et al., 2009; Real et al., 2017) is that we emulate cultural evolution instead of biological evolution. In biological evolution agents change from generation to generation while in cultural evolution the language itself evolves, so the same agent can have different languages at different times. Agents can directly benefit from evolutionary innovations throughout their lifetime instead of only at the beginning.\nOur approach is also different from iterated learning because our version of cultural transmission is implicit instead of explicit. Instead of teachers telling students exactly how to refer to the world, language is shared only to the extent doing so helps accomplish the goal.\nThrough our experiments and analysis, we show that periodic agent replacement is an effective way to induce cultural transmission and more compositionally generalizable language. To summarize, our contributions are:\n1. We propose a method for inducing implicit cultural transmission to neural language models. 2. We introduce new metrics to measure the similarity between agent languages and verify cultural transmission has occurred. 3. We show our cultural transmission procedure induces compositionality in neural language models, going from 16% accuracy on a compositionally novel test set to 59% in the best configuration. Furthermore, we show that this is complementary with previously used priors which encourage compositionality."
    }, {
      "heading" : "2. Approach",
      "text" : "We start by introducing goal-driven neural dialog models similar to Kottur et al. (2017), then describe how we incorporate cultural transmission."
    }, {
      "heading" : "2.1. Goal-Driven Neural Dialog",
      "text" : "In this work, we consider a setting where two agents, Q-bot (questioner) and A-bot (answerer), must communicate by exchanging single tokens over multiple rounds to solve a mutual task. Specifically, Q-bot must report some attributes of an object seen only by A-bot. To accomplish this task, Q-bot must query A-bot for the information.\nUsing the example in Fig. 2, an ideal dialog goes like this. At the beginning, Q-bot is given some task to solve (color, shape) and asks A-bot a question indicating the task (“X”, requesting the color). A-bot observes the object and answers the question (“1”, meaning purple). This dialog continues for a couple more rounds; Q-bot says “Y” and A-bot responds “2”, so Q-bot also knows the shape is square. Finally, Q-bot reports its prediction (purple, square) and both agents receive a reward if the prediction is correct.\nEach conversation has T rounds (we use T = 2). Q-bot starts by observing A-bot’s previous response mt−1A , its view of the world xQ (e.g., task), and its previous memory ht−1Q , then outputs memory h t Q summarizing its current view of the dialog and utters message mtQ. Functionally, mtQ, h t Q = Q(m t−1 A , xQ, h t−1 Q ). Similarly, A-bot responds by computing mtA, h t A = A(m t Q, xA, h t−1 A ). After the conversation, Q-bot tries to solve the given task by predicting u (e.g., corresponding to red square) as a function of its observation and final memory: û = U(xQ, hTQ). Both agents are rewarded if both attributes are correct. As in Kottur et al. (2017), we implement Q,A, and U as neural networks.\nOur model is trained to maximize the reward using policy gradients (Williams, 1992). Unlike an approach supervised by human dialogues, nothing orients the agents toward specific meanings for specific words, so they must create their own appropriately grounded language to solve the task.1 This approach–summarized in the black lines (4-9) of Algorithm 1–is our starting point. In Kottur et al. (2017) it was used to generate a somewhat compositional language given\nAlgorithm 1: Training with Replacement and Multiple Agents\n1 for epoch e = 1, . . . , Nepochs do 2 Sample Q-bot iQ from U{1, NQ} 3 Sample A-bot iA from U{1, NA} 4 for xQ, xA, u in each batch do 5 for conversation rounds t = 1, . . . T do 6 mtQ, h t Q = Q iQ(mt−1A , xQ, h t−1 Q ) 7 mtA, h t A = A iA(mt−1Q , xA, h t−1 A )\n8 û = U(xQ, h T Q) 9 Policy gradient update w.r.t. both Q-bot and A-bot parameters\n10 if e mod E = 0 then 11 Sample replacement set B using π 12 Re-initialize parameters and optimizer for all\nagents in B\n13 return all Q-bots and A-bots.\nappropriate agent and vocabulary configurations."
    }, {
      "heading" : "2.2. Language Emergence with Cultural Transmission",
      "text" : "Here we add cultural transmission to neural dialog models by considering an implicit model of cultural transmission. Implicit cultural transmission does not use word-level supervision, as opposed to explicit cultural transmission in which students are told which words refer to which objects. In implicit cultural transmission shared language emerges from shared goals. We develop an implicit model of cultural transmission2 that periodically replaces agents. Consequentially, older agents remember the old language while new agents learn it, favoring more easily compressible compositional language.\nReplacement. One open choice in this approach is how to select which agents to re-initialize – we explore different options in this section. Every E epochs, replacement policy π is called and returns a list of agents to be re-initialized, as seen at the blue lines (10-12) of Algorithm 13. This process creates generations of agents such that each generation learns languages that are slightly different but eventually\n1This lack of alignment also means Q-bot and A-bot messages aren’t necessarily interpretable as questions and answers, respectively.\n2 One of the first things we tried was to implement an explicit teaching phase where some agents would generate language used as training data for other agents during an auxiliary training phase. This never turned out to be helpful, often hurting compositional generalization. We also tried simulating the experiments from (Kirby et al., 2008) with neural agents. Despite two major architecture variations our initial results were negative. In future work we plan to study why this happens and improve the result.\nimprove upon those of previous generations.\nSingle Agent. In Kottur et al. (2017) there is only one pair of agents (NQ = NA = 1) so we cannot replace both agents at the same round because all existing language would be lost. Instead, we consider two strategies that only replace one bot at a time:\n– Random. Sample Q-bot or A-bot uniformly – Alternate. Alternate between Q-bot and A-bot\nMulti Agent. Q-bot and A-bot have different roles due to the asymmetry in information, so they use different parts of the language. Replacing A-bot (alt. Q-bot) means Q-bot (alt. A-bot) has to remember what A-bot says or else knowledge about that part of the language is lost. If A-bot was replaced but other A-bots speaking the same language were present then Q-bot would have incentive to remember the original language because of the other bots, preventing language loss. Furthermore, a Multi Agent environment may add compressibility pressure (Raviv et al., 2018) as bots have more to remember if there is any difference between the languages of their conversational partners. Finally, having multiple agents per type could introduce more variations in language, providing an opportunity to favor even better languages. Thus we introduce multiple A-bots and Q-bots.\nMore concretely, we consider a population of Qbots {Q1, . . . , QNQ} and a population of A-bots {A1, . . . , ANA}. Each member of the populations has a different set of parameters, but any Q-bot-A-bot pair might be sampled to interact with each other in the same batch. During learning we sample random pairs to interact and receive gradient updates. These changes correspond to the red lines (2-3) of Algorithm 1. In this Multi Agent scenario we replace one Q-bot and one A-bot every generation. We investigate three ways to sample agents to replace:\n– Uniform Random. Sample one Q-bot and one A-bot at random, placing equal probably on each option. – Epsilon Greedy. Replace agents as in Uniform Random with probability ε (we use ε = 0.2). Replace the agent with lowest validation accuracy with probability 1− ε. – Oldest. Consider only A-bots and Q-bots which have been around for the most epochs and sample (uniformly) one of each to replace.\nTo see how this could cause cultural transmission, consider an A-bot Ai that was just replaced with a new bot Āi. Most of the Q-bots already know how to translate one set of symbols (the ones from Ai) into correct predictions. If Āi is going to help these Q-bots solve their task then it will be more efficient for it to use the language already known\n3Note that our world is rather small so there is only one batch per epoch.\nto the receivers, that of Ai. Thus Āi will learn the existing language because it is more efficient than alternatives. This pressure for cultural transmission comes from the imbalance in knowledge between young and old agents created by reinitializing old bots. Section 4 supports this argument by showing that languages evolved via our approach are more similar to each other than otherwise."
    }, {
      "heading" : "3. Experiments",
      "text" : "In this section we investigate how our language evolution mechanism affects the structure of emergent languages. We show that our replacement approach causes compositionality and that cultural transmission does occur despite the implicit nature of our implementation."
    }, {
      "heading" : "3.1. Neural Question Answering Details",
      "text" : "Task Description. As in Kottur et al. (2017), our world contains objects with 3 attributes (shape, size, color) such that each attribute has 4 possible values. Objects are represented ‘symbolically’ as 3-hot vectors and not rendered as RGB images.\nEvaluation with Compositional Dataset. The explicit annotation of independent properties like shape and color allows compositionality to be tested, a necessarily domain specific evaluation. Certain combinations of attributes (e.g., purple square) are held out of the training set while ensuring that at least one instance of each attribute is present (e.g., at least one purple thing and one square thing). If the language created by interaction between agents can identify the held out instances (e.g., it has unique words for purple square which both agents understand) then it is compositional. This is simply measured by accuracy on the test set. Previous work also measures generalization to held out compositions of attributes to measure compositionality (Kottur et al., 2017; Kirby et al., 2015).\nUnlike Kottur et al. (2017), we use a slightly harder version of their dataset which aligns better with the goal of compositional language. For a few selected pairs of attributes 4 our version ensures those combinations are never seen outside the test set. This disallows opportunities for non-compositional generalization. Without this constraint, agents could generalize perfectly using words for attribute pairs like “red triangle” and “filled star” instead of words for “red,” “triangle,” “filled,” and “star.” The drop in accuracy5 between test and validation (which does not hold out attribute pairs) is roughly 20 points.\n4In our case, red triangles, filled stars, and blue dotted objects. 5Throughout the paper accuracy in this setting refers to the “Both” and not the “One” setting from (Kottur et al., 2017). That means a Q-bot is correct only if both û1 = u1 and û2 = u2.\nArchitecture and Training. Our A-bots and Q-bots have the same architecture and hyperparameter variations as in Kottur et al. (2017), but with our cultural transmission training procedure and some other differences identified below. Like Kottur et al. (2017), our hyperparameter variations consider the number of vocab words Q-bot (VQ) and A-bot (VA) may utter and whether or not A-bot has memory between dialog rounds. The memoryless version of A-bot simply sets htA = 0 between each round of dialog. This means A-bot cannot represent which attributes it has already communicated. When there are too many vocab words available there is less pressure to develop a compositional language because for every new object there is always an unused sequence of words which isn’t too similar to existing words, an effect also noticed elsewhere (Mordatch & Abbeel, 2018; Nowak et al., 2000). We add one setting where A-bot has no memory yet the number of vocab words is still overcomplete to help understand and disentangle these two factors. Specifically, we consider the following settings:\n– Small Vocab6: VQ=3, VA=4 – Memoryless + Small Vocab: VQ=3, VA=4, htA=0 – Overcomplete: VQ=64, VA=64 – Memoryless + Overcomplete: VQ=VA=64, htA=0\nAll agents are trained forE = 5000 epochs with a batch size of 1000 (so 1 batch per epoch) and the Adam (Kingma & Ba, 2015) optimizer with learning rate 0.01. In the Multi Agent setting we use NA = NQ = 5. To decide when to stop we measure validation set accuracy averaged over all Q-bot-Abot pairs and choose the first population whose validation accuracy did not improve for 200k epochs.7 This differs from Kottur et al. (2017), which stopped once train accuracy reached 100%. Furthermore, we do not mine negatives for each training batch.\nBaselines. Two baselines help verify our approach:\n– No Replacement. Never replace any agent (i.e., Algorithm 1 without blue lines). – Replace All. Always replace every agent (i.e., withB = all agents at line 11 of Algorithm 1).\nComparing to the No Replacement baseline establishes the main result by measuring the difference replacement makes. However, each time lines 11 and 12 of Algorithm 1 are executed there is one more chance of getting a lucky random initialization. Since the No Replacement baseline never does this it has a smaller chance of running in to one such lucky agent. Thus we compare to the Replace All baseline, which has the greatest chance of seeing a lucky initialization and thereby ensures that gains over the No Replacement baseline\n6This is slightly different from Small Vocab in (Kottur et al., 2017).\n7There are few objects in the environment, so each batch contains all objects and is an entire epoch.\nare not simply due to luck. In the Multi Agent setting we increasedE from 5000 to 25000 because agents were slower to converge."
    }, {
      "heading" : "3.2. Impact of Cultural Transmission",
      "text" : "Agent performance has a lot of variance, so we split the train data into 4 separate folds and perform cross-validation, averaging across folds as well as 4 different random seeds within each fold for a total of 16 runs per experiment. Results with standard deviations are reported in Fig. 3 and p-values for all t-tests are reported in the supplement.\nCultural transmission induces compositionality. First we consider variations in replacement strategy given each model type. Our main result is that cultural transmission approaches always outperform baseline approaches without cultural transmission. This can be seen by noting that the darker bars (cultural transmission) in Fig. 3 are larger than the lighter bars (baselines). After running a dependent paired t-test against all pairs of baselines and cultural transmission approaches we find a significant difference in all almost all cases (p ≤ 0.05; see supplement for all p-values). This is strong support for our claim that our version of cultural transmission encourages compositional language because it causes better generalization to novel compositions of attributes.\nPopulation dynamics without replacement do not consistently lead to compositionality. The Multi Agent No Replacement policy is only better than the Single Agent No Replacement policy for the Memoryless models (p ≤ 0.05), and not otherwise. It is somewhat surprising that this difference is not stronger since having multiple agents in the environment is one of the factors found to lead to compositionality without using cultural (more specifically, generational) transmission in (Raviv et al., 2018).\nVariations in replacement strategy do not appear to significantly affect outcomes. The Single Agent Random/Alternate replacement strategies are usually not significantly different than each other (p ≤ 0.05). The same is true for the Multi Agent Uniform Random/Epsilon Greedy/Oldest strategies. Significant differences only occur in the Overcomplete setting, where Single Agent Alternate outperforms Multi Agent Uniform Random, and in a few Small Vocab settings. This suggests that while some agent replacement needs to occur, it does not much matter whether agents with worse language are replaced or whether there is a pool of similarly typed agents to remember knowledge lost from older generations. The main factor is that new agents learn in the presence of others who already know a language.\nCultural transmission is complementary with other factors that encourage compositionality. The models considered in Kottur et al. (2017) were ordered, from best to worse, as: Memoryless + Small Vocab > Small Vocab > Overcomplete. Our trends tend to agree with that conclusion though the differences are smaller– mainly comparing the Memoryless + Small Vocab model to others in cultural transmission settings. Only in the Oldest setting are the differences all significant enough to completely establish the above ordering. This agrees with factors noted elsewhere (Kottur et al., 2017; Mordatch & Abbeel, 2018; Nowak et al., 2000).\nRemoving memory sometimes hurts. Removing memory always makes a significant difference (p ≤ 0.05) to Small Vocab models and only sometimes makes a difference for Overcomplete models. When it is significant, it helps Small Vocab models and hurts Overcomplete models. As the Memoryless + Overcomplete setting has not been reported before, these results suggest that the relationship between inter-round memory and compositionality is not clear.\nOverall, these results show that adding cultural transmission to neural dialog agents improves the compositional generalization of the languages learned by those agents in a way complementary to other priors. It thereby shows how to transfer the cultural transmission principle from evolutionary linguistics to deep learning."
    }, {
      "heading" : "4. Cultural Transmission Analysis",
      "text" : "Unlike iterated learning (Kirby et al., 2014), cultural transmission is implicit in our model. It is indirect, so we would like to measure whether or not it is actually occurring – that is, whether languages are actually being transferred. Qbot doesn’t know anything about the domain of objects, so instead of it directly teaching how to refer to xA (explicit cultural transmission), A-bot can only learn how Q-bot refers to an object from what it says about that object, mtA. Ei-\nther A-bot identified the purple filled square in a way that made Q-bot correctly identify it as a purple square or A-bot should refer to the purple square differently. By encouraging A-bot to say things that allow Q-bot to answer correctly it aligns the meanings the two bots understand. This is how our bots transmit linguistic knowledge.\nBecause it is implicit, cultural transmission may not actually be occurring; improvements may be from other sources. How can we measure cultural transmission? We take a simple approach. We assume that if two bots ‘speak the same language’ then that language was culturally transmitted. There is a combinatorial explosion of possible languages that could refer to all the objects of interest, so if the words that refer to the same object for two agents are the same then they were very likely transmitted from the other agents, rather than suspiciously similar languages emerging from scratch by chance. This leads to a simple approach: consider pairs of bots and see if they say similar things in the same context. If they do, then their shared language was probably transmitted.\nMore formally, consider the distribution of tokens A-bot8 Ai might use to describe its object xA when talking to Q-bot Qk: pk,i(mtA|xA) or pk,i for short. We want to know how similar Ai’s language is to that of another A-bot Aj . We’ll start by comparing those two distributions by computing the KL divergence between them and then taking an average over context (Q-bots, questions, and objects) to get our pairwise agent language similarity metric Dij :\nDij = ÊxA,k,t [ DKL ( pk,i(m t A|xA), pk,j(mtA|xA) )] (1)\nTaking another average, this time over all pairs of bots, gives our final measure of language similarity reported in Fig. 4.\nD = Êi,j s.t. i6=j [Dij ] (2)\nThis number D should be smaller the more similar language is between bots. Note that even thoughDij is not symmetric\n(because KL divergence is not), D is symmetric because it averages over both directions of pairs.\nWe compute D by sampling an empirical distribution over all messages and observations, taking 10 sample dialogues in each possible test state (xA, xQ) of the world using the final populations of agents as in Fig. 3. Note that this metric applies to a group of agents, so we measure it for only the Multi Agent settings, including two new baselines colored red in Fig. 4. The Single Agents Combined baseline trains 4 Single Agent No Replacement models independently then puts them together and computes D for that group. These agents only speak similar languages by chance, so D should be high. The Random Initialization baseline evaluates language similarity using newly initialized models. All agents have about a uniform distribution over words at every utterance, so their languages are both very similar and useless. These baselines act like a sort of practical (not strict) upper and lower bound to D, respectively.\nAs reported in table Fig. 4, A-bot languages are more similar for our models than for the No Replacement baseline. This provides evidence that cultural transmission is indeed occurring in our model.\nFurthermore, in Fig. 5 we plot D as it changes during language evolution. Similar to the post-training results in Fig. 4, we see that agents in the No Replacement and Single Agent strategies don’t learn the same language, but languages that evolve via implicit cultural transmission do converge to some high degree of similarity. Even multiple agents in the\nsame environment without replacement tend towards more similar languages than they would have created otherwise (No Replacement vs Single Agents Combined).\nA striking features of Fig. 5 is the oscillations, which are only apparent for Multi Agent All. Each time an agent is killed it causes the average language similarity to decrease because the newly initialized agent doesn’t know how to communicate with its companions. These oscillations also occur in the other replacement models, but in those cases only one agent was replaced and the period is shorted (E = 5000 instead of E = 25000)."
    }, {
      "heading" : "5. Related work",
      "text" : "There are two main veins of related work. In the first, evolutionary linguistics explains compositionality with cultural evolution. In the second, neural language models create languages to communicate and achieve their goals.\nLanguage Evolution Causes Structure. Researchers have spent decades studying how unique properties of human language like compositionality could have emerged. There is general agreement that people acquire language using a combination of innate cognitive capacity and learning from other language speakers (cultural transmission), with the degree of each being widely disputed (Perfors, 2002; Pinker & Bloom, 1990). Most agree the answer is something in between. Both innate cognitive capacity and specific modern human languages like English coe-\nvolved (Briscoe, 2000) via biological (Pinker & Bloom, 1990) and cultural (Tomasello, 1999; Smith, 2006) evolution, respectively. Thus an explanation of these evolutionary processes is essential to explaining human language.\nIn particular, explanations of how the cultural evolution of languages themselves could cause those languages to develop structure like compositionality are in abundance (Nowak & Krakauer, 1999; Nowak et al., 2000; Smith et al., 2003; Brighton, 2002; Vogt, 2005; Kirby et al., 2014; Spike et al., 2017). An important piece of the explanation of linguistic structure is the iterated learning model (Kirby et al., 2014; Kirby, 2001; Kirby et al., 2008) described in Section 1. This model focuses on the cultural transmission and bottlenecks that restrict how languages can be learned, showing compositional language emerges in computational (Kirby, 2001; 2002; Christiansen & Kirby, 2003b; Smith et al., 2003) and human (Kirby et al., 2008; Cornish et al., 2009; Scott-Phillips & Kirby, 2010) experiments. Even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics (Raviv et al., 2018) and deep learning (Kottur et al., 2017; Mordatch & Abbeel, 2018) show cultural transmission may not be necessary for compositionality to emerge.\nWhile existing work in deep learning has focused on biases that encourage compositionality, it has not considered settings where language is permitted to evolve as it is passed down over generations of agents. We consider such a setting because of its potential to complement our existing understanding.\nLanguage Emergence in Deep Learning. Recent work in deep learning has increasingly focused on multi-agent environments where deep agents learn to accomplish goals (possibly cooperative or competitive) by interacting appropriately with the environment and each other. Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017). Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018; Kottur et al., 2017). Both Mordatch & Abbeel (2018) and Kottur et al. (2017) find that limiting the vocabulary size so that there aren’t too many more words than there are objects to refer to encourages compositionality, which follows earlier results in evolutionary linguistics (Nowak et al., 2000). Follow up work has continued to investigate the emergence of compositional language among neural agents, mainly focusing on perceptual as opposed to symbolic input and how the structure of the input relates to the tendency for compositional language to emerge (Choi et al., 2018; Havrylov & Titov, 2017; Lazaridou et al., 2018). Other work investigating\nemergent translation has shown that Multi Agent interaction leads to better translation (Lee et al., 2018), but they do not measure compositionality.\nCultural Evolution and Neural Nets. Some work has considered the evolution of ideas by cultural transmission using neural agents. Most recently, Bengio (2012) considers what benefit cultural evolution might bestow on neural agents. The main idea is that culturally transmitted ideas may provide an important mechanism for escaping local minima in large complex models. Experiments in Gülçehre & Bengio (2016) followed up on that idea and supported part of the hypothesis by showing that supervision of intermediate representations allows a more complex toy task to be learned. Unlike our work, these experiments use direct language supervision provided by the designed environment rather than indirect and implicit supervision provided by other agents."
    }, {
      "heading" : "6. Conclusion",
      "text" : "In this work we investigated cultural transmission in deep conversational agents, applying it to language emergence. The evolutionary linguistics community has long used cultural transmission to explain how compositional languages could have emerged. The deep learning community, having recently become interested in language emergence, has not investigated that link until now. Instead of explicit models of cultural transmission familiar in evolutionary linguistics, we favor an implicit model where language is transmitted from generation to generation only because it helps agents achieve their goals. We show that this does indeed cause cultural transmission, and more important, that it also causes the emerged languages to be more compositional.\nFuture work. There is room for finding better ways to encourage compositional language. One very relevant approach would be to engineer more explicit models of cultural transmission and add other factors that encourage compositionality (e.g., as studied elswhere in deep learning (Vedantam et al., 2018)). On the other hand, we would like even deep non-language representations to be compositional. Cultural transmission may provide an appropriate prior for those cases as well."
    }, {
      "heading" : "Acknowledgements",
      "text" : "We would like to thank Satwik Kottur for code and comments as well as Karan Desai for additional code. We would also like to thank Douwe Kiela, Diane Bouchacourt, Sainbayar Sukhbaatar, and Marco Baroni for comments on earlier versions of this paper.\nThis work was supported in part by NSF, AFRL, DARPA,\nSiemens, Samsung, Google, Amazon, ONR YIPs and ONR Grants N00014-16-1-{2713,2793}. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the U.S. Government, or any sponsor."
    }, {
      "heading" : "A. Replacement Strategies",
      "text" : "Our approach to cultural transmission periodically replaces agents by re-initializing them. The approach section outlines various replacement strategies (policy π), but does not detail their implementation. We do so here.\nThese strategies depend on a number of possible inputs:\n• e the current epoch\n• E the period of agent replacement\n• vQi /vAi the validation accuracy of agent i for Q-bots/Abots. For Q-bots this is averaged over all potential A-bot partners, and vice-versa for A-bots.\n• aQi /aAi the age in epochs of agent i for Q-bots/A-bots\nSingle Agent strategies are given in Algorithm 2 and Algorithm 3. Multi Agent strategies are given in Algorithm 4, Algorithm 5, and Algorithm 6. Note that Single Agent strategies always replace one agent while Multi Agent strategies always replace one Q-bot and one A-bot. An additional Replace All baseline strategy is given in Algorithm 7 and generalizes to both Single and Multi Agent cases.\nAlgorithm 2: Single Agent - Random Replacement 1 d ∼ U{0, 1} 2 if d = 0 then 3 return { A-bot } 4 else 5 return { Q-bot }\nAlgorithm 3: Single Agent - Alternate Replacement 1 Input: e 2 if be/Ec = 0 then 3 return { A-bot } 4 else 5 return { Q-bot }\nAlgorithm 4: Multi Agent - Uniform Random Replacement\n1 iA ∼ U{1, NA} 2 iQ ∼ U{1, NQ} 3 return { A-bot iA, Q-bot iQ }\nAlgorithm 5: Multi Agent - Epsilon Greedy Replacement 1 Input: vQi ∀i, vAi ∀i, ε ∈ [0, 1) (usually 0.2) 2 d ∼ U [0, 1) 3 if d < ε then 4 iA ∼ U{1, NA} 5 iQ ∼ U{1, NQ} 6 else 7 iA = argmini v A i (unique in our experiments) 8 iQ = argmini v Q i (unique in our experiments) 9 return { A-bot iA, Q-bot iQ }\nAlgorithm 6: Multi Agent - Oldest Replacement\n1 Input: aQi ∀i, aVi ∀i 2 iA = U{argmaxi aAi } 3 iQ = U{argmaxi a Q i } 4 return { A-bot iA, Q-bot iQ }\nAlgorithm 7: Single/Multi Agent - Replace All 1 return {1, . . . , NA} ∪ {1, . . . , NQ}"
    }, {
      "heading" : "B. Detailed Results",
      "text" : "In our experiments we compare models and we compare replacement strategies. We ran dependent paired t-tests across random seeds, cross-val folds, and replacement strategies to compare models. We ran dependent paired t-tests across random seeds, cross-val folds, and models to compare replacement strategies. The p-values for all of these t-tests are reported here.\nReplacement strategy comparisons are in Fig. 7 (Single Agent) and Fig. 8 (Multi Agent). Model comparisons are in Fig. 6.\n11"
    } ],
    "references" : [ {
      "title" : "Don’t just assume; look and answer: Overcoming priors for visual question answering",
      "author" : [ "Agrawal", "Aishwarya", "Batra", "Dhruv", "Parikh", "Devi", "Kembhavi", "Aniruddha" ],
      "venue" : "In IEEE Conference on Computer Vision and Pattern Recognition",
      "citeRegEx" : "Agrawal et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Agrawal et al\\.",
      "year" : 2018
    }, {
      "title" : "Translating neuralese",
      "author" : [ "Andreas", "Jacob", "Dragan", "Anca D", "Klein", "Dan" ],
      "venue" : "In ACL,",
      "citeRegEx" : "Andreas et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Andreas et al\\.",
      "year" : 2017
    }, {
      "title" : "VQA: Visual Question Answering",
      "author" : [ "Antol", "Stanislaw", "Agrawal", "Aishwarya", "Lu", "Jiasen", "Mitchell", "Margaret", "Batra", "Dhruv", "C Lawrence Zitnick", "Parikh", "Devi" ],
      "venue" : "In ICCV,",
      "citeRegEx" : "Antol et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Antol et al\\.",
      "year" : 2015
    }, {
      "title" : "Neural machine translation by jointly learning to align and translate",
      "author" : [ "Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua" ],
      "venue" : "CoRR, abs/1409.0473,",
      "citeRegEx" : "Bahdanau et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Bahdanau et al\\.",
      "year" : 2014
    }, {
      "title" : "Evolving culture vs local minima",
      "author" : [ "Bengio", "Yoshua" ],
      "venue" : "CoRR, abs/1203.2990,",
      "citeRegEx" : "Bengio and Yoshua.,? \\Q2012\\E",
      "shortCiteRegEx" : "Bengio and Yoshua.",
      "year" : 2012
    }, {
      "title" : "Compositional syntax from cultural transmission",
      "author" : [ "Brighton", "Henry" ],
      "venue" : "Artificial Life,",
      "citeRegEx" : "Brighton and Henry.,? \\Q2002\\E",
      "shortCiteRegEx" : "Brighton and Henry.",
      "year" : 2002
    }, {
      "title" : "Grammatical acquisition: Inductive bias and coevolution of language and the language acquisition device",
      "author" : [ "Briscoe", "Ted" ],
      "venue" : "In Language,",
      "citeRegEx" : "Briscoe and Ted.,? \\Q2000\\E",
      "shortCiteRegEx" : "Briscoe and Ted.",
      "year" : 2000
    }, {
      "title" : "Compositional obverter communication learning from raw visual input",
      "author" : [ "Choi", "Edward", "Lazaridou", "Angeliki", "de Freitas", "Nando" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Choi et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Language evolution: consensus and controversies",
      "author" : [ "Christiansen", "Morten H", "Kirby", "Simon" ],
      "venue" : "Trends in cognitive sciences,",
      "citeRegEx" : "Christiansen et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Christiansen et al\\.",
      "year" : 2003
    }, {
      "title" : "Complex adaptive systems and the origins of adaptive structure: What experiments can tell us",
      "author" : [ "Cornish", "Hannah", "Tamariz", "Monica", "Kirby", "Simon" ],
      "venue" : "Language Learning,",
      "citeRegEx" : "Cornish et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Cornish et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning cooperative visual dialog agents with deep reinforcement learning",
      "author" : [ "Das", "Abhishek", "Kottur", "Satwik", "Moura", "José M.F", "Lee", "Stefan", "Batra", "Dhruv" ],
      "venue" : "In Proceedings of the IEEE International Conference on Computer Vision (ICCV),",
      "citeRegEx" : "Das et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Das et al\\.",
      "year" : 2017
    }, {
      "title" : "Learning to communicate with deep multi-agent reinforcement learning",
      "author" : [ "Foerster", "Jakob", "Assael", "Ioannis Alexandros", "de Freitas", "Nando", "Whiteson", "Shimon" ],
      "venue" : "In Advances in Neural Information Processing Systems,",
      "citeRegEx" : "Foerster et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2016
    }, {
      "title" : "A paradigm for situated and goal-driven language learning",
      "author" : [ "Gauthier", "Jon", "Mordatch", "Igor" ],
      "venue" : null,
      "citeRegEx" : "Gauthier et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gauthier et al\\.",
      "year" : 2016
    }, {
      "title" : "Knowledge matters: Importance of prior information for optimization",
      "author" : [ "Gülçehre", "Çağlar", "Bengio", "Yoshua" ],
      "venue" : "Journal of Machine Learning Research,",
      "citeRegEx" : "Gülçehre et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Gülçehre et al\\.",
      "year" : 2016
    }, {
      "title" : "Emergence of language with multi-agent games: Learning to communicate with sequences of symbols",
      "author" : [ "Havrylov", "Serhii", "Titov", "Ivan" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Havrylov et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Havrylov et al\\.",
      "year" : 2017
    }, {
      "title" : "Virtual embodiment: A scalable long-term strategy for artificial intelligence research",
      "author" : [ "Kiela", "Douwe", "Bulat", "Luana", "Vero", "Anita L", "Clark", "Stephen" ],
      "venue" : "arXiv preprint arXiv:1610.07432,",
      "citeRegEx" : "Kiela et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Kiela et al\\.",
      "year" : 2016
    }, {
      "title" : "Adam: A method for stochastic optimization",
      "author" : [ "Kingma", "Diederik P", "Ba", "Jimmy" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Kingma et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kingma et al\\.",
      "year" : 2015
    }, {
      "title" : "Spontaneous evolution of linguistic structurean iterated learning model of the emergence of regularity and irregularity",
      "author" : [ "Kirby", "Simon" ],
      "venue" : "IEEE Trans. Evolutionary Computation,",
      "citeRegEx" : "Kirby and Simon.,? \\Q2001\\E",
      "shortCiteRegEx" : "Kirby and Simon.",
      "year" : 2001
    }, {
      "title" : "Natural language from artificial life",
      "author" : [ "Kirby", "Simon" ],
      "venue" : "In Artificial Life,",
      "citeRegEx" : "Kirby and Simon.,? \\Q2002\\E",
      "shortCiteRegEx" : "Kirby and Simon.",
      "year" : 2002
    }, {
      "title" : "Cumulative cultural evolution in the laboratory: An experimental approach to the origins of structure in human language",
      "author" : [ "Kirby", "Simon", "Cornish", "Hannah", "Smith", "Kenny" ],
      "venue" : "Proceedings of the National Academy of Sciences,",
      "citeRegEx" : "Kirby et al\\.,? \\Q2008\\E",
      "shortCiteRegEx" : "Kirby et al\\.",
      "year" : 2008
    }, {
      "title" : "Iterated learning and the evolution of language",
      "author" : [ "Kirby", "Simon", "Griffiths", "Tom", "Smith", "Kenny" ],
      "venue" : "Current Opinion in Neurobiology,",
      "citeRegEx" : "Kirby et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kirby et al\\.",
      "year" : 2014
    }, {
      "title" : "Compression and communication in the cultural evolution of linguistic structure",
      "author" : [ "Kirby", "Simon", "Tamariz", "Monica", "Cornish", "Hannah", "Smith", "Kenny" ],
      "venue" : null,
      "citeRegEx" : "Kirby et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Kirby et al\\.",
      "year" : 2015
    }, {
      "title" : "Natural language does not emerge ’naturally’ in multi-agent dialog",
      "author" : [ "Kottur", "Satwik", "Moura", "José M. F", "Lee", "Stefan", "Batra", "Dhruv" ],
      "venue" : null,
      "citeRegEx" : "Kottur et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2017
    }, {
      "title" : "Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks",
      "author" : [ "Lake", "Brenden M", "Baroni", "Marco" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Lake et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lake et al\\.",
      "year" : 2018
    }, {
      "title" : "Multi-agent cooperation and the emergence of (natural) language",
      "author" : [ "Lazaridou", "Angeliki", "Peysakhovich", "Alexander", "Baroni", "Marco" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2017
    }, {
      "title" : "Emergence of linguistic communication from referential games with symbolic and pixel input",
      "author" : [ "Lazaridou", "Angeliki", "Hermann", "Karl Moritz", "Tuyls", "Karl", "Clark", "Stephen" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2018
    }, {
      "title" : "Emergent translation in multi-agent communication",
      "author" : [ "Lee", "Jason D", "Cho", "Kyunghyun", "Weston", "Jason", "Kiela", "Douwe" ],
      "venue" : null,
      "citeRegEx" : "Lee et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation",
      "author" : [ "Liu", "Chia-Wei", "Lowe", "Ryan", "Serban", "Iulian", "Noseworthy", "Michael", "Charlin", "Laurent", "Pineau", "Joelle" ],
      "venue" : null,
      "citeRegEx" : "Liu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Liu et al\\.",
      "year" : 2016
    }, {
      "title" : "A roadmap towards machine intelligence",
      "author" : [ "Mikolov", "Tomas", "Joulin", "Armand", "Baroni", "Marco" ],
      "venue" : "In CICLing,",
      "citeRegEx" : "Mikolov et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Mikolov et al\\.",
      "year" : 2016
    }, {
      "title" : "Emergence of grounded compositional language in multi-agent populations",
      "author" : [ "Mordatch", "Igor", "Abbeel", "Pieter" ],
      "venue" : "In AAAI,",
      "citeRegEx" : "Mordatch et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Mordatch et al\\.",
      "year" : 2018
    }, {
      "title" : "The evolution of language",
      "author" : [ "Nowak", "Martin A", "Krakauer", "David C" ],
      "venue" : "Proceedings of the National Academy of Sciences of the United States of America,",
      "citeRegEx" : "Nowak et al\\.,? \\Q1999\\E",
      "shortCiteRegEx" : "Nowak et al\\.",
      "year" : 1999
    }, {
      "title" : "Simulated evolution of language: a review of the field",
      "author" : [ "Perfors", "Amy" ],
      "venue" : "J. Artificial Societies and Social Simulation,",
      "citeRegEx" : "Perfors and Amy.,? \\Q2002\\E",
      "shortCiteRegEx" : "Perfors and Amy.",
      "year" : 2002
    }, {
      "title" : "Natural language and natural selection",
      "author" : [ "Pinker", "Steven", "Bloom", "Paul" ],
      "venue" : "Behavioral and brain sciences,",
      "citeRegEx" : "Pinker et al\\.,? \\Q1990\\E",
      "shortCiteRegEx" : "Pinker et al\\.",
      "year" : 1990
    }, {
      "title" : "Compositional structure can emerge without generational transmission",
      "author" : [ "Raviv", "Limor", "Meyer", "Antje", "Lev-Ari", "Shiri" ],
      "venue" : null,
      "citeRegEx" : "Raviv et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Raviv et al\\.",
      "year" : 2018
    }, {
      "title" : "Large-scale evolution of image classifiers",
      "author" : [ "Real", "Esteban", "Moore", "Sherry", "Selle", "Andrew", "Saxena", "Saurabh", "Suematsu", "Yutaka Leon", "Le", "Quoc V", "Kurakin", "Alex" ],
      "venue" : null,
      "citeRegEx" : "Real et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Real et al\\.",
      "year" : 2017
    }, {
      "title" : "Cultural evolution of language",
      "author" : [ "Smith", "Kenny" ],
      "venue" : "Encyclopedia of Language and Linguistics 2 Edition,",
      "citeRegEx" : "Smith and Kenny.,? \\Q2006\\E",
      "shortCiteRegEx" : "Smith and Kenny.",
      "year" : 2006
    }, {
      "title" : "Iterated learning: A framework for the emergence of language",
      "author" : [ "Smith", "Kenny", "Kirby", "Simon", "Brighton", "Henry" ],
      "venue" : "Artificial Life,",
      "citeRegEx" : "Smith et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2003
    }, {
      "title" : "Minimal requirements for the emergence of learned signaling",
      "author" : [ "Spike", "Matthew", "Stadler", "Kevin", "Kirby", "Simon", "Smith", "Kenny" ],
      "venue" : "In Cognitive Science,",
      "citeRegEx" : "Spike et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Spike et al\\.",
      "year" : 2017
    }, {
      "title" : "Evolving neural networks through augmenting topologies",
      "author" : [ "Stanley", "Kenneth O", "Miikkulainen", "Risto" ],
      "venue" : "Evolutionary Computation,",
      "citeRegEx" : "Stanley et al\\.,? \\Q2002\\E",
      "shortCiteRegEx" : "Stanley et al\\.",
      "year" : 2002
    }, {
      "title" : "A hypercube-based encoding for evolving largescale neural networks",
      "author" : [ "Stanley", "Kenneth O", "D’Ambrosio", "David B", "Gauci", "Jason" ],
      "venue" : "Artificial Life,",
      "citeRegEx" : "Stanley et al\\.,? \\Q2009\\E",
      "shortCiteRegEx" : "Stanley et al\\.",
      "year" : 2009
    }, {
      "title" : "Learning multiagent communication with backpropagation",
      "author" : [ "Sukhbaatar", "Sainbayar", "Szlam", "Arthur", "Fergus", "Rob" ],
      "venue" : "In NIPS,",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2016
    }, {
      "title" : "The cultural origins of human cognition",
      "author" : [ "Tomasello", "Michael" ],
      "venue" : "Harvard university press,",
      "citeRegEx" : "Tomasello and Michael.,? \\Q1999\\E",
      "shortCiteRegEx" : "Tomasello and Michael.",
      "year" : 1999
    }, {
      "title" : "Generative models of visually grounded imagination",
      "author" : [ "Vedantam", "Ramakrishna", "Fischer", "Ian", "Huang", "Jonathan", "Murphy", "Kevin" ],
      "venue" : "In International Conference on Learning Representations (ICLR),",
      "citeRegEx" : "Vedantam et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Vedantam et al\\.",
      "year" : 2018
    }, {
      "title" : "The emergence of compositional structures in perceptually grounded language games",
      "author" : [ "Vogt", "Paul" ],
      "venue" : "Artificial intelligence,",
      "citeRegEx" : "Vogt and Paul.,? \\Q2005\\E",
      "shortCiteRegEx" : "Vogt and Paul.",
      "year" : 2005
    }, {
      "title" : "Simple statistical gradient-following algorithms for connectionist reinforcement learning",
      "author" : [ "Williams", "Ronald J" ],
      "venue" : "Machine Learning,",
      "citeRegEx" : "Williams and J.,? \\Q1992\\E",
      "shortCiteRegEx" : "Williams and J.",
      "year" : 1992
    }, {
      "title" : "Show, attend and tell: Neural image caption generation with visual attention",
      "author" : [ "Xu", "Kelvin", "Ba", "Jimmy", "Kiros", "Ryan", "Cho", "Kyunghyun", "Courville", "Aaron C", "Salakhutdinov", "Ruslan", "Zemel", "Richard S", "Bengio", "Yoshua" ],
      "venue" : "In ICML,",
      "citeRegEx" : "Xu et al\\.,? \\Q2015\\E",
      "shortCiteRegEx" : "Xu et al\\.",
      "year" : 2015
    } ],
    "referenceMentions" : [ {
      "referenceID" : 20,
      "context" : "Compositionality is an important structure of a language, interesting to both linguists and machine learning researchers, which evolutionary linguists explain via cultural transmission (Kirby et al., 2014).",
      "startOffset" : 185,
      "endOffset" : 205
    }, {
      "referenceID" : 22,
      "context" : "Existing work has investigated conditions under which compositional languages emerge between neural agents in simple environments (Mordatch & Abbeel, 2018; Kottur et al., 2017), but it only investigates how language changes within a generation.",
      "startOffset" : 130,
      "endOffset" : 176
    }, {
      "referenceID" : 19,
      "context" : "Simulating cultural transmission, the iterated learning model (Kirby et al., 2008; Kirby, 2001; Kirby et al., 2014) has found that generational dynamics cause compositional language to emerge using experiments in simulation (Kirby, ar X iv :1 90 4.",
      "startOffset" : 62,
      "endOffset" : 115
    }, {
      "referenceID" : 20,
      "context" : "Simulating cultural transmission, the iterated learning model (Kirby et al., 2008; Kirby, 2001; Kirby et al., 2014) has found that generational dynamics cause compositional language to emerge using experiments in simulation (Kirby, ar X iv :1 90 4.",
      "startOffset" : 62,
      "endOffset" : 115
    }, {
      "referenceID" : 21,
      "context" : ", objects) and compressible enough to be learned (Kirby et al., 2015).",
      "startOffset" : 49,
      "endOffset" : 69
    }, {
      "referenceID" : 33,
      "context" : "The explanation is so prominent that it was somewhat surprising when it was recently found that other factors can cause enough compressibility pressure to get compositional language emergence without generational transmission (Raviv et al., 2018).",
      "startOffset" : 226,
      "endOffset" : 246
    }, {
      "referenceID" : 3,
      "context" : "On the fully supervised end of the spectrum, agents have been successfully trained to mimic human utterances for applications like machine translation (Bahdanau et al., 2014), image captioning (Xu et al.",
      "startOffset" : 151,
      "endOffset" : 174
    }, {
      "referenceID" : 45,
      "context" : ", 2014), image captioning (Xu et al., 2015), visual question answering (Antol et al.",
      "startOffset" : 26,
      "endOffset" : 43
    }, {
      "referenceID" : 2,
      "context" : ", 2015), visual question answering (Antol et al., 2015), and visual dialog (Das et al.",
      "startOffset" : 35,
      "endOffset" : 55
    }, {
      "referenceID" : 27,
      "context" : "In addition to the large amounts of data required, these systems still do not generalize nearly as well as we would like, and are rather hard to evaluate because natural language is open-ended and has strong priors that obscure true understanding (Liu et al., 2016; Agrawal et al., 2018).",
      "startOffset" : 247,
      "endOffset" : 287
    }, {
      "referenceID" : 0,
      "context" : "In addition to the large amounts of data required, these systems still do not generalize nearly as well as we would like, and are rather hard to evaluate because natural language is open-ended and has strong priors that obscure true understanding (Liu et al., 2016; Agrawal et al., 2018).",
      "startOffset" : 247,
      "endOffset" : 287
    }, {
      "referenceID" : 28,
      "context" : "Other approaches use less supervision, placing multiple agents in carefully designed environments and giving them goals which require communication (Mikolov et al., 2016; Gauthier & Mordatch, 2016; Kiela et al., 2016).",
      "startOffset" : 148,
      "endOffset" : 217
    }, {
      "referenceID" : 15,
      "context" : "Other approaches use less supervision, placing multiple agents in carefully designed environments and giving them goals which require communication (Mikolov et al., 2016; Gauthier & Mordatch, 2016; Kiela et al., 2016).",
      "startOffset" : 148,
      "endOffset" : 217
    }, {
      "referenceID" : 11,
      "context" : "On the other end of the spectrum, and most relevant to us, some work has found that languages will emerge to enable communication-centric tasks to be solved without direct or even indirect language supervision (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017; Das et al., 2017b).",
      "startOffset" : 210,
      "endOffset" : 301
    }, {
      "referenceID" : 40,
      "context" : "On the other end of the spectrum, and most relevant to us, some work has found that languages will emerge to enable communication-centric tasks to be solved without direct or even indirect language supervision (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017; Das et al., 2017b).",
      "startOffset" : 210,
      "endOffset" : 301
    }, {
      "referenceID" : 24,
      "context" : "On the other end of the spectrum, and most relevant to us, some work has found that languages will emerge to enable communication-centric tasks to be solved without direct or even indirect language supervision (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017; Das et al., 2017b).",
      "startOffset" : 210,
      "endOffset" : 301
    }, {
      "referenceID" : 1,
      "context" : "Even attempts to translate these emerged languages for other agents are not completely successful (Andreas et al., 2017), possibly because the target languages can’t express the same concepts as the source languages.",
      "startOffset" : 98,
      "endOffset" : 120
    }, {
      "referenceID" : 22,
      "context" : "This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents (Kottur et al., 2017; Mordatch & Abbeel, 2018; Choi et al., 2018).",
      "startOffset" : 119,
      "endOffset" : 184
    }, {
      "referenceID" : 7,
      "context" : "This desire for structure motivates the previously mentioned work on compositional language emergence in neural agents (Kottur et al., 2017; Mordatch & Abbeel, 2018; Choi et al., 2018).",
      "startOffset" : 119,
      "endOffset" : 184
    }, {
      "referenceID" : 39,
      "context" : "One difference between our approach and evolutionary methods applied elsewhere in deep learning (Stanley & Miikkulainen, 2002; Stanley et al., 2009; Real et al., 2017) is that we emulate cultural evolution instead of biological evolution.",
      "startOffset" : 96,
      "endOffset" : 167
    }, {
      "referenceID" : 34,
      "context" : "One difference between our approach and evolutionary methods applied elsewhere in deep learning (Stanley & Miikkulainen, 2002; Stanley et al., 2009; Real et al., 2017) is that we emulate cultural evolution instead of biological evolution.",
      "startOffset" : 96,
      "endOffset" : 167
    }, {
      "referenceID" : 19,
      "context" : "We also tried simulating the experiments from (Kirby et al., 2008) with neural agents.",
      "startOffset" : 46,
      "endOffset" : 66
    }, {
      "referenceID" : 33,
      "context" : "Furthermore, a Multi Agent environment may add compressibility pressure (Raviv et al., 2018) as bots have more to remember if there is any difference between the languages of their conversational partners.",
      "startOffset" : 72,
      "endOffset" : 92
    }, {
      "referenceID" : 22,
      "context" : "Previous work also measures generalization to held out compositions of attributes to measure compositionality (Kottur et al., 2017; Kirby et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 151
    }, {
      "referenceID" : 21,
      "context" : "Previous work also measures generalization to held out compositions of attributes to measure compositionality (Kottur et al., 2017; Kirby et al., 2015).",
      "startOffset" : 110,
      "endOffset" : 151
    }, {
      "referenceID" : 22,
      "context" : "(5)Throughout the paper accuracy in this setting refers to the “Both” and not the “One” setting from (Kottur et al., 2017).",
      "startOffset" : 101,
      "endOffset" : 122
    }, {
      "referenceID" : 22,
      "context" : "This is slightly different from Small Vocab in (Kottur et al., 2017).",
      "startOffset" : 47,
      "endOffset" : 68
    }, {
      "referenceID" : 33,
      "context" : "It is somewhat surprising that this difference is not stronger since having multiple agents in the environment is one of the factors found to lead to compositionality without using cultural (more specifically, generational) transmission in (Raviv et al., 2018).",
      "startOffset" : 240,
      "endOffset" : 260
    }, {
      "referenceID" : 22,
      "context" : "This agrees with factors noted elsewhere (Kottur et al., 2017; Mordatch & Abbeel, 2018; Nowak et al., 2000).",
      "startOffset" : 41,
      "endOffset" : 107
    }, {
      "referenceID" : 20,
      "context" : "Unlike iterated learning (Kirby et al., 2014), cultural transmission is implicit in our model.",
      "startOffset" : 25,
      "endOffset" : 45
    }, {
      "referenceID" : 36,
      "context" : "In particular, explanations of how the cultural evolution of languages themselves could cause those languages to develop structure like compositionality are in abundance (Nowak & Krakauer, 1999; Nowak et al., 2000; Smith et al., 2003; Brighton, 2002; Vogt, 2005; Kirby et al., 2014; Spike et al., 2017).",
      "startOffset" : 170,
      "endOffset" : 302
    }, {
      "referenceID" : 20,
      "context" : "In particular, explanations of how the cultural evolution of languages themselves could cause those languages to develop structure like compositionality are in abundance (Nowak & Krakauer, 1999; Nowak et al., 2000; Smith et al., 2003; Brighton, 2002; Vogt, 2005; Kirby et al., 2014; Spike et al., 2017).",
      "startOffset" : 170,
      "endOffset" : 302
    }, {
      "referenceID" : 37,
      "context" : "In particular, explanations of how the cultural evolution of languages themselves could cause those languages to develop structure like compositionality are in abundance (Nowak & Krakauer, 1999; Nowak et al., 2000; Smith et al., 2003; Brighton, 2002; Vogt, 2005; Kirby et al., 2014; Spike et al., 2017).",
      "startOffset" : 170,
      "endOffset" : 302
    }, {
      "referenceID" : 20,
      "context" : "An important piece of the explanation of linguistic structure is the iterated learning model (Kirby et al., 2014; Kirby, 2001; Kirby et al., 2008) described in Section 1.",
      "startOffset" : 93,
      "endOffset" : 146
    }, {
      "referenceID" : 19,
      "context" : "An important piece of the explanation of linguistic structure is the iterated learning model (Kirby et al., 2014; Kirby, 2001; Kirby et al., 2008) described in Section 1.",
      "startOffset" : 93,
      "endOffset" : 146
    }, {
      "referenceID" : 36,
      "context" : "This model focuses on the cultural transmission and bottlenecks that restrict how languages can be learned, showing compositional language emerges in computational (Kirby, 2001; 2002; Christiansen & Kirby, 2003b; Smith et al., 2003) and human (Kirby et al.",
      "startOffset" : 164,
      "endOffset" : 232
    }, {
      "referenceID" : 33,
      "context" : "Even though cultural transmission may aid the emergence of compositionality, recent results in evolutionary linguistics (Raviv et al., 2018) and deep learning (Kottur et al.",
      "startOffset" : 120,
      "endOffset" : 140
    }, {
      "referenceID" : 22,
      "context" : ", 2018) and deep learning (Kottur et al., 2017; Mordatch & Abbeel, 2018) show cultural transmission may not be necessary for compositionality to emerge.",
      "startOffset" : 26,
      "endOffset" : 72
    }, {
      "referenceID" : 11,
      "context" : "Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017).",
      "startOffset" : 154,
      "endOffset" : 226
    }, {
      "referenceID" : 40,
      "context" : "Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017).",
      "startOffset" : 154,
      "endOffset" : 226
    }, {
      "referenceID" : 24,
      "context" : "Some of this work has shown that deep agents will develop their own language where none exists initially if driven by a task which requires communication (Foerster et al., 2016; Sukhbaatar et al., 2016; Lazaridou et al., 2017).",
      "startOffset" : 154,
      "endOffset" : 226
    }, {
      "referenceID" : 22,
      "context" : "Most relevant is similar work which focuses on conditions under which compositional language emerges as deep agents learn to cooperate (Mordatch & Abbeel, 2018; Kottur et al., 2017).",
      "startOffset" : 135,
      "endOffset" : 181
    }, {
      "referenceID" : 7,
      "context" : "Follow up work has continued to investigate the emergence of compositional language among neural agents, mainly focusing on perceptual as opposed to symbolic input and how the structure of the input relates to the tendency for compositional language to emerge (Choi et al., 2018; Havrylov & Titov, 2017; Lazaridou et al., 2018).",
      "startOffset" : 260,
      "endOffset" : 327
    }, {
      "referenceID" : 25,
      "context" : "Follow up work has continued to investigate the emergence of compositional language among neural agents, mainly focusing on perceptual as opposed to symbolic input and how the structure of the input relates to the tendency for compositional language to emerge (Choi et al., 2018; Havrylov & Titov, 2017; Lazaridou et al., 2018).",
      "startOffset" : 260,
      "endOffset" : 327
    }, {
      "referenceID" : 26,
      "context" : "Other work investigating emergent translation has shown that Multi Agent interaction leads to better translation (Lee et al., 2018), but they do not measure compositionality.",
      "startOffset" : 113,
      "endOffset" : 131
    }, {
      "referenceID" : 42,
      "context" : ", as studied elswhere in deep learning (Vedantam et al., 2018)).",
      "startOffset" : 39,
      "endOffset" : 62
    } ],
    "year" : 2019,
    "abstractText" : "Consider a collaborative task that requires communication. Two agents are placed in an environment and must create a language from scratch in order to coordinate. Recent work has been interested in what kinds of languages emerge when deep reinforcement learning agents are put in such a situation, and in particular in the factors that cause language to be compositional–i.e. meaning is expressed by combining words which themselves have meaning. Evolutionary linguists have also studied the emergence of compositional language for decades, and they find that in addition to structural priors like those already studied in deep learning, the dynamics of transmitting language from generation to generation contribute significantly to the emergence of compositionality. In this paper, we introduce these cultural evolutionary dynamics into language emergence by periodically replacing agents in a population to create a knowledge gap, implicitly inducing cultural transmission of language. We show that this implicit cultural transmission encourages the resulting languages to exhibit better compositional generalization and suggest how elements of cultural dynamics can be further integrated into populations of deep agents.",
    "creator" : "LaTeX with hyperref package"
  }
}