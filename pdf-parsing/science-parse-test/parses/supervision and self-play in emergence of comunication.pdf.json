{
  "name" : "supervision and self-play in emergence of comunication.pdf",
  "metadata" : {
    "source" : "CRF",
    "title" : "ON THE INTERACTION BETWEEN SUPERVISION AND SELF-PLAY IN EMERGENT COMMUNICATION",
    "authors" : [ "Ryan Lowe", "Abhinav Gupta", "MILA Jakob Foerster", "Douwe Kiela", "Joelle Pineau" ],
    "emails" : [ ],
    "sections" : [ {
      "heading" : "1 INTRODUCTION",
      "text" : "Language is one of the most important aspects of human intelligence; it allows humans to coordinate and share knowledge with each other. It is also crucial for human-machine interaction, as human language is a natural means by which to exchange information, give feedback, and specify goals. A promising approach for training agents to solve problems with natural language is to have a “human in the loop”, meaning we collect problem-specific data from humans interacting directly with our agents for learning. However, human-in-the-loop data is expensive and time-consuming to obtain as it requires continuously collecting human data as the agent’s policy improves, and recent work suggests that current machine learning methods (e.g. from deep reinforcement learning) are too data-inefficient to be trained in this way from scratch (Chevalier-Boisvert et al., 2019). Thus, an important open problem is: how can we make human-in-the-loop training as data efficient as possible?\nTo maximize data efficiency, it is important to fully leverage all available training signals. In this paper, we study two categories of such training methods: imitating human data via supervised learning, and self-play to maximize reward in a multi-agent environment, both of which provide rich signals for endowing agents with language-using capabilities. However, these are potentially competing objectives, as maximizing environmental reward can lead to the resulting communication protocol drifting from natural language (Lewis et al., 2017; Lee et al., 2019). The crucial question, then, is how do we best combine self-play and supervised updates? This question has received surprisingly little attention from the emergent communication literature, where the question of how to bridge the gap from emergent protocols to natural language is generally left for future work (Mordatch & Abbeel, 2018; Lazaridou et al., 2018; Cao et al., 2018).\n∗These two authors contributed equally. Work done primarily while RL was at Facebook AI. Correspondence to: ryan.lowe@cs.mcgill.ca, abhinavg@nyu.edu\n1Code is available at https://github.com/backpropper/s2p.\nar X\niv :2\n00 2.\n01 09\n3v 1\n[ cs\n.C L\n] 4\nF eb\n2 02\nOur goal in this paper is to investigate algorithms for combining supervised learning with self-play — which we call supervised self-play (S2P) algorithms — using two classic emergent communication tasks: a Lewis signaling game with symbolic inputs, and a more complicated image-based referential game with natural language descriptions. Our first finding is that supervised learning followed by self-play outperforms emergent communication with supervised fine-tuning in these environments, and we provide three reasons for why this is the case. We then empirically investigate several supervised-first S2P methods in our environments. Existing approaches in this area have used various ad-hoc schedules for alternating between the two kinds of updates (Lazaridou et al., 2017), but to our knowledge there has been no systematic study that has compared these approaches. Lastly, we propose the use of population-based methods for S2P, and find that it leads to improved performance in the more challenging image-based referential game. Our findings highlight the need for further work in combining supervised learning and self-play to develop more sample-efficient language learners."
    }, {
      "heading" : "2 RELATED WORK",
      "text" : "In the past few years, there has been a renewed interest in the field of emergent communication (Sukhbaatar et al., 2016; Foerster et al., 2016; Lazaridou et al., 2017; Havrylov & Titov, 2017) culminating in 3 NeurIPS workshops. Empirical studies have showed that agents can autonomously evolve a communication protocol using discrete symbols when deployed in a multi-agent environment which helps them to play a cooperative or competitive game (Singh et al., 2019; Cao et al., 2018; Choi et al., 2018; Resnick* et al., 2019; Evtimova et al., 2018).\nWhile the idea of promoting coordination among agents through communication sounds promising, recent experiments (Lowe et al., 2019; Chaabouni et al., 2019; Kottur et al., 2017; Jaques et al., 2019) have emphasized the difficulty in learning meaningful emergent communication protocols even with centralized training.\nApart from the above advances in emergent communication, there has been a long outstanding goal of learning intelligent conversational agents to be able to interact with humans. This involves training the artificial agents in a way so that they achieve high scores while solving the task and their language is interpretable by humans or close to natural language. Recent works also add another axis orthogonal to communication where the agent also takes a discrete action in an interactive environment (de Vries et al., 2018; Mul et al., 2019). Lewis et al. (2017) introduced a negotiation task which involves learning linguistic and reasoning skills. They train models imitating human utterances using supervised learning and found that the model generated human-like captions but were poor negotiators. So they perform self-play with these pretrained agents in an interleaved manner and found that the performance improved drastically while avoiding language drift. Lee et al. (2019) also propose using an auxiliary task for grounding the communication to counter language drift. They use visual grounding to learn the semantics of the language while still generating messages that are close to English.\nA recent trend on using population based training for multi-agent communication is a promising avenue for research using inspirations from language evolution literature (Smith et al., 2003; Kirby,\n2014; Raviv & Arnon, 2018). Cultural transmission is one such technique which focuses on the structure and compression of languages, since a language must be used and learned by all individuals of the culture in which it resides and at the same time be suitable for a variety of tasks. Harding Graesser et al. (2019) shows the emergence of linguistic phenomena when a pool of agents contact each other giving rise to novel creole languages. Li & Bowling (2019); Cogswell et al. (2019); Tieleman et al. (2018) have also tried different ways of imposing cultural pressures on agents, by simulating a large population of them and pairing agents to solve a cooperative game with communication. They train the agent against a sampled generation of agents where the generation corresponds to the different languages of the different agent at different times in the history.\nOur work is inspired from these works where we aim to formalize the recent advancements in using self-play in dialog modeling, through the lens of emergent communication."
    }, {
      "heading" : "3 METHODS",
      "text" : ""
    }, {
      "heading" : "3.1 PROBLEM DEFINITION",
      "text" : "Our agents are embedded in a multi-agent environment with N agents where they receive observations o ∈ O (which are functions of a hidden state S) and perform actions a ∈ A . Some actions AL ⊂ A involve sending a message m ∈ AL over a discrete, costless communication channel (i.e. a cheap talk channel (Farrell & Rabin, 1996)). The agents are rewarded with a reward r ∈ R for their performance in the environment. We assume throughout that the environment is cooperative and thus the agents are trained to maximize the sum of rewards R = ∑ t=1:T ∑ i=1:N ri,t across both agents. This can be thought of as a cooperative partially-observable Markov game (Littman (1994)).\nWe define a target language L∗ ∈ L, usually corresponding to natural language, that we want our agents to learn (we further assume L∗ can be used to achieve high task reward). In this paper, we consider a language L ∈ L to be simply a set of valid messages AL and a mapping between observations and messages in the environment, L : O × AL 7→ [0, 1]. For example, in an English image-based referential game (Section 4) this corresponds to the mapping between images and image descriptions in English. We are given a dataset D consisting of |D| (observation, action) pairs, corresponding to Ne ‘experts’ (for us, Ne = 2) playing the game using the target language L∗. Our goal is to train agents to achieve a high reward in the game while speaking language L∗ with an ‘expert’. Specifically, we want our agents to generalize and to perform well on examples that are not contained in D. To summarize, we want agents that can perform well on a collaborative task with English-speaking humans, and we can train them using a supervised dataset D and via self-play."
    }, {
      "heading" : "3.2 SUPERVISED SELF-PLAY (S2P)",
      "text" : "In recent years, there have been several approaches to language learning that have combined supervised or imitation learning with self-play. In this paper, we propose an umbrella term for these algorithms called supervised self-play (S2P). S2P requires two things: (1) a multi-agent environment where at least one agent can send messages over a dedicated communication channel, along with a reward function that measures how well the agents are doing at some task; and (2) a supervised dataset D of experts acting and speaking language L∗ in the environment (such that they perform well on the task). Given these ingredients, we define S2P below (see Figure 2). Definition 3.1. Supervised self-play (S2P). Supervised self-play is a class of language learning algorithms that combines: (1) self-play updates in a multi-agent language environment, and (2) supervised updates on an expert dataset D.\nS2P algorithms can differ in how they combine self-play and supervised learning updates on D. When supervised learning is performed before self-play, we refer to the dataset D as the seed data. Why might we want to train our agents via self-play? Won’t their language diverge from L∗? One way to intuitively understand why S2P is beneficial is to think in terms of constraints. In our set-up, there are two known constraints on the target language L∗: (1) it is consistent with the samples from the supervised dataset D, and (2) L∗ can be used to obtain a high reward in the environment. Thus, finding L∗ can be loosely viewed as a constrained optimization problem, and enforcing both constraints should clearly lead to better performance."
    }, {
      "heading" : "3.3 ALGORITHMS FOR S2P",
      "text" : "Here we describe several methods for S2P training. Our goal is not to exhaustively enumerate all possible optimization strategies, but rather provide a categorization of some well-known ways to combine self-play and supervised learning. To help describe these methods, we further split the seed dataset D into Dtrain, which is used for training, and Dval which is used for early-stopping. We also visualize the schedules in Figure 2.\nEmergent communication with supervised fine-tuning (sp2sup): We first perform self-play updates until the learning converges on the task performance. It is then followed by supervised updates on Dtrain until the listener performance converges on the dataset Dval. Supervised learning with self-play (sup2sp): This is the complement of the above method which involves doing supervised updates until convergence on Dval followed by self-play updates until convergence on the task performance.\nRandom updates (rand): This is the method used in (Lazaridou et al., 2017). At each time step, we sample a bernoulli random variable z ∼ Bernoulli(q) where q is fixed. If z = 1, we perform one supervised update, else we do one self-play update, and repeat until both losses convergence on Dval. Scheduled updates (sched): We first pretrain the listener and the\nspeaker until convergence on Dval. Then we create a schedule, where we perform l self-play updates followed by m supervised updates, and repeat until convergence on the dataset.\nScheduled updates with speaker freezing (sched_frz): This method is based on the findings of Lewis et al. (2017), who do sched S2P while freezing the parameters of the speaker during self-play to reduce the amount of language drift. In our case, we freeze the parameters of the speaker after the initial supervised learning.\nScheduled updates with random speaker freezing (sched_rand_frz): Experimentally, we noticed that sched_frz didn’t perform well in self-play. Thus, we introduce a variation, we sample a bernoulli random variable z ∼ Bernoulli(r) where r is fixed. If z = 1, we freeze the parameters of the speaker during both self-play and supervised learning, else we allow updates to the speaker as well.\n3.4 POPULATION-BASED S2P (POP-S2P)\nAs explained above, the goal of S2P is to produce agents that follow dataset D while maximizing reward in the environment. However, there are many such policies satisfying these criteria. This results in a large space of possible solutions, that increases as the environment grows more complex (but decreases with increasing |D|). Experimentally, we find that this can result in diverse agent policies. We show this in Figure 3 by training 50 randomly initialized agents on the image-based referential game (defined in Sec. 4) the agents can often make diverse predictions for a given image (Figure 3a) and achieve variable performance when playing with other populations with a slight preference towards their own partner (the diagonal in Figure 3b). Inspired by these findings, we propose to aug-\nment S2P by training a population of N agents, and subsequently aggregating them back into a\nsingle agent (the ‘student’). We call this population-based S2P (Pop-S2P). While there are many feasible ways of doing this, in this paper we train the populations by simply randomizing the initial seed, and we aggregate the populations using a simple form of policy distillation (Rusu et al., 2016). Another simple technique to boost performance is via ensembling where we simply take the majority prediction at each time step."
    }, {
      "heading" : "4 ENVIRONMENTS & IMPLEMENTATION DETAILS",
      "text" : "We consider environments based on classical problems in emergent communication. These environments are cooperative and involve the interaction between a speaker, who makes an observation and sends a message, and a listener, who observes the message and makes a prediction (see Figure 1b). Our goal is to train a listener such that it achieves high reward when playing with an expert speaking the target language L∗ on inputs unseen during training.2\nEnvironment 1: Object Reconstruction (OR) Our first game is a Lewis signaling game (Lewis, 1969) and a simpler version of the Task & Talk game from Kottur et al. (2017), with a single turn and a much larger input space. The speaker agent observes an object with a certain set of properties, and must describe the object to the listener using a sequence of words. The listener then attempts to reconstruct the object. More specifically, the input space consists of p properties (e.g. shape, color) of t types each (e.g. triangle, square). The speaker observes a symbolic representation of the input, consisting of the concatenation of p = 6 one-hot vectors, each of length t = 10. The number of possible inputs scales as tp. We define the vocabulary size (length of each one-hot vector sent from the speaker) as |V | = 60, and the number of words (fixed length message) sent to be T = 6. For our target language L∗ for this task, we programatically generate a perfectly compositional language, by assigning each object a unique word. In other words, to describe a ‘blue shaded triangle’, we create a language where the output description would be “blue, triangle, shaded”, in some arbitrary order. By ‘unique symbol’, we mean that no two properties are assigned the same word. The speaker and listener policies are parameterized using a 2-layer linear network (results were similar with added non-linearity and significantly worse with 1-layer linear networks) with 200 hidden units. During both supervised learning and self-play, the listener is trained to minimize the cross-entropy loss over property predictions.\nEnvironment 2: Image-Based Referential game with natural language (IBR) Our second game is the communication task introduced in Lee et al. (2018). The speaker observes a target image d∗, and must describe the image using a set of words. The listener observes the target image along with D distractor images sampled uniformly at random from the training set (for us, D = 9), and the message yd∗ from the speaker, and is rewarded for correctly selecting the target image. For this game, the target language L∗ is English — we obtain English image descriptions using caption data from MS COCO and Flickr30k. We set the vocabulary size |V | = 100, and filter out any descriptions that contain more than 30% unknown tokens while keeping the maximum message length T to 15.\nSimilar to (Mordatch & Abbeel, 2018; Sukhbaatar et al., 2016), we train our agents end-to-end with backpropagation. Since the speaker sends discrete messages, we use the Straight-Through version of Gumbel-Softmax (Jang et al., 2017; Maddison et al., 2017) to allow gradient flow to the speaker during self-play (Jself-play). The speaker’s predictions are trained on the ground truth English captions m∗ using the cross entropy loss Jspk-supervised. The listener is trained using the cross-entropy loss Jlsn-supervised where the logits are the reciprocal of the mean squared error which was found to perform better than directly minimizing MSE loss in Lee et al. (2018). The mean squared error is taken over the listener’s image representation blsn of the distractor (or target) image and the message representation given as input. The loss functions are defined as:\nJspk-supervised(d∗) = − T∑\nt=1\nlog pspk(mt|m<t, d∗)\nJlsn-supervised(m∗, d∗, D) = − D+1∑ d=1 log(softmax(1/plsn(m ∗)− blsn(d))2)\n2Our approach could equally be used to train a speaker of language L∗; we leave this to future work.\nJself-play(d∗, D) = − D+1∑ d=1 log(softmax(1/plsn(yd∗)− blsn(d))2)\nwhere yd∗ is the concatenation of T one-hot vectors ytd∗ = ST-GumbelSoftmax(p t spk).\nWe use the same architecture as described in Lee et al. (2018). The speaker and listener are parameterized by recurrent policies, both using an embedding layer of size 256 followed by a GRU (Cho et al., 2014) of size 512. We provide further hyperparameter details in Table 1 in the Appendix."
    }, {
      "heading" : "5 DO SUPERVISED LEARNING BEFORE SELF-PLAY",
      "text" : "A central question in our work is how to combine supervised and self-play updates for effective pre-training of conversational agents. In this section, we study this question by conducting experiments with two schedules: training with emergent communication followed by supervised learning (sp2sup), and training with supervised learning followed by self-play (sup2sp). We also interpolate between these two regimes by performing the rand and sched on 0 < n < |D| samples, followed by supervised fine-tuning on the remaining |D| − n samples. Our first finding is that it is best to use all of your samples for supervised learning before doing self-play. This can be seen in Figure 4: when all of the samples are used first for supervised learning, the number of total samples required to solve the OR game drastically, and in the IBR game the accuracy for a fixed number of samples is maximized (Figure 4a). While this may seem to be common sense, it in fact runs counter to the prevailing wisdom in some emergent communication literature, where languages are emerged from scratch with the ultimate goal of translating them to natural language.\nTo better understand why it is best to do supervised learning first, we now conduct a set of targeted experiments using the environments from Section 4. Results of our experiments suggest three main explanations:\n(1) Emerging a language is hard. For many environments, with emergent communication it’s often hard to find an equilibrium where the agents meaningfully communicate. The difficulty of ‘emergent language discovery’ has been well-known in emergent communication (Lowe et al., 2017), so we will only briefly discuss it here. In short, to discover a useful communication protocol agents\nhave to coordinate repeatedly over time, which is difficult when agents are randomly initialized, particularly in environments with sparse reward. Compounding the difficulty is that, if neither agent communicates and both agents act optimally given their lack of knowledge, they converge to a Nash equilibrium called babbling equilibrium (Farrell & Rabin, 1996). This equilibrium must be overcome to learn a useful communication protocol. In S2P, the initial language supervision can help overcome the discovery problem, as it provides an initial policy for how agents could usefully communicate (Lewis et al., 2017).\n(2) Emergent languages are different than natural language. Even if one does find an equilibrium where agents communicate and perform well on the task, the distribution of languages they find will usually be very different from natural language. This is a problem because, if the languages obtained through self-play are sufficiently different from L∗, they will not be helpful for learning. This is seen for the OR game in Figure 4a, where 17 samples are required in the seed before S2P outperforms the supervised learning baseline. We speculate that this is due to the different pressures exerted during the emergence of artificial languages and human languages.\nThankfully, we can learn languages closer to L∗ by simply adding more samples to our initial supervised learning phase. We show this in Figure 4b, where we train populations of 50 agents on the IBR game and use Pop-S2P to produce a single distilled agent. With both 1K and 10K initial supervised samples, the distill agent generalizes to agents in the validation set of their population. However, the distilled agent trained with 10000 samples performs significantly better when playing with an expert agent speaking L∗, indicating that the training agents from that population speak languages closer to L∗.\n(3) Starting with self-play violates constraints. Even if you have ‘perfect emergent communication’ that learns a distribution over languages under which L∗ has high probability, current methods of supervised fine-tuning do not properly learn from this distribution. What if we had all the correct learning pressures, such that we emerged a distribution over languages L with structure identical to L∗, and then trained a Pop-S2P agent using this distribution? Surprisingly, we find that S2P with all of the samples in the seed performs better than even this optimistic case, in terms of providing useful information for training a Pop-S2P agent. We conduct an experiment in the OR game where we programmatically define a distribution over compositional languages Lc, of which our target language L∗ is a sample. Each language L ∈ Lc has the same structure and are obtained by randomly permuting the mapping between the word IDs and the corresponding type IDs, along with the order of properties in an utterance. Next, we compare two distilled policies using 50 populations: one is distilled from S2P populations (trained with X samples), and the other is distilled from ‘perfect emergent communication’ and fine-tuned on X samples. As can be seen in Figure 4c, we show that when we train a Pop-S2P agent on 50 of these compositional populations, we still need 3X more samples than regular Pop-S2P (trained on 50 S2P agents with all of the samples in the seed) to reach 95% test accuracy3.\n3Here the value of X is the corresponding number of samples on the horizontal axis in Figure 4c.\nTo understand why this happens, we conduct a case study in an even simpler setting: single-agent S2P in the OR game with p = 1, t = 10, |V | = 10. We find that agents trained via emergent communication consistently learn to solve this task. However, as shown in Figure 5, when subsequently trained via supervised learning on D to learn L∗, the learned language is no longer coherent (it maps different words to the same type) and doesn’t solve the task. On the other hand, agents trained first with supervised learning are able to learn a language that both solves the task and is consistent with D. Intuitively, what’s happening is that the samples in D are also valid for solving the task, since we assume agents speaking L∗ can solve the task. Thus, self-play after supervised learning simply ‘fills in the gaps’ for examples not in D.4 Emergent languages that start with self-play, on the other hand, contain input-output mappings that are inconsistent with L∗, which must be un-learned during subsequent supervised learning.\nIn theory, the above issue could be resolved using Pop-S2P; if the distilled agent could use the population of emergent languages to discover structural rules (e.g. discovering that the languages in the OR game in Figure 4c are compositional), it could use the samples from D to refine a posterior distribution over target languages that is consistent with these rules (e.g. learning the distribution of compositional languages consistent with D). Current approaches to supervised fine-tuning in language, though, do not do this (Lazaridou et al., 2017; Lewis et al., 2017). An interesting direction for future work is examining how to apply Bayesian techniques to S2P.\n6 EXPLORING VARIANTS OF S2P\n6.1 POPULATION-BASED S2P\nIn this section, we aim to show that (1) S2P outperforms the supervised learning baseline, and (2) Pop-S2P outperforms S2P. We conduct our experiments in the more complex IBR game, since the agents must communicate in English, and measure performance by calculating the accuracy at different (fixed) numbers of samples. Our baseline is then the performance of a supervised learner on a fixed number of samples.\nWe show the results in Figure 6. We first note that, when both 1k and 10k samples are used for supervised learning, S2P (sched) outperforms the supervised learning baseline. We can also see that the population-based approach outperforms single agent S2P (sched) by a significant margin. We also compare our distillation method to an ensembling method that keeps all 50 populations at test time, and find that ensembling performs significantly better, although it is much less efficient. This suggests that there is room to push distilled Pop-S2P to even better performance."
    }, {
      "heading" : "6.2 EXAMINING S2P SCHEDULES",
      "text" : "In this section, we aim to: (1) evaluate several S2P schedules empirically on the IBR game; and (2) attain a better understanding of S2P through quantitative experiments.\nParameter freezing improves S2P We show the results comparing different S2P schedules in Figure 7a. We find that in this more complex game, the sup2sp S2P performs much worse than the other options. We also see that adding freezing slightly improves the performance on the target language (Figure 8 in the Appendix also shows that it converges more quickly). We hypothesize that this is because it reduces the language drift that is experienced during each round of self-play updates (Lee et al., 2019). Overall, however, the difference between different S2P schedules is relatively small, and it’s unclear if the same ordering will hold in a different domain.\n4In practice, we find that self-play updates can undo some of the learning of D, which is why we apply an alternating schedule.\nSelf-play acts as a regularizer What is the role of self-play in S2P? We can start to decipher this by taking a closer look at the sched S2P. We plot the training performance of this method in Figure 7b. Interestingly, we notice from the zig-zag pattern that the validation performance usually goes down after every set of self-play updates. However, the overall validation performance goes up after the next round of supervised updates. This is also reflected in the poor performance of the sup2sp S2P in Figure 6.\nThis phenomenon can be explained by framing self-play as a form of regularization: alternating between supervised and self-play updates is a way to satisfy the parallel constraints of ‘is consistent with the dataset D’ and ‘performs well on the task’. We visualize this pictorially in Figure 7b: while a set of self-play updates results in poor performance on D, eventually the learned language moves closer to satisfying both constraints."
    }, {
      "heading" : "7 DISCUSSION",
      "text" : "In this work, we investigated the research question of how to combine supervised and self-play updates, with a focus on training agents to learn a language. However, this research question is not only important for language learning; it is also a important in equilibrium selection and learning social conventions (Lerer & Peysakhovich, 2019) in general games. For example, in robotics there may be a trade-off between performing a task well (moving an object to a certain place) and having your policy be interpretable by humans (so that they will not stumble over you). Examining how to combine supervised and self-play updates in these settings is an exciting direction for future work.\nThere are several axes of complexity not addressed in our environments and problem set-up. First, we consider only single-state environments, and agents don’t have to make temporally extended decisions. Second, we do not consider pre-training on large text corpora that are separate from the desired task (Radford et al., 2019; Devlin et al., 2018). Third, we limit our exploration of self-play to the multi-agent setting, which is not the case in works such as instruction following (Andreas & Klein, 2015). Introducing these elements may result in additional practical considerations for S2P learning, which we leave for future work. Our goal in this paper is not to determine the best method of S2P in all of these settings, but rather to inspire others to use the framing of ‘supervised self-play algorithms’ to make progress on sample efficient human-in-the-loop language learning."
    }, {
      "heading" : "ACKNOWLEDGEMENTS",
      "text" : "We are very grateful to Angeliki Lazaridou, with whom discussions at ICML 2019 shifted the direction of this work considerably. We also thank Jean Harb, Liam Fedus, Amy Zhang, Evgeny Naumov, Cinjon Resnick, Igor Mordatch, and others at MILA and Facebook AI Research for discussions related to the ideas in this paper. Special thanks to Arthur Szlam and Kavya Srinet for discussing their ongoing work with us. RL is supported in part by a Vanier Scholarship."
    }, {
      "heading" : "A HYPERPARAMETERS",
      "text" : "We provide hyperparameter details in Table 1."
    }, {
      "heading" : "B CALCULATION OF OPTIMAL SAMPLE COMPLEXITY IN OR GAME",
      "text" : "Here we provide a quick calculation for how quickly a human might learn a new compositional language L in the OR game in as few examples as possible, which we use as a baseline in Figure 4a. We assume a OR game with p = 6 properties, t = 10 types, T = 6 words sent per message (concatenated together), and |V | = 60 vocabulary size. If this language L is compositional, then each word in the vocabulary is assigned to 1 type. Thus, we need to learn 60 total assignments. In this analysis we assume we can construct (i.e. hand-design) the samples seen by the human, and thus the final number should be considered something like a lower bound.\nSince T = 6, we get information about 6 word←type assignments for every sample. However, this information is entangled as we don’t know which word corresponded to which type. Thus, we (1) divide the problem up by first constructing 9 (word sequence, object) sample pairs where none of the object types overlap between each sample. With this information, we are able to narrow down the word←type assignments into 10 groups of 6 (that is, in each group we have 6 words corresponding to 6 types, but we don’t know which type belongs to which word). Note we don’t need 10 samples as the last one can be inferred by exclusion. (2) We then construct 5 more samples where each type belongs to a separate group. We can do this because t > p. Because each type belongs to a separate group, cross-referencing the words observed from samples in (1) and (2) uniquely defines each word←type assignment. Note again we don’t need 6 samples as the last one can be inferred by exclusion. This gives us a total of 9 + 5 = 14 samples."
    }, {
      "heading" : "C ADDITIONAL PLOTS",
      "text" : "We show training curves for various S2P schedules."
    } ],
    "references" : [ {
      "title" : "Alignment-based compositional semantics for instruction following",
      "author" : [ "Jacob Andreas", "Dan Klein" ],
      "venue" : "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Andreas and Klein.,? \\Q2015\\E",
      "shortCiteRegEx" : "Andreas and Klein.",
      "year" : 2015
    }, {
      "title" : "Emergent communication through negotiation",
      "author" : [ "Kris Cao", "Angeliki Lazaridou", "Marc Lanctot", "Joel Z Leibo", "Karl Tuyls", "Stephen Clark" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Cao et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Cao et al\\.",
      "year" : 2018
    }, {
      "title" : "Anti-efficient encoding in emergent communication",
      "author" : [ "Rahma Chaabouni", "Eugene Kharitonov", "Emmanuel Dupoux", "Marco Baroni" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Chaabouni et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chaabouni et al\\.",
      "year" : 2019
    }, {
      "title" : "BabyAI: First steps towards grounded language learning with a human in the loop",
      "author" : [ "Maxime Chevalier-Boisvert", "Dzmitry Bahdanau", "Salem Lahlou", "Lucas Willems", "Chitwan Saharia", "Thien Huu Nguyen", "Yoshua Bengio" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Chevalier.Boisvert et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Chevalier.Boisvert et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning phrase representations using RNN encoder–decoder for statistical machine translation",
      "author" : [ "Kyunghyun Cho", "Bart van Merriënboer", "Caglar Gulcehre", "Dzmitry Bahdanau", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio" ],
      "venue" : "In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP),",
      "citeRegEx" : "Cho et al\\.,? \\Q2014\\E",
      "shortCiteRegEx" : "Cho et al\\.",
      "year" : 2014
    }, {
      "title" : "Multi-agent compositional communication learning from raw visual input",
      "author" : [ "Edward Choi", "Angeliki Lazaridou", "Nando de Freitas" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Choi et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Choi et al\\.",
      "year" : 2018
    }, {
      "title" : "Emergence of Compositional Language with Deep Generational Transmission",
      "author" : [ "Michael Cogswell", "Jiasen Lu", "Stefan Lee", "Devi Parikh", "Dhruv Batra" ],
      "venue" : null,
      "citeRegEx" : "Cogswell et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Cogswell et al\\.",
      "year" : 2019
    }, {
      "title" : "Talk the walk: Navigating new york city through grounded dialogue",
      "author" : [ "Harm de Vries", "Kurt Shuster", "Dhruv Batra", "Devi Parikh", "Jason Weston", "Douwe Kiela" ],
      "venue" : "arXiv preprint arXiv:1807.03367,",
      "citeRegEx" : "Vries et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Vries et al\\.",
      "year" : 2018
    }, {
      "title" : "Pre-training of deep bidirectional transformers for language understanding",
      "author" : [ "Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova. Bert" ],
      "venue" : "arXiv preprint arXiv:1810.04805,",
      "citeRegEx" : "Devlin et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Devlin et al\\.",
      "year" : 2018
    }, {
      "title" : "Emergent communication in a multi-modal, multi-step referential game",
      "author" : [ "Katrina Evtimova", "Andrew Drozdov", "Douwe Kiela", "Kyunghyun Cho" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Evtimova et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Evtimova et al\\.",
      "year" : 2018
    }, {
      "title" : "Cheap talk",
      "author" : [ "Joseph Farrell", "Matthew Rabin" ],
      "venue" : "Journal of Economic Perspectives,",
      "citeRegEx" : "Farrell and Rabin.,? \\Q1996\\E",
      "shortCiteRegEx" : "Farrell and Rabin.",
      "year" : 1996
    }, {
      "title" : "Learning to Communicate with Deep Multi-Agent Reinforcement Learning",
      "author" : [ "Jakob Foerster", "Ioannis Alexandros Assael", "Nando de Freitas", "Shimon Whiteson" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Foerster et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Foerster et al\\.",
      "year" : 2016
    }, {
      "title" : "Emergent linguistic phenomena in multi-agent communication games",
      "author" : [ "Laura Harding Graesser", "Kyunghyun Cho", "Douwe Kiela" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Graesser et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Graesser et al\\.",
      "year" : 2019
    }, {
      "title" : "Emergence of Language with Multi-agent Games: Learning to Communicate with Sequences of Symbols",
      "author" : [ "Serhii Havrylov", "Ivan Titov" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Havrylov and Titov.,? \\Q2017\\E",
      "shortCiteRegEx" : "Havrylov and Titov.",
      "year" : 2017
    }, {
      "title" : "Categorical Reparameterization with GumbelSoftmax",
      "author" : [ "Eric Jang", "Shixiang Gu", "Ben Poole" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Jang et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Jang et al\\.",
      "year" : 2017
    }, {
      "title" : "Social Influence as Intrinsic Motivation for Multi-Agent Deep Reinforcement Learning",
      "author" : [ "Natasha Jaques", "Angeliki Lazaridou", "Edward Hughes", "Caglar Gulcehre", "Pedro Ortega", "Dj Strouse", "Joel Z. Leibo", "Nando De Freitas" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Jaques et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Jaques et al\\.",
      "year" : 2019
    }, {
      "title" : "Iterated learning and the evolution of language",
      "author" : [ "Simon Kirby" ],
      "venue" : "Current Opinion in Neurobiology, pp",
      "citeRegEx" : "Kirby.,? \\Q2014\\E",
      "shortCiteRegEx" : "Kirby.",
      "year" : 2014
    }, {
      "title" : "Natural language does not emerge ‘naturally’ in multi-agent dialog",
      "author" : [ "Satwik Kottur", "José Moura", "Stefan Lee", "Dhruv Batra" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Kottur et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Kottur et al\\.",
      "year" : 2017
    }, {
      "title" : "Multi-Agent Cooperation and the Emergence of (Natural) Language",
      "author" : [ "Angeliki Lazaridou", "Alexander Peysakhovich", "Marco Baroni" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2017
    }, {
      "title" : "Emergence of linguistic communication from referential games with symbolic and pixel input",
      "author" : [ "Angeliki Lazaridou", "Karl Moritz Hermann", "Karl Tuyls", "Stephen Clark" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Lazaridou et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lazaridou et al\\.",
      "year" : 2018
    }, {
      "title" : "Emergent translation in multiagent communication",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Jason Weston", "Douwe Kiela" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Lee et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2018
    }, {
      "title" : "Countering language drift via visual grounding",
      "author" : [ "Jason Lee", "Kyunghyun Cho", "Douwe Kiela" ],
      "venue" : "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),",
      "citeRegEx" : "Lee et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lee et al\\.",
      "year" : 2019
    }, {
      "title" : "Learning existing social conventions via observationally augmented self-play",
      "author" : [ "Adam Lerer", "Alexander Peysakhovich" ],
      "venue" : "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society,",
      "citeRegEx" : "Lerer and Peysakhovich.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lerer and Peysakhovich.",
      "year" : 2019
    }, {
      "title" : "Convention: A philosophical study",
      "author" : [ "David Lewis" ],
      "venue" : null,
      "citeRegEx" : "Lewis.,? \\Q1969\\E",
      "shortCiteRegEx" : "Lewis.",
      "year" : 1969
    }, {
      "title" : "Deal or no deal? endto-end learning of negotiation dialogues",
      "author" : [ "Mike Lewis", "Denis Yarats", "Yann Dauphin", "Devi Parikh", "Dhruv Batra" ],
      "venue" : "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "citeRegEx" : "Lewis et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lewis et al\\.",
      "year" : 2017
    }, {
      "title" : "Ease-of-Teaching and Language Structure from Emergent Communication",
      "author" : [ "Fushan Li", "Michael Bowling" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Li and Bowling.,? \\Q2019\\E",
      "shortCiteRegEx" : "Li and Bowling.",
      "year" : 2019
    }, {
      "title" : "Markov games as a framework for multi-agent reinforcement learning",
      "author" : [ "Michael L Littman" ],
      "venue" : "In International Conference on Machine Learning,",
      "citeRegEx" : "Littman.,? \\Q1994\\E",
      "shortCiteRegEx" : "Littman.",
      "year" : 1994
    }, {
      "title" : "MultiAgent Actor-Critic for Mixed Cooperative-Competitive Environments",
      "author" : [ "Ryan Lowe", "YI WU", "Aviv Tamar", "Jean Harb", "OpenAI Pieter Abbeel", "Igor Mordatch" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Lowe et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2017
    }, {
      "title" : "On the pitfalls of measuring emergent communication",
      "author" : [ "Ryan Lowe", "Jakob Foerster", "Y-Lan Boureau", "Joelle Pineau", "Yann Dauphin" ],
      "venue" : "In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems,",
      "citeRegEx" : "Lowe et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Lowe et al\\.",
      "year" : 2019
    }, {
      "title" : "The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables",
      "author" : [ "Chris J. Maddison", "Andriy Mnih", "Yee Whye Teh" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Maddison et al\\.,? \\Q2017\\E",
      "shortCiteRegEx" : "Maddison et al\\.",
      "year" : 2017
    }, {
      "title" : "Emergence of grounded compositional language in multi-agent populations",
      "author" : [ "Igor Mordatch", "Pieter Abbeel" ],
      "venue" : "In AAAI Conference on Artificial Intelligence,",
      "citeRegEx" : "Mordatch and Abbeel.,? \\Q2018\\E",
      "shortCiteRegEx" : "Mordatch and Abbeel.",
      "year" : 2018
    }, {
      "title" : "Mastering emergent language: learning to guide in simulated navigation",
      "author" : [ "Mathijs Mul", "Diane Bouchacourt", "Elia Bruni" ],
      "venue" : "[cs],",
      "citeRegEx" : "Mul et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Mul et al\\.",
      "year" : 2019
    }, {
      "title" : "Language models are unsupervised multitask learners",
      "author" : [ "Alec Radford", "Jeffrey Wu", "Rewon Child", "David Luan", "Dario Amodei", "Ilya Sutskever" ],
      "venue" : null,
      "citeRegEx" : "Radford et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Radford et al\\.",
      "year" : 2019
    }, {
      "title" : "Systematicity, but not compositionality: Examining the emergence of linguistic structure in children and adults using iterated learning",
      "author" : [ "Limor Raviv", "Inbal Arnon" ],
      "venue" : "Cognition,",
      "citeRegEx" : "Raviv and Arnon.,? \\Q2018\\E",
      "shortCiteRegEx" : "Raviv and Arnon.",
      "year" : 2018
    }, {
      "title" : "Capacity, bandwidth, and compositionality in emergent language",
      "author" : [ "Cinjon Resnick", "Abhinav Gupta", "Jakob N. Foerster", "Andrew M. Dai", "Kyunghyun Cho" ],
      "venue" : "learning. ArXiv,",
      "citeRegEx" : "Resnick. et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Resnick. et al\\.",
      "year" : 2019
    }, {
      "title" : "Policy distillation",
      "author" : [ "Andrei A Rusu", "Sergio Gomez Colmenarejo", "Caglar Gulcehre", "Guillaume Desjardins", "James Kirkpatrick", "Razvan Pascanu", "Volodymyr Mnih", "Koray Kavukcuoglu", "Raia Hadsell" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Rusu et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Rusu et al\\.",
      "year" : 2016
    }, {
      "title" : "Individualized controlled continuous communication model for multiagent cooperative and competitive tasks",
      "author" : [ "Amanpreet Singh", "Tushar Jain", "Sainbayar Sukhbaatar" ],
      "venue" : "In International Conference on Learning Representations,",
      "citeRegEx" : "Singh et al\\.,? \\Q2019\\E",
      "shortCiteRegEx" : "Singh et al\\.",
      "year" : 2019
    }, {
      "title" : "Complex Systems In Language Evolution: The Cultural Emergence Of Compositional Structure",
      "author" : [ "Kenny Smith", "Henry Brighton", "Simon Kirby" ],
      "venue" : "Advances in Complex Systems (ACS),",
      "citeRegEx" : "Smith et al\\.,? \\Q2003\\E",
      "shortCiteRegEx" : "Smith et al\\.",
      "year" : 2003
    }, {
      "title" : "Learning multiagent communication with backpropagation",
      "author" : [ "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus" ],
      "venue" : "Advances in Neural Information Processing Systems",
      "citeRegEx" : "Sukhbaatar et al\\.,? \\Q2016\\E",
      "shortCiteRegEx" : "Sukhbaatar et al\\.",
      "year" : 2016
    }, {
      "title" : "Shaping representations through communication. 2018",
      "author" : [ "Olivier Tieleman", "Angeliki Lazaridou", "Shibl Mourad", "Charles Blundell", "Doina Precup" ],
      "venue" : "URL https://openreview.net/pdf?id= HkzL4hR9Ym",
      "citeRegEx" : "Tieleman et al\\.,? \\Q2018\\E",
      "shortCiteRegEx" : "Tieleman et al\\.",
      "year" : 2018
    } ],
    "referenceMentions" : [ {
      "referenceID" : 3,
      "context" : "from deep reinforcement learning) are too data-inefficient to be trained in this way from scratch (Chevalier-Boisvert et al., 2019).",
      "startOffset" : 98,
      "endOffset" : 131
    }, {
      "referenceID" : 24,
      "context" : "However, these are potentially competing objectives, as maximizing environmental reward can lead to the resulting communication protocol drifting from natural language (Lewis et al., 2017; Lee et al., 2019).",
      "startOffset" : 168,
      "endOffset" : 206
    }, {
      "referenceID" : 21,
      "context" : "However, these are potentially competing objectives, as maximizing environmental reward can lead to the resulting communication protocol drifting from natural language (Lewis et al., 2017; Lee et al., 2019).",
      "startOffset" : 168,
      "endOffset" : 206
    }, {
      "referenceID" : 19,
      "context" : "The crucial question, then, is how do we best combine self-play and supervised updates? This question has received surprisingly little attention from the emergent communication literature, where the question of how to bridge the gap from emergent protocols to natural language is generally left for future work (Mordatch & Abbeel, 2018; Lazaridou et al., 2018; Cao et al., 2018).",
      "startOffset" : 311,
      "endOffset" : 378
    }, {
      "referenceID" : 1,
      "context" : "The crucial question, then, is how do we best combine self-play and supervised updates? This question has received surprisingly little attention from the emergent communication literature, where the question of how to bridge the gap from emergent protocols to natural language is generally left for future work (Mordatch & Abbeel, 2018; Lazaridou et al., 2018; Cao et al., 2018).",
      "startOffset" : 311,
      "endOffset" : 378
    }, {
      "referenceID" : 18,
      "context" : "Existing approaches in this area have used various ad-hoc schedules for alternating between the two kinds of updates (Lazaridou et al., 2017), but to our knowledge there has been no systematic study that has compared these approaches.",
      "startOffset" : 117,
      "endOffset" : 141
    }, {
      "referenceID" : 38,
      "context" : "In the past few years, there has been a renewed interest in the field of emergent communication (Sukhbaatar et al., 2016; Foerster et al., 2016; Lazaridou et al., 2017; Havrylov & Titov, 2017) culminating in 3 NeurIPS workshops.",
      "startOffset" : 96,
      "endOffset" : 192
    }, {
      "referenceID" : 11,
      "context" : "In the past few years, there has been a renewed interest in the field of emergent communication (Sukhbaatar et al., 2016; Foerster et al., 2016; Lazaridou et al., 2017; Havrylov & Titov, 2017) culminating in 3 NeurIPS workshops.",
      "startOffset" : 96,
      "endOffset" : 192
    }, {
      "referenceID" : 18,
      "context" : "In the past few years, there has been a renewed interest in the field of emergent communication (Sukhbaatar et al., 2016; Foerster et al., 2016; Lazaridou et al., 2017; Havrylov & Titov, 2017) culminating in 3 NeurIPS workshops.",
      "startOffset" : 96,
      "endOffset" : 192
    }, {
      "referenceID" : 36,
      "context" : "Empirical studies have showed that agents can autonomously evolve a communication protocol using discrete symbols when deployed in a multi-agent environment which helps them to play a cooperative or competitive game (Singh et al., 2019; Cao et al., 2018; Choi et al., 2018; Resnick* et al., 2019; Evtimova et al., 2018).",
      "startOffset" : 216,
      "endOffset" : 319
    }, {
      "referenceID" : 1,
      "context" : "Empirical studies have showed that agents can autonomously evolve a communication protocol using discrete symbols when deployed in a multi-agent environment which helps them to play a cooperative or competitive game (Singh et al., 2019; Cao et al., 2018; Choi et al., 2018; Resnick* et al., 2019; Evtimova et al., 2018).",
      "startOffset" : 216,
      "endOffset" : 319
    }, {
      "referenceID" : 5,
      "context" : "Empirical studies have showed that agents can autonomously evolve a communication protocol using discrete symbols when deployed in a multi-agent environment which helps them to play a cooperative or competitive game (Singh et al., 2019; Cao et al., 2018; Choi et al., 2018; Resnick* et al., 2019; Evtimova et al., 2018).",
      "startOffset" : 216,
      "endOffset" : 319
    }, {
      "referenceID" : 34,
      "context" : "Empirical studies have showed that agents can autonomously evolve a communication protocol using discrete symbols when deployed in a multi-agent environment which helps them to play a cooperative or competitive game (Singh et al., 2019; Cao et al., 2018; Choi et al., 2018; Resnick* et al., 2019; Evtimova et al., 2018).",
      "startOffset" : 216,
      "endOffset" : 319
    }, {
      "referenceID" : 9,
      "context" : "Empirical studies have showed that agents can autonomously evolve a communication protocol using discrete symbols when deployed in a multi-agent environment which helps them to play a cooperative or competitive game (Singh et al., 2019; Cao et al., 2018; Choi et al., 2018; Resnick* et al., 2019; Evtimova et al., 2018).",
      "startOffset" : 216,
      "endOffset" : 319
    }, {
      "referenceID" : 28,
      "context" : "While the idea of promoting coordination among agents through communication sounds promising, recent experiments (Lowe et al., 2019; Chaabouni et al., 2019; Kottur et al., 2017; Jaques et al., 2019) have emphasized the difficulty in learning meaningful emergent communication protocols even with centralized training.",
      "startOffset" : 113,
      "endOffset" : 198
    }, {
      "referenceID" : 2,
      "context" : "While the idea of promoting coordination among agents through communication sounds promising, recent experiments (Lowe et al., 2019; Chaabouni et al., 2019; Kottur et al., 2017; Jaques et al., 2019) have emphasized the difficulty in learning meaningful emergent communication protocols even with centralized training.",
      "startOffset" : 113,
      "endOffset" : 198
    }, {
      "referenceID" : 17,
      "context" : "While the idea of promoting coordination among agents through communication sounds promising, recent experiments (Lowe et al., 2019; Chaabouni et al., 2019; Kottur et al., 2017; Jaques et al., 2019) have emphasized the difficulty in learning meaningful emergent communication protocols even with centralized training.",
      "startOffset" : 113,
      "endOffset" : 198
    }, {
      "referenceID" : 15,
      "context" : "While the idea of promoting coordination among agents through communication sounds promising, recent experiments (Lowe et al., 2019; Chaabouni et al., 2019; Kottur et al., 2017; Jaques et al., 2019) have emphasized the difficulty in learning meaningful emergent communication protocols even with centralized training.",
      "startOffset" : 113,
      "endOffset" : 198
    }, {
      "referenceID" : 31,
      "context" : "Recent works also add another axis orthogonal to communication where the agent also takes a discrete action in an interactive environment (de Vries et al., 2018; Mul et al., 2019).",
      "startOffset" : 138,
      "endOffset" : 179
    }, {
      "referenceID" : 18,
      "context" : "Random updates (rand): This is the method used in (Lazaridou et al., 2017).",
      "startOffset" : 50,
      "endOffset" : 74
    }, {
      "referenceID" : 35,
      "context" : "While there are many feasible ways of doing this, in this paper we train the populations by simply randomizing the initial seed, and we aggregate the populations using a simple form of policy distillation (Rusu et al., 2016).",
      "startOffset" : 205,
      "endOffset" : 224
    }, {
      "referenceID" : 23,
      "context" : "Environment 1: Object Reconstruction (OR) Our first game is a Lewis signaling game (Lewis, 1969) and a simpler version of the Task & Talk game from Kottur et al.",
      "startOffset" : 83,
      "endOffset" : 96
    }, {
      "referenceID" : 38,
      "context" : "Similar to (Mordatch & Abbeel, 2018; Sukhbaatar et al., 2016), we train our agents end-to-end with backpropagation.",
      "startOffset" : 11,
      "endOffset" : 61
    }, {
      "referenceID" : 14,
      "context" : "Since the speaker sends discrete messages, we use the Straight-Through version of Gumbel-Softmax (Jang et al., 2017; Maddison et al., 2017) to allow gradient flow to the speaker during self-play (Jself-play).",
      "startOffset" : 97,
      "endOffset" : 139
    }, {
      "referenceID" : 29,
      "context" : "Since the speaker sends discrete messages, we use the Straight-Through version of Gumbel-Softmax (Jang et al., 2017; Maddison et al., 2017) to allow gradient flow to the speaker during self-play (Jself-play).",
      "startOffset" : 97,
      "endOffset" : 139
    }, {
      "referenceID" : 4,
      "context" : "The speaker and listener are parameterized by recurrent policies, both using an embedding layer of size 256 followed by a GRU (Cho et al., 2014) of size 512.",
      "startOffset" : 126,
      "endOffset" : 144
    }, {
      "referenceID" : 27,
      "context" : "The difficulty of ‘emergent language discovery’ has been well-known in emergent communication (Lowe et al., 2017), so we will only briefly discuss it here.",
      "startOffset" : 94,
      "endOffset" : 113
    }, {
      "referenceID" : 24,
      "context" : "In S2P, the initial language supervision can help overcome the discovery problem, as it provides an initial policy for how agents could usefully communicate (Lewis et al., 2017).",
      "startOffset" : 157,
      "endOffset" : 177
    }, {
      "referenceID" : 18,
      "context" : "Current approaches to supervised fine-tuning in language, though, do not do this (Lazaridou et al., 2017; Lewis et al., 2017).",
      "startOffset" : 81,
      "endOffset" : 125
    }, {
      "referenceID" : 24,
      "context" : "Current approaches to supervised fine-tuning in language, though, do not do this (Lazaridou et al., 2017; Lewis et al., 2017).",
      "startOffset" : 81,
      "endOffset" : 125
    }, {
      "referenceID" : 21,
      "context" : "We hypothesize that this is because it reduces the language drift that is experienced during each round of self-play updates (Lee et al., 2019).",
      "startOffset" : 125,
      "endOffset" : 143
    }, {
      "referenceID" : 32,
      "context" : "Second, we do not consider pre-training on large text corpora that are separate from the desired task (Radford et al., 2019; Devlin et al., 2018).",
      "startOffset" : 102,
      "endOffset" : 145
    }, {
      "referenceID" : 8,
      "context" : "Second, we do not consider pre-training on large text corpora that are separate from the desired task (Radford et al., 2019; Devlin et al., 2018).",
      "startOffset" : 102,
      "endOffset" : 145
    } ],
    "year" : 2020,
    "abstractText" : "A promising approach for teaching artificial agents to use natural language involves using human-in-the-loop training. However, recent work suggests that current machine learning methods are too data inefficient to be trained in this way from scratch. In this paper, we investigate the relationship between two categories of learning signals with the ultimate goal of improving sample efficiency: imitating human language data via supervised learning, and maximizing reward in a simulated multi-agent environment via self-play (as done in emergent communication), and introduce the term supervised self-play (S2P) for algorithms using both of these signals. We find that first training agents via supervised learning on human data followed by self-play outperforms the converse, suggesting that it is not beneficial to emerge languages from scratch. We then empirically investigate various S2P schedules that begin with supervised learning in two environments: a Lewis signaling game with symbolic inputs, and an image-based referential game with natural language descriptions. Lastly, we introduce population based approaches to S2P, which further improves the performance over single-agent methods.1",
    "creator" : "LaTeX with hyperref package"
  }
}