{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:15.607457Z",
     "start_time": "2020-06-30T21:13:15.029786Z"
    }
   },
   "outputs": [],
   "source": [
    "### Imports and configuration\n",
    "\n",
    "# setup variables\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "from s2orc.config import CURRENT_VERSION\n",
    "\n",
    "# jsonlines https://jsonlines.readthedocs.io/en/latest/#api\n",
    "import jsonlines\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import hiplot\n",
    "\n",
    "LOCAL_S2ORC_DIR = 's2orc-data'\n",
    "\n",
    "psychology_paper_dir = os.path.join(LOCAL_S2ORC_DIR, CURRENT_VERSION, 'psychology')\n",
    "psychology_paper_suffix = 'psych.text.jsonl'\n",
    "\n",
    "links_dir = os.path.join(LOCAL_S2ORC_DIR, CURRENT_VERSION, 'psych_links')\n",
    "links_suffix = 'psych.text.link.jsonl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:17.341562Z",
     "start_time": "2020-06-30T21:13:15.609273Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:03<00:00, 25.39it/s]\n"
     ]
    }
   ],
   "source": [
    "## Get corpus into memory\n",
    "\n",
    "start = 0\n",
    "span = 100 # all: 1700\n",
    "\n",
    "links = []\n",
    "\n",
    "links_files = sorted(os.listdir(links_dir), key=lambda f: int(f.split('.')[0]))[start:(start+span)]\n",
    "for link_file in tqdm.tqdm(links_files):\n",
    "    with gzip.open(os.path.join(links_dir, link_file), 'rb') as f_in:\n",
    "        batch_links = list(jsonlines.Reader(f_in))\n",
    "        for link in batch_links:\n",
    "            if link['citing_paper']['grobid_parse'].get('body_text') is not None and link['cited_paper']['grobid_parse'].get('body_text') is not None:\n",
    "                links.append(link)\n",
    "\n",
    "np.random.seed(2134234)\n",
    "links = np.array(links)\n",
    "np.random.shuffle(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:20.428985Z",
     "start_time": "2020-06-30T21:13:17.344340Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings, DocumentPoolEmbeddings, TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:49:09.259061Z",
     "start_time": "2020-07-01T00:49:09.235899Z"
    }
   },
   "outputs": [],
   "source": [
    "import regex\n",
    "from syntok.segmenter import split\n",
    "from syntok.tokenizer import Tokenizer\n",
    "\n",
    "token_filter_re = r'^\\p{L}(\\p{L}|\\.|[0-9])+$'\n",
    "science_blacklist = {'et', 'al', 'al.'} # remove words specific for scientific papers without any importance for the task\n",
    "\n",
    "def filter_tokens(tokens):\n",
    "    # Filter out non-common-word/non-real-world-entity tokens\n",
    "    return [\n",
    "        token for token in tokens if \n",
    "            regex.match(token_filter_re, token.text) \n",
    "            and token.text not in science_blacklist\n",
    "    ]\n",
    "\n",
    "author_re = r'(((de|De|van|Van|von|Von)\\s+(\\p{Ll}+\\s+)?)?\\p{Lu}(\\p{Ll}|-)+)'\n",
    "\n",
    "citation_re = (\n",
    "               r'([(;,]|\\s)'\n",
    "            + author_re +\n",
    "               r'(\\s?((([&,]|and|\\s)+\\s?' + author_re + r')|(et al\\.?)))?' # alternative second author\n",
    "               r'('\n",
    "                 r'([;,]|\\s)+'\n",
    "                 r'\\s*[0-9]{4}\\p{Ll}?\\s*' # year\n",
    "               r')+'\n",
    "               r'([);,]|\\s)'\n",
    "              )\n",
    "\n",
    "in_text_citation_re = (\n",
    "    author_re + r'\\s*\\((\\s*[0-9]{4}\\p{Ll}?\\s*,?)\\)'\n",
    ")\n",
    "def filter_citations(text):\n",
    "    n_subs_made = 1\n",
    "    while n_subs_made > 0:\n",
    "        text, n_subs_made = regex.subn(citation_re, ';', text)\n",
    "        text, n_subs_made2 = regex.subn(citation_re, ';', text)\n",
    "        n_subs_made += n_subs_made2\n",
    "    return text\n",
    "\n",
    "def process_section_to_chunks(text):\n",
    "    chunksize = 1\n",
    "    tokenized_sents = list(split(Tokenizer().tokenize(text)))\n",
    "    sents = [' '.join(str(token) for token in sent) for sent in tokenized_sents]\n",
    "    sents = [' '.join(sents[i:i+chunksize]) for i in range(len(sents)-chunksize)]\n",
    "    sentences = []\n",
    "    for sent in sents:\n",
    "        sent = text_to_sentence(sent)\n",
    "        if sent is not None:\n",
    "            sentences.append(sent)\n",
    "    return sentences\n",
    "\n",
    "def text_to_sentence(text):\n",
    "    text = filter_citations(text) \n",
    "    s = Sentence(text, use_tokenizer=True)\n",
    "    s.tokens = filter_tokens(s.tokens)\n",
    "    if len(s.tokens) > 0:\n",
    "        return s\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:20.457917Z",
     "start_time": "2020-06-30T21:13:20.450560Z"
    }
   },
   "outputs": [],
   "source": [
    "# extract the important information from the original S2ORC corpus format\n",
    "def process_link(link, process_section):\n",
    "    context = link['citation_context']\n",
    "    citing_paper = link['citing_paper']\n",
    "    cited_paper = link['cited_paper']\n",
    "    \n",
    "    parts = []\n",
    "    for text_chunk in cited_paper['grobid_parse']['body_text']:\n",
    "        text = text_chunk.get('text')\n",
    "        if text is not None:\n",
    "            chunk_parts = process_section(text)\n",
    "            parts.extend(chunk_parts)\n",
    "    citing_string = ''.join([context['pre_context'], context['context_string'], context['post_context']])\n",
    "    return {\n",
    "        'citing_str': context['context_string'],\n",
    "        'citing_context': citing_string,\n",
    "        'citing_context_part': text_to_sentence(citing_string),\n",
    "        'cited_text_parts': parts,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:20.471720Z",
     "start_time": "2020-06-30T21:13:20.460677Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_embedding_scores(link, metrics, embedding_name, embedding):\n",
    "    all_sim = {}\n",
    "    \n",
    "    s = link['citing_context_part']\n",
    "    embedding.embed(s)\n",
    "    citation_embedding = s.embedding.detach().numpy()\n",
    "\n",
    "    sentences = link['cited_text_parts']\n",
    "    for sentence in sentences:\n",
    "        embedding.embed(sentence)\n",
    "        all_sim[sentence.to_plain_string()] = {}\n",
    "        for name, metric in metrics.items():\n",
    "            sim = 1 - metric(sentence.embedding.detach().numpy(), citation_embedding)\n",
    "            full_name = '_'.join([embedding_name, name])\n",
    "            all_sim[sentence.to_plain_string()][full_name] = sim\n",
    "        sentence.clear_embeddings()\n",
    "    s.clear_embeddings()\n",
    "    \n",
    "    return pd.DataFrame(all_sim).T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words (unweighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:20.479335Z",
     "start_time": "2020-06-30T21:13:20.473929Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_bow_occurrence_score(link, name):\n",
    "    scores = []\n",
    "    s = Sentence(link['citing_context'], use_tokenizer=True)\n",
    "    context_token_set = set(token.text for token in s.tokens)\n",
    "    sentences = link['cited_text_parts']\n",
    "    for sentence in sentences:\n",
    "        sent_token_set = set(token.text for token in sentence.tokens)\n",
    "        score = len(sent_token_set.intersection(context_token_set)) / len(sent_token_set.union(context_token_set))            \n",
    "        scores.append(score)\n",
    "    return pd.DataFrame({\n",
    "        name: scores\n",
    "    }, index=[sent.to_plain_string() for sent in sentences])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of words weighted by Inverse Term Frequency (with lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:51:16.699962Z",
     "start_time": "2020-07-01T00:51:16.688560Z"
    }
   },
   "outputs": [],
   "source": [
    "from wordfreq import word_frequency\n",
    "from collections import defaultdict\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import skfuzzy as fuzz\n",
    "\n",
    "def get_length_penalty(sent_len):\n",
    "    # 0.5 value for len=2\n",
    "    # quickly raising to 1 at ~10 words\n",
    "    return fuzz.sigmf(sent_len, 2, 0.5)\n",
    "\n",
    "def get_word_itf(string_tokens):\n",
    "    min_val = 10e-7\n",
    "    return [1./max(min_val, word_frequency(token, 'en')) for token in string_tokens]\n",
    "\n",
    "def transform_tokens(string_tokens, lemmatizer):\n",
    "    return [lemmatizer.lemmatize(token.lower()) for token in string_tokens]\n",
    "\n",
    "def calc_bow_itf_score(link, name):\n",
    "    \"\"\"\n",
    "    Score using term frequency\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    scores = []\n",
    "    s = link['citing_context_part']\n",
    "    s_tokens = [token.text for token in s.tokens]\n",
    "    context_token_set = set(transform_tokens(s_tokens, lemmatizer=lemmatizer))\n",
    "    sentences = link['cited_text_parts']\n",
    "    for sentence in sentences:\n",
    "        sent_tokens = [token.text for token in sentence.tokens]\n",
    "        sent_token_set = set(transform_tokens(sent_tokens, lemmatizer))\n",
    "        if len(sent_token_set) == 0:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        matching_tokens = sent_token_set.intersection(context_token_set)\n",
    "        all_tokens = sent_token_set.union(context_token_set)\n",
    "        raw_score = np.sum(get_word_itf(matching_tokens)) / np.sum(get_word_itf(all_tokens))\n",
    "        # Eliminate artifacts by penalizing extremely short matches\n",
    "        raw_score *= get_length_penalty(len(sent_token_set))\n",
    "        if np.isclose(raw_score, 0):\n",
    "            score = 0.\n",
    "        else:\n",
    "            score = 1./-np.log(raw_score)\n",
    "        scores.append(score)\n",
    "    return pd.DataFrame({\n",
    "        name: scores\n",
    "    }, index=[sent.to_plain_string() for sent in sentences])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TS SS distance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:20.775775Z",
     "start_time": "2020-06-30T21:13:20.761633Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "from scipy.linalg import norm\n",
    "\n",
    "#the below functions are slightly modified versions \n",
    "#of the ones in https://github.com/taki0112/Vector_Similarity\n",
    "\n",
    "def TS_SS(vec1, vec2) :\n",
    "    val = Triangle(vec1, vec2) * Sector(vec1, vec2)\n",
    "    return min(1, val/3)\n",
    "\n",
    "def Triangle(vec1, vec2) :\n",
    "    theta = math.radians(Theta(vec1,vec2))\n",
    "    return (norm(vec1) * norm(vec2) * math.sin(theta)) / 2\n",
    "\n",
    "def Theta(vec1, vec2) :\n",
    "    return math.acos(1 - distance.cosine(vec1, vec2)) + math.radians(10)\n",
    "\n",
    "def Magnitude_Difference(vec1, vec2) :\n",
    "    return abs(norm(vec1) - norm(vec2))\n",
    "\n",
    "def Sector(vec1, vec2) :\n",
    "    ED = distance.euclidean(vec1, vec2)\n",
    "    MD = Magnitude_Difference(vec1, vec2)\n",
    "    theta = Theta(vec1, vec2)\n",
    "    return math.pi * math.pow((ED+MD),2) * theta/360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.059245Z",
     "start_time": "2020-06-30T21:13:20.778094Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_embedding = TransformerDocumentEmbeddings('bert-base-uncased', fine_tune=False)\n",
    "roberta_embedding = TransformerDocumentEmbeddings('roberta-base', fine_tune=False)\n",
    "glove_embedding = DocumentPoolEmbeddings([WordEmbeddings('glove')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.064526Z",
     "start_time": "2020-06-30T21:13:37.060830Z"
    }
   },
   "outputs": [],
   "source": [
    "n_train_links = int(0.5 * len(links))\n",
    "n_validation_links = int(0.2 * len(links))\n",
    "n_test_links = len(links) - n_train_links - n_validation_links\n",
    "train_links = links[:15]\n",
    "validation_links = links[15:45]\n",
    "test_links = links[-n_test_links:]\n",
    "#links = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.072464Z",
     "start_time": "2020-06-30T21:13:37.066269Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def prepare_link_and_val(link, val_links):\n",
    "    result = {\"original\" : process_link(link, process_section_to_chunks)}\n",
    "    for i in range(len(val_links)):\n",
    "        val_link = copy.deepcopy(val_links[i])\n",
    "        val_link2 = copy.deepcopy(link)\n",
    "        val_link2['citation_context'] = val_link['citation_context']\n",
    "        val_link['citation_context'] = link['citation_context']\n",
    "        result.update({\n",
    "            \"val_orig_context_\"+str(i) : process_link(val_link, process_section_to_chunks),\n",
    "            \"val_orig_paper_\"+str(i) : process_link(val_link2, process_section_to_chunks)\n",
    "        })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:42:04.519456Z",
     "start_time": "2020-07-01T00:42:04.504036Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from plot_text_sim import plot_text_sim \n",
    "from scipy.spatial import distance\n",
    "import itertools\n",
    "metrics = {\"cos\" : distance.cosine, \"ts_ss\" : TS_SS}\n",
    "\n",
    "# do it in a function to prevent memory leaks\n",
    "def calculate_similarities(train_links, validation_links):\n",
    "    results = []\n",
    "    for link, val_links in tqdm.tqdm(list(itertools.zip_longest(train_links, validation_links))):\n",
    "        \n",
    "        preprocessed = prepare_link_and_val(link, val_links or [])\n",
    "        bow_itf = {name: calc_bow_itf_score(link, name='bow_itf_'+name) for name, link in preprocessed.items()}\n",
    "        \n",
    "        bert = {name: calc_embedding_scores(link, metrics, embedding_name='bert_'+name, embedding = bert_embedding) for name, link in preprocessed.items()}\n",
    "        \n",
    "        glove = {name: calc_embedding_scores(link, metrics, embedding_name='glove_'+name, embedding = glove_embedding) for name, link in preprocessed.items()}\n",
    "        roberta = {name: calc_embedding_scores(link, metrics, embedding_name='roberta_'+name, embedding = roberta_embedding) for name, link in preprocessed.items()}\n",
    "        bow_occur = {name: calc_bow_occurrence_score(link, name='bow_occurrence_'+name) for name, link in preprocessed.items()}\n",
    "        \n",
    "        data_emb = {name : pd.merge(pd.merge(glove[name], bert[name], left_index = True, right_index = True), roberta[name], left_index = True, right_index = True) for name, link in preprocessed.items()}\n",
    "        data_bow = {name : pd.merge(bow_occur[name], bow_itf[name], left_index = True, right_index = True) for name, link in preprocessed.items()}\n",
    "        data = {name : pd.merge(data_emb[name], data_bow[name], left_index = True, right_index = True) for name, link in preprocessed.items()}\n",
    "#         data = {name : bow_itf[name] for name, link in preprocessed.items()}\n",
    "        results.append({\n",
    "            'citing_str': preprocessed['original']['citing_str'],\n",
    "            'citing_context': preprocessed['original']['citing_context'],\n",
    "            'data': data,\n",
    "        })\n",
    "\n",
    "    return results, preprocessed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2428724f1b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_links\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mval_span\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_links\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalculate_similarities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-a9de2d9a7cd8>\u001b[0m in \u001b[0;36mcalculate_similarities\u001b[0;34m(train_links, validation_links)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_links\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzip_longest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_links\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_links\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mpreprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_link_and_val\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_links\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mbow_itf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcalc_bow_itf_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bow_itf_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpreprocessed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "\n",
    "start = 12\n",
    "span = 1\n",
    "val_span = 2\n",
    "val = np.split(validation_links[start:start+val_span*span], span)\n",
    "train = train_links[start:start+span]\n",
    "results, preprocessed = calculate_similarities(train, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.503291Z",
     "start_time": "2020-06-30T21:13:37.108552Z"
    }
   },
   "outputs": [],
   "source": [
    "processed_results = copy.deepcopy(results)\n",
    "\n",
    "for result in processed_results:\n",
    "    df = result['data']['original']\n",
    "    df['bert_original_cos_ampl'] = df['bert_original_cos']**6\n",
    "    df['comb_bow_itf_bert_cos'] = np.sqrt(df['bow_itf_original'] * df['bert_original_cos_ampl'])\n",
    "    df['comb_bow_itf_bert_ts_ss'] = np.sqrt(df['bow_itf_original'] * df['bert_original_ts_ss'])\n",
    "    # derive weighted moving average\n",
    "    for column in df.columns:\n",
    "        df[f'{column}_rolling'] = df[column].rolling(4, center=True, win_type='gaussian').mean(std=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_results = copy.deepcopy(results)\n",
    "\n",
    "for result in processed_results:\n",
    "    df = result['data']['original']\n",
    "    df['comb_bow_itf_bert'] = np.sqrt(df['bow_itf_original'] * (1 - df['bert_original_cos'])**4)\n",
    "    df['comb_bow_itf_glove'] = np.sqrt(df['bow_itf_original'] * (1 - df['glove_original_cos'])**4)\n",
    "    df['comb_bow_itf_roberta'] = np.sqrt(df['bow_itf_original'] * (1 - df['roberta_original_cos'])**4)\n",
    "    # derive weighted moving average\n",
    "    for column in df.columns:\n",
    "        df[f'{column}_rolling'] = df[column].rolling(4, center=True, win_type='gaussian').mean(std=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.505816Z",
     "start_time": "2020-06-30T21:13:15.078Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"results_emb_bow.p\", \"wb\") as file:\n",
    "    pickle.dump(processed_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.507159Z",
     "start_time": "2020-06-30T21:13:15.082Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"preprocessed_.p\", \"wb\") as file:\n",
    "    pickle.dump(preprocessed, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.508401Z",
     "start_time": "2020-06-30T21:13:15.088Z"
    }
   },
   "outputs": [],
   "source": [
    "def mean_results(results):\n",
    "    return{\n",
    "        name : {\n",
    "            column : np.mean(df[column]) for column in df.columns if df[column].dtype == float\n",
    "        } for name, df in results.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.510400Z",
     "start_time": "2020-06-30T21:13:15.097Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_res = mean_results(results[0][\"data\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.511959Z",
     "start_time": "2020-06-30T21:13:15.101Z"
    }
   },
   "outputs": [],
   "source": [
    "with open(\"results_mean.p\", \"wb\") as file:\n",
    "    pickle.dump(mean_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:13:37.513188Z",
     "start_time": "2020-06-30T21:13:15.104Z"
    }
   },
   "outputs": [],
   "source": [
    "process_link(train_links[start], process_section_to_chunks)['citing_str'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T21:26:17.121513Z",
     "start_time": "2020-06-30T21:26:17.098414Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import hiplot as hip\n",
    "result_df = results[0]['data']['original'].assign(text_beginning = lambda df: df.index.str[:10])\n",
    "exp = hip.Experiment.from_dataframe(result_df)\n",
    "displayed_exp = exp.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T00:51:34.639279Z",
     "start_time": "2020-07-01T00:51:22.943070Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_test_results, _test_preprocessed = calculate_similarities([test[2]], [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slightly wider manual comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T01:12:51.260155Z",
     "start_time": "2020-07-01T01:01:22.383060Z"
    }
   },
   "outputs": [],
   "source": [
    "start = 0\n",
    "span = 10\n",
    "test = test_links[start:start+span]\n",
    "\n",
    "test_results, test_preprocessed = calculate_similarities(test, [])\n",
    "\n",
    "processed_test_results = copy.deepcopy(test_results)\n",
    "\n",
    "for result in processed_test_results:\n",
    "    df = result['data']['original']\n",
    "    df['bert_original_cos_ampl'] = df['bert_original_cos']**6\n",
    "    df['comb_bow_itf_bert_cos'] = np.sqrt(df['bow_itf_original'] * df['bert_original_cos_ampl'])\n",
    "    df['comb_bow_itf_bert_ts_ss'] = np.sqrt(df['bow_itf_original'] * df['bert_original_ts_ss'])\n",
    "    # derive weighted moving average\n",
    "    for column in df.columns:\n",
    "        df[f'{column}_rolling'] = df[column].rolling(4, center=True, win_type='gaussian').mean(std=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T01:35:50.391489Z",
     "start_time": "2020-07-01T01:35:50.374825Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"results_emb_bow_test.p\", \"wb\") as file:\n",
    "    pickle.dump(processed_test_results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-30T23:57:49.563942Z",
     "start_time": "2020-06-30T23:57:49.551920Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "stats.spearmanr(df['bert_original_cos_ampl'],df['bert_original_ts_ss'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T01:18:05.332648Z",
     "start_time": "2020-07-01T01:18:05.222873Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def display_best(df, column):\n",
    "    print(f'\\n-- {column.upper()} --')\n",
    "    print(*[f'{i+1}: \"{chunk}\"' \n",
    "            for i, chunk in enumerate(df[column].sort_values(ascending=False).index.values.tolist()[:5])\n",
    "           ],\n",
    "          sep='\\n'\n",
    "         )\n",
    "    \n",
    "for result in processed_test_results:\n",
    "    result['citing_str']\n",
    "    result['citing_context']\n",
    "    df = result['data']['original']\n",
    "    display_best(df, 'bow_itf_original')\n",
    "    display_best(df, 'bert_original_ts_ss')\n",
    "    display_best(df, 'comb_bow_itf_bert_ts_ss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-01T01:18:18.421339Z",
     "start_time": "2020-07-01T01:18:14.942528Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for result in processed_test_results:\n",
    "    result['citing_str']\n",
    "    result['citing_context']\n",
    "    df = result['data']['original']\n",
    "    _ = plot_text_sim(df['bow_itf_original'], df.index, title='bow_itf')\n",
    "    _ = plot_text_sim(df['bert_original_cos_ampl'], df.index, title='bert_cos')\n",
    "    _ = plot_text_sim(df['bert_original_ts_ss'], df.index, title='bert_ts_ss')\n",
    "    _ = plot_text_sim(df['comb_bow_itf_bert_cos'], df.index, title='comb_bow_itf_bert_cos')\n",
    "    _ = plot_text_sim(df['comb_bow_itf_bert_ts_ss'], df.index, title='comb_bow_itf_bert_ts_ss')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
