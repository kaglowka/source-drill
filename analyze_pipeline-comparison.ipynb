{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T11:12:27.421444Z",
     "start_time": "2020-06-28T11:12:26.917752Z"
    }
   },
   "outputs": [],
   "source": [
    "### Imports and configuration\n",
    "\n",
    "# setup variables\n",
    "\n",
    "import os\n",
    "import json\n",
    "import tqdm\n",
    "from s2orc.config import CURRENT_VERSION\n",
    "\n",
    "# jsonlines https://jsonlines.readthedocs.io/en/latest/#api\n",
    "import jsonlines\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import hiplot # <3\n",
    "\n",
    "LOCAL_S2ORC_DIR = 's2orc-data'\n",
    "\n",
    "psychology_paper_dir = os.path.join(LOCAL_S2ORC_DIR, CURRENT_VERSION, 'psychology')\n",
    "psychology_paper_suffix = 'psych.text.jsonl'\n",
    "\n",
    "links_dir = os.path.join(LOCAL_S2ORC_DIR, CURRENT_VERSION, 'psych_links')\n",
    "links_suffix = 'psych.text.link.jsonl'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T11:12:29.902524Z",
     "start_time": "2020-06-28T11:12:28.093401Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:04<00:00, 21.31it/s]\n"
     ]
    }
   ],
   "source": [
    "## Get corpus into memory\n",
    "\n",
    "start = 0\n",
    "span = 100 # all: 1700\n",
    "\n",
    "links = []\n",
    "\n",
    "links_files = sorted(os.listdir(links_dir), key=lambda f: int(f.split('.')[0]))[start:(start+span)]\n",
    "for link_file in tqdm.tqdm(links_files):\n",
    "    with gzip.open(os.path.join(links_dir, link_file), 'rb') as f_in:\n",
    "        batch_links = list(jsonlines.Reader(f_in))\n",
    "        for link in batch_links:\n",
    "            if link['citing_paper']['grobid_parse'].get('body_text') is not None and link['cited_paper']['grobid_parse'].get('body_text') is not None:\n",
    "                links.append(link)\n",
    "\n",
    "np.random.seed(2134234)\n",
    "links = np.array(links)\n",
    "np.random.shuffle(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# work on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T11:13:49.932522Z",
     "start_time": "2020-06-28T11:13:49.924878Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/kasia/anaconda3/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from syntok.segmenter import split\n",
    "from syntok.tokenizer import Tokenizer\n",
    "import json\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentPoolEmbeddings, StackedEmbeddings, TransformerDocumentEmbeddings\n",
    "from flair.data import Sentence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T11:13:50.635385Z",
     "start_time": "2020-06-28T11:13:50.628041Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_section_to_chunks(text):\n",
    "    chunksize = 3\n",
    "    tokenized_sents = list(split(Tokenizer().tokenize(text)))\n",
    "    sents = [' '.join(str(token) for token in sent) for sent in tokenized_sents]\n",
    "    sents = [' '.join(sents[i:i+chunksize]) for i in range(len(sents)-chunksize)]\n",
    "    sentences = [s for s in [Sentence(sent, use_tokenizer=True) for sent in sents]\\\n",
    "                 if len(s.tokens) > 0]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T11:13:52.381057Z",
     "start_time": "2020-06-28T11:13:52.372585Z"
    }
   },
   "outputs": [],
   "source": [
    "def partition_get_whole_section(text):\n",
    "    return [text]\n",
    "\n",
    "def process_link(link, process_section):\n",
    "    context = link['citation_context']\n",
    "    citing_paper = link['citing_paper']\n",
    "    cited_paper = link['cited_paper']\n",
    "    \n",
    "    parts = []\n",
    "    for text_chunk in cited_paper['grobid_parse']['body_text']:\n",
    "        text = text_chunk.get('text')\n",
    "        if text is not None:\n",
    "            chunk_parts = process_section(text)\n",
    "            parts.extend(chunk_parts)\n",
    "    citing_string = ''.join([context['pre_context'], context['context_string'], context['post_context']])\n",
    "    return {\n",
    "        'citing_str': context['context_string'],\n",
    "        'citing_context': citing_string,\n",
    "        'cited_text_parts': parts,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T11:13:56.830978Z",
     "start_time": "2020-06-28T11:13:56.824737Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_embedding_scores(link, metrics, embedding_name, embedding):\n",
    "    all_sim = {}\n",
    "    \n",
    "    s = Sentence(link['citing_context'], use_tokenizer=True)\n",
    "    embedding.embed(s)\n",
    "    citation_embedding = s.embedding.detach().numpy()\n",
    "    sentences = link['cited_text_parts']\n",
    "    for sentence in sentences:\n",
    "        sentence.clear_embeddings()\n",
    "    embedding.embed(sentences)\n",
    "    for sentence in sentences:\n",
    "        all_sim[sentence.to_plain_string()] = {}\n",
    "        for name, metric in metrics.items():\n",
    "            sim = metric(sentence.embedding.detach().numpy(), citation_embedding)\n",
    "            full_name = '_'.join([embedding_name, name])\n",
    "            all_sim[sentence.to_plain_string()][full_name] = sim\n",
    "    return pd.DataFrame(all_sim).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T11:13:57.854079Z",
     "start_time": "2020-06-28T11:13:57.839966Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.23606797749979\n",
      "0.9999999999999998\n",
      "0.0004639582566999478\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "from scipy.linalg import norm\n",
    "from Vector_Similarity import *\n",
    "\n",
    "def TS_SS(vec1, vec2) :\n",
    "    return Triangle(vec1, vec2) * Sector(vec1, vec2)\n",
    "\n",
    "def Triangle(vec1, vec2) :\n",
    "    theta = math.radians(Theta(vec1,vec2))\n",
    "    return (norm(vec1) * norm(vec2) * math.sin(theta)) / 2\n",
    "\n",
    "def Theta(vec1, vec2):\n",
    "    try:\n",
    "        return math.acos(1-distance.cosine(vec1, vec2)) + math.radians(10)\n",
    "    except:\n",
    "        #print(vec1)\n",
    "        #print(vec2)\n",
    "        print(distance.cosine(vec1, vec2))\n",
    "        \n",
    "\n",
    "def Magnitude_Difference(vec1, vec2) :\n",
    "    return abs(norm(vec1) - norm(vec2))\n",
    "\n",
    "def Sector(vec1, vec2) :\n",
    "    ED = distance.euclidean(vec1, vec2)\n",
    "    MD = Magnitude_Difference(vec1, vec2)\n",
    "    theta = Theta(vec1, vec2)\n",
    "    return math.pi * math.pow((ED+MD),2) * theta/360"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cool pipelines with cool plots and coolest hiplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T11:14:41.459106Z",
     "start_time": "2020-06-28T11:14:11.307856Z"
    }
   },
   "outputs": [],
   "source": [
    "bert_embedding = TransformerDocumentEmbeddings('bert-base-uncased', fine_tune=False)\n",
    "roberta_embedding = TransformerDocumentEmbeddings('roberta-base', fine_tune=False)\n",
    "glove_embedding = DocumentPoolEmbeddings([WordEmbeddings('glove')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "def prepare_link_and_val(link, val_links):\n",
    "    result = {\"original\" : process_link(link, process_section_to_chunks)}\n",
    "    for i in range(len(val_links)):\n",
    "        val_link = copy.deepcopy(val_links[i])\n",
    "        val_link2 = copy.deepcopy(link)\n",
    "        val_link2['citation_context'] = val_link['citation_context']\n",
    "        val_link['citation_context'] = link['citation_context']\n",
    "        result.update({\n",
    "            \"val_orig_context_\"+str(i) : process_link(val_link, process_section_to_chunks),\n",
    "            \"val_orig_paper_\"+str(i) : process_link(val_link2, process_section_to_chunks)\n",
    "        })\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_train_links = int(0.5 * len(links))\n",
    "#n_validation_links = int(0.2 * len(links))\n",
    "#n_test_links = len(links) - n_train_links - n_validation_links\n",
    "train_links = links[:15]\n",
    "validation_links = links[15:30]\n",
    "#test_links = links[-n_test_links:]\n",
    "#links = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-28T11:16:04.973412Z",
     "start_time": "2020-06-28T11:15:47.137313Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plot_text_sim import plot_text_sim \n",
    "metrics = {\"cos\" : distance.cosine, \"ts_ss\" : TS_SS}\n",
    "start = 7\n",
    "span = 1\n",
    "val_span = 2\n",
    "results = []\n",
    "val = np.split(validation_links[start:start+val_span], span)\n",
    "for link, val_links in zip(train_links[start:start+span], val):\n",
    "    preprocessed = prepare_link_and_val(link, val_links)\n",
    "    glove = {name: calc_embedding_scores(link, metrics, embedding_name='glove_'+name, embedding = glove_embedding) for name, link in preprocessed.items()}\n",
    "    bert = {name: calc_embedding_scores(link, metrics, embedding_name='bert_'+name, embedding = bert_embedding) for name, link in preprocessed.items()}\n",
    "    roberta = {name: calc_embedding_scores(link, metrics, embedding_name='roberta_'+name, embedding = roberta_embedding) for name, link in preprocessed.items()}\n",
    "    data = {name : pd.merge(pd.merge(glove[name], bert[name], left_index = True, right_index = True), roberta[name], left_index = True, right_index = True) for name, link in preprocessed.items()}\n",
    "    results.append(data)\n",
    "    for link in preprocessed.values():\n",
    "        for sentence in link['cited_text_parts']:\n",
    "            sentence.clear_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pickle' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9d7cfb6bad54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"preprocessed_chunk3.p\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pickle' is not defined"
     ]
    }
   ],
   "source": [
    "with open(\"preprocessed_chunk3.p\", \"wb\") as file:\n",
    "    pickle.dump(preprocessed, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"first_results_chunk3.p\", \"wb\") as file:\n",
    "    pickle.dump(results, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"original\"]['text_beginning'] = results[0][\"original\"].index.str[:10]\n",
    "_ = plot_text_sim(results[0][\"original\"]['bert_original_cos'], results[0][\"original\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"val1\"]['text_beginning'] = results[0][\"val1\"].index.str[:10]\n",
    "_ = plot_text_sim(results[0][\"val1\"]['bert_val1_cos'], results[0][\"val1\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[0][\"val2\"]['text_beginning'] = results[0][\"val2\"].index.str[:10]\n",
    "_ = plot_text_sim(results[0][\"val2\"]['bert_val2_cos'], results[0][\"val2\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_results(results):\n",
    "    return{\n",
    "        name : {\n",
    "            column : np.mean(df[column]) for column in df.columns if df[column].dtype == float\n",
    "        } for name, df in results.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_res = mean_results(results[0])\n",
    "mean_res[\"original\"], mean_res[\"val1\"], mean_res[\"val2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"first_results_mean_chunk3.p\", \"wb\") as file:\n",
    "    pickle.dump(mean_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T21:02:25.161581Z",
     "start_time": "2020-06-27T21:02:25.113223Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import hiplot as hip\n",
    "exp = hip.Experiment.from_dataframe(results[0][\"original\"])\n",
    "displayed_exp = exp.display()\n",
    "displayed_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hiplot as hip\n",
    "exp = hip.Experiment.from_dataframe(results[0][\"val2\"])\n",
    "displayed_exp = exp.display()\n",
    "displayed_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hiplot as hip\n",
    "exp = hip.Experiment.from_dataframe(results[0][\"val1\"])\n",
    "displayed_exp = exp.display()\n",
    "displayed_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-27T20:58:53.319987Z",
     "start_time": "2020-06-27T20:58:53.310354Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "selected = displayed_exp.get_selected()\n",
    "idx = np.array([int(data_point.uid) for data_point in selected])\n",
    "clear_output(wait=True)\n",
    "print(*list(zip(idx, df.iloc[idx, :].index.to_list())), sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    " ## Dimensionality reduction of embeddings!!!!!!!!!!!!!!!!!1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
